from _typeshed import Incomplete
from contextlib import AbstractContextManager

class Native:
    CreateBoosterFlags_Default: int
    CreateBoosterFlags_DifferentialPrivacy: int
    CreateBoosterFlags_DisableSIMD: int
    TermBoostFlags_Default: int
    TermBoostFlags_DisableNewtonGain: int
    TermBoostFlags_DisableNewtonUpdate: int
    TermBoostFlags_GradientSums: int
    TermBoostFlags_RandomSplits: int
    CreateInteractionFlags_Default: int
    CreateInteractionFlags_DifferentialPrivacy: int
    CreateInteractionFlags_DisableSIMD: int
    CalcInteractionFlags_Default: int
    CalcInteractionFlags_Pure: int
    def __init__(self) -> None: ...
    @staticmethod
    def get_native_singleton(is_debug: bool = False, simd: bool = True): ...
    @staticmethod
    def get_count_scores_c(n_classes): ...
    def set_logging(self, level: Incomplete | None = None) -> None: ...
    def clean_float(self, val): ...
    def create_rng(self, random_state): ...
    def copy_rng(self, rng): ...
    def branch_rng(self, rng): ...
    def generate_seed(self, rng): ...
    def generate_gaussian_random(self, rng, stddev, count): ...
    def get_histogram_cut_count(self, X_col): ...
    def cut_uniform(self, X_col, max_cuts): ...
    def cut_quantile(self, X_col, min_samples_bin, is_rounded, max_cuts): ...
    def cut_winsorized(self, X_col, max_cuts): ...
    def suggest_graph_bounds(self, cuts, min_feature_val=..., max_feature_val=...): ...
    def discretize(self, X_col, cuts): ...
    def measure_dataset_header(self, n_features, n_weights, n_targets): ...
    def measure_feature(self, n_bins, is_missing, is_unknown, is_nominal, bin_indexes): ...
    def measure_weight(self, weights): ...
    def measure_classification_target(self, n_classes, targets): ...
    def measure_regression_target(self, targets): ...
    def fill_dataset_header(self, n_features, n_weights, n_targets, dataset) -> None: ...
    def fill_feature(self, n_bins, is_missing, is_unknown, is_nominal, bin_indexes, dataset) -> None: ...
    def fill_weight(self, weights, dataset) -> None: ...
    def fill_classification_target(self, n_classes, targets, dataset) -> None: ...
    def fill_regression_target(self, targets, dataset) -> None: ...
    def check_dataset(self, dataset) -> None: ...
    def extract_dataset_header(self, dataset): ...
    def extract_bin_counts(self, dataset, n_features): ...
    def extract_target_classes(self, dataset, n_targets): ...
    def sample_without_replacement(self, rng, count_training_samples, count_validation_samples): ...
    def sample_without_replacement_stratified(self, rng, n_classes, count_training_samples, count_validation_samples, targets): ...
    def determine_link(self, is_private, objective): ...
    def get_output_type(self, link): ...

class Booster(AbstractContextManager):
    """Lightweight wrapper for EBM C boosting code."""
    dataset: Incomplete
    bag: Incomplete
    init_scores: Incomplete
    term_features: Incomplete
    n_inner_bags: Incomplete
    rng: Incomplete
    create_booster_flags: Incomplete
    objective: Incomplete
    experimental_params: Incomplete
    def __init__(self, dataset, bag, init_scores, term_features, n_inner_bags, rng, create_booster_flags, objective, experimental_params) -> None:
        """Initializes internal wrapper for EBM C code.

        Args:
            dataset: binned data in a compressed native form
            bag: definition of what data is included. 1 = training, -1 = validation, 0 = not included
            init_scores: predictions from a prior predictor
                that this class will boost on top of.  For regression
                there is 1 prediction per sample.  For binary classification
                there is one logit.  For multiclass there are n_classes logits
            term_features: List of term feature indexes
            n_inner_bags: number of inner bags.
            rng: native random number generator
            experimental_params: unused data that can be passed into the native layer for debugging
        """
    def __enter__(self): ...
    def __exit__(self, *args) -> None: ...
    def close(self) -> None:
        """Deallocates C objects used to boost EBM."""
    def generate_term_update(self, rng, term_idx, term_boost_flags, learning_rate, min_samples_leaf, max_leaves):
        """Generates a boosting step update per feature
            by growing a shallow decision tree.

        Args:
            term_idx: The index for the term to generate the update for
            term_boost_flags: C interface options
            learning_rate: Learning rate as a float.
            min_samples_leaf: Min observations required to split.
            max_leaves: Max leaf nodes on feature step.

        Returns:
            gain for the generated boosting step.
        """
    def apply_term_update(self):
        """Updates the interal C state with the last model update

        Args:

        Returns:
            Validation loss for the boosting step.
        """
    def get_best_model(self): ...
    def get_current_model(self): ...
    def get_term_update_splits(self): ...
    def get_term_update(self): ...
    def set_term_update(self, term_idx, update_scores) -> None: ...

class InteractionDetector(AbstractContextManager):
    """Lightweight wrapper for EBM C interaction code."""
    dataset: Incomplete
    bag: Incomplete
    init_scores: Incomplete
    create_interaction_flags: Incomplete
    objective: Incomplete
    experimental_params: Incomplete
    def __init__(self, dataset, bag, init_scores, create_interaction_flags, objective, experimental_params) -> None:
        """Initializes internal wrapper for EBM C code.

        Args:
            dataset: binned data in a compressed native form
            bag: definition of what data is included. 1 = training, -1 = validation, 0 = not included
            init_scores: predictions from a prior predictor
                that this class will boost on top of.  For regression
                there is 1 prediction per sample.  For binary classification
                there is one logit.  For multiclass there are n_classes logits
            experimental_params: unused data that can be passed into the native layer for debugging

        """
    def __enter__(self): ...
    def __exit__(self, *args) -> None: ...
    def close(self) -> None:
        """Deallocates C objects used to determine interactions in EBM."""
    def calc_interaction_strength(self, feature_idxs, calc_interaction_flags, max_cardinality, min_samples_leaf):
        """Provides a strength measurement of a feature interaction. Higher is better."""
