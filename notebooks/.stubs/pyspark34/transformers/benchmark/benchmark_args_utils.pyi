from ..utils import logging as logging
from _typeshed import Incomplete
from dataclasses import dataclass
from typing import List

logger: Incomplete

def list_field(default: Incomplete | None = None, metadata: Incomplete | None = None): ...

@dataclass
class BenchmarkArguments:
    """
    BenchMarkArguments are arguments we use in our benchmark scripts **which relate to the training loop itself**.

    Using `HfArgumentParser` we can turn this class into argparse arguments to be able to specify them on the command
    line.
    """
    models: List[str] = ...
    batch_sizes: List[int] = ...
    sequence_lengths: List[int] = ...
    inference: bool = ...
    cuda: bool = ...
    tpu: bool = ...
    fp16: bool = ...
    training: bool = ...
    verbose: bool = ...
    speed: bool = ...
    memory: bool = ...
    trace_memory_line_by_line: bool = ...
    save_to_csv: bool = ...
    log_print: bool = ...
    env_print: bool = ...
    multi_process: bool = ...
    inference_time_csv_file: str = ...
    inference_memory_csv_file: str = ...
    train_time_csv_file: str = ...
    train_memory_csv_file: str = ...
    env_info_csv_file: str = ...
    log_filename: str = ...
    repeat: int = ...
    only_pretrain_model: bool = ...
    def __post_init__(self) -> None: ...
    def to_json_string(self):
        """
        Serializes this instance to a JSON string.
        """
    @property
    def model_names(self): ...
    @property
    def do_multi_processing(self): ...
    def __init__(self, models, batch_sizes, sequence_lengths, inference, cuda, tpu, fp16, training, verbose, speed, memory, trace_memory_line_by_line, save_to_csv, log_print, env_print, multi_process, inference_time_csv_file, inference_memory_csv_file, train_time_csv_file, train_memory_csv_file, env_info_csv_file, log_filename, repeat, only_pretrain_model) -> None: ...
