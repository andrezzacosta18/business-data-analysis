import torch
from .. import PreTrainedModel as PreTrainedModel, PretrainedConfig as PretrainedConfig, logging as logging
from ..models.auto import get_values as get_values
from ..models.auto.modeling_auto import MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES as MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES, MODEL_FOR_BACKBONE_MAPPING_NAMES as MODEL_FOR_BACKBONE_MAPPING_NAMES, MODEL_FOR_CAUSAL_LM_MAPPING_NAMES as MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, MODEL_FOR_CTC_MAPPING_NAMES as MODEL_FOR_CTC_MAPPING_NAMES, MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES as MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES, MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES as MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES, MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING_NAMES as MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING_NAMES, MODEL_FOR_MASKED_LM_MAPPING_NAMES as MODEL_FOR_MASKED_LM_MAPPING_NAMES, MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES as MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES, MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING_NAMES as MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING_NAMES, MODEL_FOR_PRETRAINING_MAPPING_NAMES as MODEL_FOR_PRETRAINING_MAPPING_NAMES, MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES as MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES, MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES as MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING_NAMES, MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES as MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES, MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES as MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES, MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES as MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES, MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES as MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES, MODEL_MAPPING_NAMES as MODEL_MAPPING_NAMES
from ..utils import ENV_VARS_TRUE_VALUES as ENV_VARS_TRUE_VALUES, TORCH_FX_REQUIRED_VERSION as TORCH_FX_REQUIRED_VERSION, is_torch_fx_available as is_torch_fx_available
from ..utils.versions import importlib_metadata as importlib_metadata
from _typeshed import Incomplete
from torch import nn as nn
from torch.fx import Graph as Graph, GraphModule as GraphModule, Proxy, Tracer
from typing import Any, Callable, Dict, List, Optional, Union

logger: Incomplete

def torch_nn_embedding(self, input): ...
def torch_nn_functional_embedding(input, weight, padding_idx: Incomplete | None = None, max_norm: Incomplete | None = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, sparse: bool = False): ...
def torch_nn_layernorm(self, input): ...
def torch_nn_groupnorm(self, input): ...
def torch_nn_linear(self, input): ...
def torch_relu(x): ...
def torch_nn_relu(self, x): ...
def torch_nn_functional_relu(x, inplace: bool = False): ...
def torch_where(condition, x, y): ...
def torch_abs(input, *, out: Incomplete | None = None): ...
def torch_arange(*args, **kwargs): ...
def torch_full(*args, **kwargs): ...
def torch_cat(tensors, dim: Incomplete | None = None, axis: Incomplete | None = None, *, out: Incomplete | None = None): ...
def torch_stack(tensors, dim: Incomplete | None = None, axis: Incomplete | None = None, *, out: Incomplete | None = None): ...
def torch_add(input, other, *, alpha: int = 1, out: Incomplete | None = None): ...
def torch_mul(input, other, *, out: Incomplete | None = None): ...
def torch_tensor_mul(self, other): ...
def torch_matmul(input, other, *, out: Incomplete | None = None): ...
def torch_bmm(input, mat2, *, out: Incomplete | None = None): ...
def torch_baddbmm(input, batch1, batch2, *, beta: int = 1, alpha: int = 1, out: Incomplete | None = None): ...
def torch_tensor_baddbmm(self, batch1, batch2, *, beta: int = 1, alpha: int = 1, out: Incomplete | None = None): ...
def torch_einsum(equation, *operands): ...
def torch_tensor_repeat(self, *sizes): ...
def torch_index_select(input, dim, index, *, out: Incomplete | None = None): ...
def torch_tensor_index_select(self, dim, index): ...
def torch_roll(input, shifts, dims: Incomplete | None = None): ...
def torch_flip(input, dims): ...
def torch_tensor_flip(self, dims): ...
def torch_nn_conv1d(self, input): ...
def torch_nn_conv2d(self, input): ...
def torch_squeeze(input, dim: Incomplete | None = None): ...
def torch_tensor_squeeze(self, dim: Incomplete | None = None): ...
def torch_unsqueeze(input, dim): ...
def torch_tensor_unsqueeze(self, dim): ...
def torch_unique_consecutive(input, **kwargs): ...
def torch_nn_functional_one_hot(tensor, num_classes: int = -1): ...
def torch_nn_mseloss(self, input, target): ...
def torch_nn_crossentropyloss(self, input, target): ...
def torch_nn_bcewithlogitsloss(self, input, target): ...
def operator_getitem(a, b): ...

class HFProxy(Proxy):
    """
    Proxy that uses metadata to handle data-dependent control-flow.
    """
    def install_metadata(self, metadata) -> None: ...
    @property
    def shape(self): ...
    @property
    def device(self): ...
    def __len__(self) -> int: ...
    def __bool__(self) -> bool: ...
    def __getattr__(self, k): ...
    def __setitem__(self, indices, values) -> None: ...
    def __contains__(self, key) -> bool: ...

class HFAttribute(HFProxy):
    root: Incomplete
    attr: Incomplete
    tracer: Incomplete
    def __init__(self, root, attr: str) -> None: ...
    @property
    def node(self): ...
    def __call__(self, *args, **kwargs): ...

class MetaDeviceAttribute(HFAttribute): ...

class HFTracer(Tracer):
    """
    Tracer that is able to symbolically trace models from the library. To do that, it uses the HFProxy instead of the
    regular PyTorch torch.fx.Proxy.
    """
    proxy_buffer_attributes: bool
    allow_insert_stateless_mods: bool
    def __init__(self, autowrap_modules=..., autowrap_functions=()) -> None: ...
    def create_proxy(self, kind, target, args, kwargs, name: Incomplete | None = None, type_expr: Incomplete | None = None, proxy_factory_fn: Incomplete | None = None): ...
    def getattr(self, attr: str, attr_val: Any, parameter_proxy_cache: Dict[str, Any]): ...
    orig_forward: Incomplete
    def call_module(self, m, forward, args, kwargs): ...
    def proxy(self, node): ...
    meta_args: Incomplete
    patched_torch_methods: Incomplete
    orig_fns: Incomplete
    graph: Incomplete
    def trace(self, root: Union[torch.nn.Module, Callable[..., Any]], concrete_args: Optional[Dict[str, Any]] = None, dummy_inputs: Optional[Dict[str, Any]] = None, complete_concrete_args_with_inputs_not_in_dummy_inputs: bool = True) -> Graph:
        """
        Traces `root` and returns the corresponding FX `torch.fx.Graph` representation. `root` can either be a
        `torch.nn.Module` instance or a Python callable. Note that after this call, `self.root` may be different from
        the `root` passed in here. For example, when a free function is passed to `trace()`, we will create a
        `torch.nn.Module` instance to use as the root and add embedded constants to.

        Args:
            root (`torch.nn.Module` or  `Callable`):
                Either a `torch.nn.Module`` or a function to be traced through. If root is not a
                [`~transformers.PreTrainedModel`], then `dummy_inputs` must be passed, otherwise tracing will fail.
            concrete_args (`Dict[str, Any], *optional*):
                Concrete arguments that should not be treated as Proxies
            dummy_inputs (`Dict[str, Any]`, *optional*):
                The dummy inputs needed to handle data-dependent control-flow if `root` is not a
                [`~transformers.PreTrainedModel`]. It can also be used when `root` is a
                [`~transformers.PreTrainedModel`] to specify custom dummy inputs for a subset or all the model inputs.
            complete_concrete_args_with_inputs_not_in_dummy_inputs (`bool`, *optional*, defaults to `True`):
                If `True`, and `dummy_inputs` is specified, every argument that `root` can take that is not in
                `dummy_inputs` and not in `concrete_args` will be added to `concrete_args`, otherwise does nothing.

        Returns:
            `torch.fx.Graph`:
                A FX `torch.fx.Graph` representing the semantics of the passed-in `root`.

        """
    def path_of_module(self, mod: nn.Module) -> str:
        '''
        Helper method to find the qualified name of `mod` in the Module hierarchy of `root`. For example, if `root` has
        a submodule named `foo`, which has a submodule named `bar`, passing `bar` into this function will return the
        string "foo.bar".

        Args:
            mod (str): The `Module` to retrieve the qualified name for.
        '''
    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool: ...

def get_concrete_args(model: nn.Module, input_names: List[str]): ...
def check_if_model_is_supported(model: PreTrainedModel): ...
def symbolic_trace(model: PreTrainedModel, input_names: Optional[List[str]] = None, disable_check: bool = False) -> GraphModule:
    '''
    Performs symbolic tracing on the model.

    Args:
        model ([`PretrainedModel`]):
            The model to trace.
        input_names (`List[str]`, *optional*):
            The names of the inputs of the traced model. If unset, model.dummy_inputs.keys() are used instead.
        disable_check (`bool`, *optional*, defaults to `False`):
            If `True`, no check is done before trying to trace the model, this is mostly usesul for debugging purposes.

    Returns:
        `torch.fx.GraphModule`: A GraphModule constructed by recording operations seen while tracing the model.

    Example:

        ```python
        from transformers.utils.fx import symbolic_trace

        traced_model = symbolic_trace(model, input_names=["input_ids", "attention_mask", "token_type_ids"])
        ```
    '''
