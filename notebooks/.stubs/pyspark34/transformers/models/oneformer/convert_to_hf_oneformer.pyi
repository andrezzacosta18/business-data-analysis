from _typeshed import Incomplete
from dataclasses import dataclass
from pathlib import Path
from torch import Tensor, nn as nn
from transformers import CLIPTokenizer as CLIPTokenizer, DinatConfig as DinatConfig, SwinConfig as SwinConfig
from transformers.models.oneformer.image_processing_oneformer import OneFormerImageProcessor as OneFormerImageProcessor
from transformers.models.oneformer.modeling_oneformer import OneFormerConfig as OneFormerConfig, OneFormerForUniversalSegmentation as OneFormerForUniversalSegmentation, OneFormerForUniversalSegmentationOutput as OneFormerForUniversalSegmentationOutput, OneFormerModel as OneFormerModel, OneFormerModelOutput as OneFormerModelOutput
from transformers.models.oneformer.processing_oneformer import OneFormerProcessor as OneFormerProcessor
from transformers.utils import logging as logging
from typing import Any, Dict, Iterator, List, Tuple

StateDict = Dict[str, Tensor]
logger: Incomplete

class TrackedStateDict:
    to_track: Incomplete
    def __init__(self, to_track: Dict) -> None:
        '''This class "tracks" a python dictionary by keeping track of which item is accessed.

        Args:
            to_track (Dict): The dictionary we wish to track
        '''
    def __getitem__(self, key: str) -> Any: ...
    def __setitem__(self, key: str, item: Any): ...
    def diff(self) -> List[str]:
        """This method returns a set difference between the keys in the tracked state dict and the one we have access so far.
        This is an effective method to check if we have update all the keys

        Returns:
            List[str]: List of keys not yet updated
        """
    def copy(self) -> Dict: ...

def prepare_img(): ...

@dataclass
class Args:
    """Fake command line arguments needed by oneformer/detectron2 implementation"""
    config_file: str
    def __init__(self, config_file) -> None: ...

def setup_cfg(args: Args): ...

class OriginalOneFormerConfigToOursConverter:
    def __call__(self, original_config: object, is_swin: bool) -> OneFormerConfig: ...

class OriginalOneFormerConfigToProcessorConverter:
    def __call__(self, original_config: object, model_repo: str) -> OneFormerProcessor: ...

class OriginalOneFormerCheckpointToOursConverter:
    original_model: Incomplete
    config: Incomplete
    def __init__(self, original_model: nn.Module, config: OneFormerConfig) -> None: ...
    def pop_all(self, renamed_keys: List[Tuple[str, str]], dst_state_dict: StateDict, src_state_dict: StateDict): ...
    def replace_swin_backbone(self, dst_state_dict: StateDict, src_state_dict: StateDict, config: OneFormerConfig): ...
    def replace_dinat_backbone(self, dst_state_dict: StateDict, src_state_dict: StateDict, config: OneFormerConfig): ...
    def replace_pixel_module(self, dst_state_dict: StateDict, src_state_dict: StateDict, is_swin: bool): ...
    def replace_keys_qkv_transformer_decoder(self, dst_state_dict: StateDict, src_state_dict: StateDict): ...
    def replace_transformer_module(self, dst_state_dict: StateDict, src_state_dict: StateDict): ...
    def replace_task_mlp(self, dst_state_dict: StateDict, src_state_dict: StateDict): ...
    def replace_text_projector(self, dst_state_dict: StateDict, src_state_dict: StateDict): ...
    def replace_text_mapper(self, dst_state_dict: StateDict, src_state_dict: StateDict): ...
    def convert(self, oneformer: OneFormerModel, is_swin: bool) -> OneFormerModel: ...
    @staticmethod
    def using_dirs(checkpoints_dir: Path, config_dir: Path) -> Iterator[Tuple[object, Path, Path]]: ...

def post_process_sem_seg_output(outputs: OneFormerForUniversalSegmentationOutput, target_size: Tuple[int, int]): ...
def test(original_model, our_model: OneFormerForUniversalSegmentation, processor: OneFormerProcessor, model_repo: str): ...
def get_name(checkpoint_file: Path): ...
