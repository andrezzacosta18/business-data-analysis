from ...tokenization_utils_base import BatchEncoding as BatchEncoding
from ...utils import logging as logging
from .configuration_rag import RagConfig as RagConfig
from _typeshed import Incomplete
from typing import List, Optional

logger: Incomplete

class RagTokenizer:
    question_encoder: Incomplete
    generator: Incomplete
    current_tokenizer: Incomplete
    def __init__(self, question_encoder, generator) -> None: ...
    def save_pretrained(self, save_directory) -> None: ...
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs): ...
    def __call__(self, *args, **kwargs): ...
    def batch_decode(self, *args, **kwargs): ...
    def decode(self, *args, **kwargs): ...
    def prepare_seq2seq_batch(self, src_texts: List[str], tgt_texts: Optional[List[str]] = None, max_length: Optional[int] = None, max_target_length: Optional[int] = None, padding: str = 'longest', return_tensors: str = None, truncation: bool = True, **kwargs) -> BatchEncoding: ...
