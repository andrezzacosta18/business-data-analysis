from transformers import LukeConfig as LukeConfig, LukeForMaskedLM as LukeForMaskedLM, MLukeTokenizer as MLukeTokenizer, XLMRobertaTokenizer as XLMRobertaTokenizer
from transformers.tokenization_utils_base import AddedToken as AddedToken

def convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, pytorch_dump_folder_path, model_size) -> None: ...
def load_original_entity_vocab(entity_vocab_path): ...
