from datetime import datetime
from py4j.java_gateway import JavaObject
from pyspark.rdd import RDD
from pyspark.resultiterable import ResultIterable
from pyspark.serializers import Serializer
from pyspark.storagelevel import StorageLevel
from pyspark.streaming.context import StreamingContext
from typing import Callable, Generic, Hashable, Iterable, List, Tuple, TypeVar, overload

__all__ = ['DStream']

S = TypeVar('S')
T = TypeVar('T')
T_co = TypeVar('T_co', covariant=True)
U = TypeVar('U')
K = TypeVar('K', bound=Hashable)
V = TypeVar('V')

class DStream(Generic[T_co]):
    """
    A Discretized Stream (DStream), the basic abstraction in Spark Streaming,
    is a continuous sequence of RDDs (of the same type) representing a
    continuous stream of data (see :class:`RDD` in the Spark core documentation
    for more details on RDDs).

    DStreams can either be created from live data (such as, data from TCP
    sockets, etc.) using a :class:`StreamingContext` or it can be
    generated by transforming existing DStreams using operations such as
    `map`, `window` and `reduceByKeyAndWindow`. While a Spark Streaming
    program is running, each DStream periodically generates a RDD, either
    from live data or by transforming the RDD generated by a parent DStream.

    DStreams internally is characterized by a few basic properties:
     - A list of other DStreams that the DStream depends on
     - A time interval at which the DStream generates an RDD
     - A function that is used to generate an RDD after each time interval
    """
    is_cached: bool
    is_checkpointed: bool
    def __init__(self, jdstream: JavaObject, ssc: StreamingContext, jrdd_deserializer: Serializer) -> None: ...
    def context(self) -> StreamingContext:
        """
        Return the StreamingContext associated with this DStream
        """
    def count(self) -> DStream[int]:
        """
        Return a new DStream in which each RDD has a single element
        generated by counting each RDD of this DStream.
        """
    def filter(self, f: Callable[[T], bool]) -> DStream[T]:
        """
        Return a new DStream containing only the elements that satisfy predicate.
        """
    def flatMap(self, f: Callable[[T], Iterable[U]], preservesPartitioning: bool = False) -> DStream[U]:
        """
        Return a new DStream by applying a function to all elements of
        this DStream, and then flattening the results
        """
    def map(self, f: Callable[[T], U], preservesPartitioning: bool = False) -> DStream[U]:
        """
        Return a new DStream by applying a function to each element of DStream.
        """
    def mapPartitions(self, f: Callable[[Iterable[T]], Iterable[U]], preservesPartitioning: bool = False) -> DStream[U]:
        """
        Return a new DStream in which each RDD is generated by applying
        mapPartitions() to each RDDs of this DStream.
        """
    def mapPartitionsWithIndex(self, f: Callable[[int, Iterable[T]], Iterable[U]], preservesPartitioning: bool = False) -> DStream[U]:
        """
        Return a new DStream in which each RDD is generated by applying
        mapPartitionsWithIndex() to each RDDs of this DStream.
        """
    def reduce(self, func: Callable[[T, T], T]) -> DStream[T]:
        """
        Return a new DStream in which each RDD has a single element
        generated by reducing each RDD of this DStream.
        """
    def reduceByKey(self, func: Callable[[V, V], V], numPartitions: int | None = None) -> DStream[Tuple[K, V]]:
        """
        Return a new DStream by applying reduceByKey to each RDD.
        """
    def combineByKey(self, createCombiner: Callable[[V], U], mergeValue: Callable[[U, V], U], mergeCombiners: Callable[[U, U], U], numPartitions: int | None = None) -> DStream[Tuple[K, U]]:
        """
        Return a new DStream by applying combineByKey to each RDD.
        """
    def partitionBy(self, numPartitions: int, partitionFunc: Callable[[K], int] = ...) -> DStream[Tuple[K, V]]:
        """
        Return a copy of the DStream in which each RDD are partitioned
        using the specified partitioner.
        """
    @overload
    def foreachRDD(self, func: Callable[[RDD[T]], None]) -> None: ...
    @overload
    def foreachRDD(self, func: Callable[[datetime, RDD[T]], None]) -> None: ...
    def pprint(self, num: int = 10) -> None:
        """
        Print the first num elements of each RDD generated in this DStream.

        Parameters
        ----------
        num : int, optional
            the number of elements from the first will be printed.
        """
    def mapValues(self, f: Callable[[V], U]) -> DStream[Tuple[K, U]]:
        """
        Return a new DStream by applying a map function to the value of
        each key-value pairs in this DStream without changing the key.
        """
    def flatMapValues(self, f: Callable[[V], Iterable[U]]) -> DStream[Tuple[K, U]]:
        """
        Return a new DStream by applying a flatmap function to the value
        of each key-value pairs in this DStream without changing the key.
        """
    def glom(self) -> DStream[List[T]]:
        """
        Return a new DStream in which RDD is generated by applying glom()
        to RDD of this DStream.
        """
    def cache(self) -> DStream[T]:
        """
        Persist the RDDs of this DStream with the default storage level
        (`MEMORY_ONLY`).
        """
    def persist(self, storageLevel: StorageLevel) -> DStream[T]:
        """
        Persist the RDDs of this DStream with the given storage level
        """
    def checkpoint(self, interval: int) -> DStream[T]:
        """
        Enable periodic checkpointing of RDDs of this DStream

        Parameters
        ----------
        interval : int
            time in seconds, after each period of that, generated
            RDD will be checkpointed
        """
    def groupByKey(self, numPartitions: int | None = None) -> DStream[Tuple[K, Iterable[V]]]:
        """
        Return a new DStream by applying groupByKey on each RDD.
        """
    def countByValue(self) -> DStream[Tuple[K, int]]:
        """
        Return a new DStream in which each RDD contains the counts of each
        distinct value in each RDD of this DStream.
        """
    def saveAsTextFiles(self, prefix: str, suffix: str | None = None) -> None:
        """
        Save each RDD in this DStream as at text file, using string
        representation of elements.
        """
    @overload
    def transform(self, func: Callable[[RDD[T]], RDD[U]]) -> TransformedDStream[U]: ...
    @overload
    def transform(self, func: Callable[[datetime, RDD[T]], RDD[U]]) -> TransformedDStream[U]: ...
    @overload
    def transformWith(self, func: Callable[[RDD[T], RDD[U]], RDD[V]], other: DStream[U], keepSerializer: bool = ...) -> DStream[V]: ...
    @overload
    def transformWith(self, func: Callable[[datetime, RDD[T], RDD[U]], RDD[V]], other: DStream[U], keepSerializer: bool = ...) -> DStream[V]: ...
    def repartition(self, numPartitions: int) -> DStream[T]:
        """
        Return a new DStream with an increased or decreased level of parallelism.
        """
    def union(self, other: DStream[U]) -> DStream[T | U]:
        """
        Return a new DStream by unifying data of another DStream with this DStream.

        Parameters
        ----------
        other : :class:`DStream`
            Another DStream having the same interval (i.e., slideDuration)
            as this DStream.
        """
    def cogroup(self, other: DStream[Tuple[K, U]], numPartitions: int | None = None) -> DStream[Tuple[K, Tuple[ResultIterable[V], ResultIterable[U]]]]:
        """
        Return a new DStream by applying 'cogroup' between RDDs of this
        DStream and `other` DStream.

        Hash partitioning is used to generate the RDDs with `numPartitions` partitions.
        """
    def join(self, other: DStream[Tuple[K, U]], numPartitions: int | None = None) -> DStream[Tuple[K, Tuple[V, U]]]:
        """
        Return a new DStream by applying 'join' between RDDs of this DStream and
        `other` DStream.

        Hash partitioning is used to generate the RDDs with `numPartitions`
        partitions.
        """
    def leftOuterJoin(self, other: DStream[Tuple[K, U]], numPartitions: int | None = None) -> DStream[Tuple[K, Tuple[V, U | None]]]:
        """
        Return a new DStream by applying 'left outer join' between RDDs of this DStream and
        `other` DStream.

        Hash partitioning is used to generate the RDDs with `numPartitions`
        partitions.
        """
    def rightOuterJoin(self, other: DStream[Tuple[K, U]], numPartitions: int | None = None) -> DStream[Tuple[K, Tuple[V | None, U]]]:
        """
        Return a new DStream by applying 'right outer join' between RDDs of this DStream and
        `other` DStream.

        Hash partitioning is used to generate the RDDs with `numPartitions`
        partitions.
        """
    def fullOuterJoin(self, other: DStream[Tuple[K, U]], numPartitions: int | None = None) -> DStream[Tuple[K, Tuple[V | None, U | None]]]:
        """
        Return a new DStream by applying 'full outer join' between RDDs of this DStream and
        `other` DStream.

        Hash partitioning is used to generate the RDDs with `numPartitions`
        partitions.
        """
    def slice(self, begin: datetime | int, end: datetime | int) -> List[RDD[T]]:
        """
        Return all the RDDs between 'begin' to 'end' (both included)

        `begin`, `end` could be datetime.datetime() or unix_timestamp
        """
    def window(self, windowDuration: int, slideDuration: int | None = None) -> DStream[T]:
        """
        Return a new DStream in which each RDD contains all the elements in seen in a
        sliding window of time over this DStream.

        Parameters
        ----------
        windowDuration : int
            width of the window; must be a multiple of this DStream's
            batching interval
        slideDuration : int, optional
            sliding interval of the window (i.e., the interval after which
            the new DStream will generate RDDs); must be a multiple of this
            DStream's batching interval
        """
    def reduceByWindow(self, reduceFunc: Callable[[T, T], T], invReduceFunc: Callable[[T, T], T] | None, windowDuration: int, slideDuration: int) -> DStream[T]:
        '''
        Return a new DStream in which each RDD has a single element generated by reducing all
        elements in a sliding window over this DStream.

        if `invReduceFunc` is not None, the reduction is done incrementally
        using the old window\'s reduced value :

        1. reduce the new values that entered the window (e.g., adding new counts)

        2. "inverse reduce" the old values that left the window (e.g., subtracting old counts)
        This is more efficient than `invReduceFunc` is None.

        Parameters
        ----------
        reduceFunc : function
            associative and commutative reduce function
        invReduceFunc : function
            inverse reduce function of `reduceFunc`; such that for all y,
            and invertible x:
            `invReduceFunc(reduceFunc(x, y), x) = y`
        windowDuration : int
            width of the window; must be a multiple of this DStream\'s
            batching interval
        slideDuration : int
            sliding interval of the window (i.e., the interval after which
            the new DStream will generate RDDs); must be a multiple of this
            DStream\'s batching interval
        '''
    def countByWindow(self, windowDuration: int, slideDuration: int) -> DStream[int]:
        """
        Return a new DStream in which each RDD has a single element generated
        by counting the number of elements in a window over this DStream.
        windowDuration and slideDuration are as defined in the window() operation.

        This is equivalent to window(windowDuration, slideDuration).count(),
        but will be more efficient if window is large.
        """
    def countByValueAndWindow(self, windowDuration: int, slideDuration: int, numPartitions: int | None = None) -> DStream[Tuple[T, int]]:
        """
        Return a new DStream in which each RDD contains the count of distinct elements in
        RDDs in a sliding window over this DStream.

        Parameters
        ----------
        windowDuration : int
            width of the window; must be a multiple of this DStream's
            batching interval
        slideDuration : int
            sliding interval of the window (i.e., the interval after which
            the new DStream will generate RDDs); must be a multiple of this
            DStream's batching interval
        numPartitions : int, optional
            number of partitions of each RDD in the new DStream.
        """
    def groupByKeyAndWindow(self, windowDuration: int, slideDuration: int, numPartitions: int | None = None) -> DStream[Tuple[K, Iterable[V]]]:
        """
        Return a new DStream by applying `groupByKey` over a sliding window.
        Similar to `DStream.groupByKey()`, but applies it over a sliding window.

        Parameters
        ----------
        windowDuration : int
            width of the window; must be a multiple of this DStream's
            batching interval
        slideDuration : int
            sliding interval of the window (i.e., the interval after which
            the new DStream will generate RDDs); must be a multiple of this
            DStream's batching interval
        numPartitions : int, optional
            Number of partitions of each RDD in the new DStream.
        """
    def reduceByKeyAndWindow(self, func: Callable[[V, V], V], invFunc: Callable[[V, V], V] | None, windowDuration: int, slideDuration: int | None = None, numPartitions: int | None = None, filterFunc: Callable[[Tuple[K, V]], bool] | None = None) -> DStream[Tuple[K, V]]:
        '''
        Return a new DStream by applying incremental `reduceByKey` over a sliding window.

        The reduced value of over a new window is calculated using the old window\'s reduce value :
         1. reduce the new values that entered the window (e.g., adding new counts)
         2. "inverse reduce" the old values that left the window (e.g., subtracting old counts)

        `invFunc` can be None, then it will reduce all the RDDs in window, could be slower
        than having `invFunc`.

        Parameters
        ----------
        func : function
            associative and commutative reduce function
        invFunc : function
            inverse function of `reduceFunc`
        windowDuration : int
            width of the window; must be a multiple of this DStream\'s
            batching interval
        slideDuration : int, optional
            sliding interval of the window (i.e., the interval after which
            the new DStream will generate RDDs); must be a multiple of this
            DStream\'s batching interval
        numPartitions : int, optional
            number of partitions of each RDD in the new DStream.
        filterFunc : function, optional
            function to filter expired key-value pairs;
            only pairs that satisfy the function are retained
            set this to null if you do not want to filter
        '''
    def updateStateByKey(self, updateFunc: Callable[[Iterable[V], S | None], S], numPartitions: int | None = None, initialRDD: RDD[Tuple[K, S]] | Iterable[Tuple[K, S]] | None = None) -> DStream[Tuple[K, S]]:
        '''
        Return a new "state" DStream where the state for each key is updated by applying
        the given function on the previous state of the key and the new values of the key.

        Parameters
        ----------
        updateFunc : function
            State update function. If this function returns None, then
            corresponding state key-value pair will be eliminated.
        '''

class TransformedDStream(DStream[U]):
    """
    TransformedDStream is a DStream generated by an Python function
    transforming each RDD of a DStream to another RDDs.

    Multiple continuous transformations of DStream can be combined into
    one transformation.
    """
    @overload
    def __init__(self, prev: DStream[T], func: Callable[[RDD[T]], RDD[U]]) -> None: ...
    @overload
    def __init__(self, prev: DStream[T], func: Callable[[datetime, RDD[T]], RDD[U]]) -> None: ...
