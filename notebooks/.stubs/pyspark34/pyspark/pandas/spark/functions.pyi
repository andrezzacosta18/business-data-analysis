from pyspark import SparkContext as SparkContext
from pyspark.sql.column import Column as Column

def product(col: Column, dropna: bool) -> Column: ...
def stddev(col: Column, ddof: int) -> Column: ...
def var(col: Column, ddof: int) -> Column: ...
def skew(col: Column) -> Column: ...
def kurt(col: Column) -> Column: ...
def mode(col: Column, dropna: bool) -> Column: ...
def covar(col1: Column, col2: Column, ddof: int) -> Column: ...
def repeat(col: Column, n: int | Column) -> Column:
    """
    Repeats a string column n times, and returns it as a new string column.
    """
def date_part(field: str | Column, source: Column) -> Column:
    """
    Extracts a part of the date/timestamp or interval source.
    """
