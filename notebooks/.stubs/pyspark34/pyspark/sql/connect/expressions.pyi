import pyspark.sql.connect.proto as proto
from _typeshed import Incomplete
from pyspark.serializers import CloudPickleSerializer as CloudPickleSerializer
from pyspark.sql.connect.client import SparkConnectClient as SparkConnectClient
from pyspark.sql.connect.types import JVM_BYTE_MAX as JVM_BYTE_MAX, JVM_BYTE_MIN as JVM_BYTE_MIN, JVM_INT_MAX as JVM_INT_MAX, JVM_INT_MIN as JVM_INT_MIN, JVM_LONG_MAX as JVM_LONG_MAX, JVM_LONG_MIN as JVM_LONG_MIN, JVM_SHORT_MAX as JVM_SHORT_MAX, JVM_SHORT_MIN as JVM_SHORT_MIN, UnparsedDataType as UnparsedDataType, proto_schema_to_pyspark_data_type as proto_schema_to_pyspark_data_type, pyspark_types_to_proto_types as pyspark_types_to_proto_types
from pyspark.sql.connect.utils import check_dependencies as check_dependencies
from pyspark.sql.connect.window import WindowSpec as WindowSpec
from pyspark.sql.types import ArrayType as ArrayType, BinaryType as BinaryType, BooleanType as BooleanType, ByteType as ByteType, DataType as DataType, DateType as DateType, DayTimeIntervalType as DayTimeIntervalType, DecimalType as DecimalType, DoubleType as DoubleType, FloatType as FloatType, IntegerType as IntegerType, LongType as LongType, NullType as NullType, ShortType as ShortType, StringType as StringType, TimestampNTZType as TimestampNTZType, TimestampType as TimestampType
from typing import Any, Callable, Sequence, Tuple

class Expression:
    """
    Expression base class.
    """
    def __init__(self) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...
    def alias(self, *alias: str, **kwargs: Any) -> ColumnAlias: ...
    def name(self) -> str: ...

class CaseWhen(Expression):
    def __init__(self, branches: Sequence[Tuple[Expression, Expression]], else_value: Expression | None) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...

class ColumnAlias(Expression):
    def __init__(self, parent: Expression, alias: Sequence[str], metadata: Any) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...

class LiteralExpression(Expression):
    """A literal expression.

    The Python types are converted best effort into the relevant proto types. On the Spark Connect
    server side, the proto types are converted to the Catalyst equivalents."""
    def __init__(self, value: Any, dataType: DataType) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression:
        """Converts the literal expression to the literal in proto."""

class ColumnReference(Expression):
    """Represents a column reference. There is no guarantee that this column
    actually exists. In the context of this project, we refer by its name and
    treat it as an unresolved attribute. Attributes that have the same fully
    qualified name are identical"""
    def __init__(self, unparsed_identifier: str, plan_id: int | None = None) -> None: ...
    def name(self) -> str:
        """Returns the qualified name of the column reference."""
    def to_plan(self, session: SparkConnectClient) -> proto.Expression:
        """Returns the Proto representation of the expression."""
    def __eq__(self, other: Any) -> bool: ...

class UnresolvedStar(Expression):
    def __init__(self, unparsed_target: str | None) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...
    def __eq__(self, other: Any) -> bool: ...

class SQLExpression(Expression):
    """Returns Expression which contains a string which is a SQL expression
    and server side will parse it by Catalyst
    """
    def __init__(self, expr: str) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression:
        """Returns the Proto representation of the SQL expression."""
    def __eq__(self, other: Any) -> bool: ...

class SortOrder(Expression):
    def __init__(self, child: Expression, ascending: bool = True, nullsFirst: bool = True) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...

class UnresolvedFunction(Expression):
    def __init__(self, name: str, args: Sequence['Expression'], is_distinct: bool = False) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...

class PythonUDF:
    """Represents a Python user-defined function."""
    def __init__(self, output_type: DataType | str, eval_type: int, func: Callable[..., Any], python_ver: str) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.PythonUDF: ...

class JavaUDF:
    """Represents a Java (aggregate) user-defined function."""
    def __init__(self, class_name: str, output_type: DataType | str | None = None, aggregate: bool = False) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.JavaUDF: ...

class CommonInlineUserDefinedFunction(Expression):
    """Represents a user-defined function with an inlined defined function body of any programming
    languages."""
    def __init__(self, function_name: str, function: PythonUDF | JavaUDF, deterministic: bool = False, arguments: Sequence[Expression] = []) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...
    def to_plan_udf(self, session: SparkConnectClient) -> proto.CommonInlineUserDefinedFunction:
        """Compared to `to_plan`, it returns a CommonInlineUserDefinedFunction instead of an
        Expression."""
    def to_plan_judf(self, session: SparkConnectClient) -> proto.CommonInlineUserDefinedFunction: ...

class WithField(Expression):
    def __init__(self, structExpr: Expression, fieldName: str, valueExpr: Expression) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...

class DropField(Expression):
    def __init__(self, structExpr: Expression, fieldName: str) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...

class UnresolvedExtractValue(Expression):
    def __init__(self, child: Expression, extraction: Expression) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...

class UnresolvedRegex(Expression):
    col_name: Incomplete
    def __init__(self, col_name: str, plan_id: int | None = None) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...

class CastExpression(Expression):
    def __init__(self, expr: Expression, data_type: DataType | str) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...

class UnresolvedNamedLambdaVariable(Expression):
    def __init__(self, name_parts: Sequence[str]) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...
    @staticmethod
    def fresh_var_name(name: str) -> str: ...

class LambdaFunction(Expression):
    def __init__(self, function: Expression, arguments: Sequence[UnresolvedNamedLambdaVariable]) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...

class WindowExpression(Expression):
    def __init__(self, windowFunction: Expression, windowSpec: WindowSpec) -> None: ...
    def to_plan(self, session: SparkConnectClient) -> proto.Expression: ...
