import pyarrow as pa
import pyspark.sql.connect.proto as proto
from _typeshed import Incomplete
from pyspark.sql.connect._typing import ColumnOrName as ColumnOrName
from pyspark.sql.connect.client import SparkConnectClient as SparkConnectClient
from pyspark.sql.connect.column import Column as Column
from pyspark.sql.connect.expressions import ColumnReference as ColumnReference, LiteralExpression as LiteralExpression, SortOrder as SortOrder
from pyspark.sql.connect.types import pyspark_types_to_proto_types as pyspark_types_to_proto_types
from pyspark.sql.connect.udf import UserDefinedFunction as UserDefinedFunction
from pyspark.sql.connect.utils import check_dependencies as check_dependencies
from pyspark.sql.types import DataType as DataType
from typing import Any, Dict, List, Mapping, Sequence

class InputValidationError(Exception): ...

class LogicalPlan:
    INDENT: int
    def __init__(self, child: LogicalPlan | None) -> None: ...
    def unresolved_attr(self, colName: str) -> proto.Expression:
        """Creates an unresolved attribute from a column name."""
    def to_attr_or_expression(self, col: ColumnOrName, session: SparkConnectClient) -> proto.Expression:
        """Returns either an instance of an unresolved attribute or the serialized
        expression value of the column."""
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def command(self, session: SparkConnectClient) -> proto.Command: ...
    def to_proto(self, session: SparkConnectClient, debug: bool = False) -> proto.Plan:
        """
        Generates connect proto plan based on this LogicalPlan.

        Parameters
        ----------
        session : :class:`SparkConnectClient`, optional.
            a session that connects remote spark cluster.
        debug: bool
            if enabled, the proto plan will be printed.
        """
    def print(self, indent: int = 0) -> str:
        """
        Print the simple string representation of the current :class:`LogicalPlan`.

        Parameters
        ----------
        indent : int
            The number of leading spaces for the output string.

        Returns
        -------
        str
            Simple string representation of this :class:`LogicalPlan`.
        """

class DataSource(LogicalPlan):
    """A datasource with a format and optional a schema from which Spark reads data"""
    def __init__(self, format: str | None = None, schema: str | None = None, options: Mapping[str, str] | None = None, paths: List[str] | None = None, predicates: List[str] | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Read(LogicalPlan):
    table_name: Incomplete
    options: Incomplete
    def __init__(self, table_name: str, options: Dict[str, str] | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def print(self, indent: int = 0) -> str: ...

class LocalRelation(LogicalPlan):
    """Creates a LocalRelation plan object based on a PyArrow Table."""
    def __init__(self, table: pa.Table | None, schema: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def print(self, indent: int = 0) -> str: ...

class ShowString(LogicalPlan):
    num_rows: Incomplete
    truncate: Incomplete
    vertical: Incomplete
    def __init__(self, child: LogicalPlan | None, num_rows: int, truncate: int, vertical: bool) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Project(LogicalPlan):
    """Logical plan object for a projection.

    All input arguments are directly serialized into the corresponding protocol buffer
    objects. This class only provides very limited error handling and input validation.

    To be compatible with PySpark, we validate that the input arguments are all
    expressions to be able to serialize them to the server.

    """
    alias: Incomplete
    def __init__(self, child: LogicalPlan | None, *columns: ColumnOrName) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class WithColumns(LogicalPlan):
    """Logical plan object for a withColumns operation."""
    def __init__(self, child: LogicalPlan | None, columnNames: Sequence[str], columns: Sequence[Column], metadata: Sequence[str] | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Hint(LogicalPlan):
    """Logical plan object for a Hint operation."""
    def __init__(self, child: LogicalPlan | None, name: str, parameters: List[Any]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Filter(LogicalPlan):
    filter: Incomplete
    def __init__(self, child: LogicalPlan | None, filter: Column) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Limit(LogicalPlan):
    limit: Incomplete
    def __init__(self, child: LogicalPlan | None, limit: int) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Tail(LogicalPlan):
    limit: Incomplete
    def __init__(self, child: LogicalPlan | None, limit: int) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Offset(LogicalPlan):
    offset: Incomplete
    def __init__(self, child: LogicalPlan | None, offset: int = 0) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Deduplicate(LogicalPlan):
    all_columns_as_keys: Incomplete
    column_names: Incomplete
    def __init__(self, child: LogicalPlan | None, all_columns_as_keys: bool = False, column_names: List[str] | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Sort(LogicalPlan):
    columns: Incomplete
    is_global: Incomplete
    def __init__(self, child: LogicalPlan | None, columns: List[Column], is_global: bool) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Drop(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, columns: List[Column | str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Sample(LogicalPlan):
    lower_bound: Incomplete
    upper_bound: Incomplete
    with_replacement: Incomplete
    seed: Incomplete
    deterministic_order: Incomplete
    def __init__(self, child: LogicalPlan | None, lower_bound: float, upper_bound: float, with_replacement: bool, seed: int | None, deterministic_order: bool = False) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Aggregate(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, group_type: str, grouping_cols: Sequence[Column], aggregate_cols: Sequence[Column], pivot_col: Column | None, pivot_values: Sequence[Any] | None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Join(LogicalPlan):
    left: Incomplete
    right: Incomplete
    on: Incomplete
    how: Incomplete
    def __init__(self, left: LogicalPlan | None, right: LogicalPlan, on: str | List[str] | Column | List[Column] | None, how: str | None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def print(self, indent: int = 0) -> str: ...

class SetOperation(LogicalPlan):
    other: Incomplete
    by_name: Incomplete
    is_all: Incomplete
    set_op: Incomplete
    allow_missing_columns: Incomplete
    def __init__(self, child: LogicalPlan | None, other: LogicalPlan | None, set_op: str, is_all: bool = True, by_name: bool = False, allow_missing_columns: bool = False) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def print(self, indent: int = 0) -> str: ...

class Repartition(LogicalPlan):
    """Repartition Relation into a different number of partitions."""
    def __init__(self, child: LogicalPlan | None, num_partitions: int, shuffle: bool) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class RepartitionByExpression(LogicalPlan):
    """Repartition Relation into a different number of partitions using Expression"""
    num_partitions: Incomplete
    columns: Incomplete
    def __init__(self, child: LogicalPlan | None, num_partitions: int | None, columns: List['ColumnOrName']) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class SubqueryAlias(LogicalPlan):
    """Alias for a relation."""
    def __init__(self, child: LogicalPlan | None, alias: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class SQL(LogicalPlan):
    def __init__(self, query: str, args: Dict[str, Any] | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
    def command(self, session: SparkConnectClient) -> proto.Command: ...

class Range(LogicalPlan):
    def __init__(self, start: int, end: int, step: int, num_partitions: int | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ToSchema(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, schema: DataType) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class WithColumnsRenamed(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, colsMap: Mapping[str, str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class Unpivot(LogicalPlan):
    """Logical plan object for a unpivot operation."""
    ids: Incomplete
    values: Incomplete
    variable_column_name: Incomplete
    value_column_name: Incomplete
    def __init__(self, child: LogicalPlan | None, ids: List['ColumnOrName'], values: List['ColumnOrName'] | None, variable_column_name: str, value_column_name: str) -> None: ...
    def col_to_expr(self, col: ColumnOrName, session: SparkConnectClient) -> proto.Expression: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CollectMetrics(LogicalPlan):
    """Logical plan object for a CollectMetrics operation."""
    def __init__(self, child: LogicalPlan | None, name: str, exprs: List['ColumnOrName']) -> None: ...
    def col_to_expr(self, col: ColumnOrName, session: SparkConnectClient) -> proto.Expression: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class NAFill(LogicalPlan):
    cols: Incomplete
    values: Incomplete
    def __init__(self, child: LogicalPlan | None, cols: List[str] | None, values: List[Any]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class NADrop(LogicalPlan):
    cols: Incomplete
    min_non_nulls: Incomplete
    def __init__(self, child: LogicalPlan | None, cols: List[str] | None, min_non_nulls: int | None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class NAReplace(LogicalPlan):
    cols: Incomplete
    replacements: Incomplete
    def __init__(self, child: LogicalPlan | None, cols: List[str] | None, replacements: Dict[Any, Any]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatSummary(LogicalPlan):
    statistics: Incomplete
    def __init__(self, child: LogicalPlan | None, statistics: List[str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatDescribe(LogicalPlan):
    cols: Incomplete
    def __init__(self, child: LogicalPlan | None, cols: List[str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatCov(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, col1: str, col2: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatApproxQuantile(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, cols: List[str], probabilities: List[float], relativeError: float) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatCrosstab(LogicalPlan):
    col1: Incomplete
    col2: Incomplete
    def __init__(self, child: LogicalPlan | None, col1: str, col2: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatFreqItems(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, cols: List[str], support: float) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatSampleBy(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, col: ColumnOrName, fractions: Dict[Any, float], seed: int | None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class StatCorr(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, col1: str, col2: str, method: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ToDF(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, cols: Sequence[str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CreateView(LogicalPlan):
    def __init__(self, child: LogicalPlan | None, name: str, is_global: bool, replace: bool) -> None: ...
    def command(self, session: SparkConnectClient) -> proto.Command: ...

class WriteOperation(LogicalPlan):
    source: Incomplete
    path: Incomplete
    table_name: Incomplete
    table_save_method: Incomplete
    mode: Incomplete
    sort_cols: Incomplete
    partitioning_cols: Incomplete
    options: Incomplete
    num_buckets: int
    bucket_cols: Incomplete
    def __init__(self, child: LogicalPlan) -> None: ...
    def command(self, session: SparkConnectClient) -> proto.Command: ...
    def print(self, indent: int = 0) -> str: ...

class WriteOperationV2(LogicalPlan):
    table_name: Incomplete
    provider: Incomplete
    partitioning_columns: Incomplete
    options: Incomplete
    table_properties: Incomplete
    mode: Incomplete
    overwrite_condition: Incomplete
    def __init__(self, child: LogicalPlan, table_name: str) -> None: ...
    def col_to_expr(self, col: ColumnOrName, session: SparkConnectClient) -> proto.Expression: ...
    def command(self, session: SparkConnectClient) -> proto.Command: ...

class CurrentDatabase(LogicalPlan):
    def __init__(self) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class SetCurrentDatabase(LogicalPlan):
    def __init__(self, db_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ListDatabases(LogicalPlan):
    def __init__(self) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ListTables(LogicalPlan):
    def __init__(self, db_name: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ListFunctions(LogicalPlan):
    def __init__(self, db_name: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ListColumns(LogicalPlan):
    def __init__(self, table_name: str, db_name: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class GetDatabase(LogicalPlan):
    def __init__(self, db_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class GetTable(LogicalPlan):
    def __init__(self, table_name: str, db_name: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class GetFunction(LogicalPlan):
    def __init__(self, function_name: str, db_name: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class DatabaseExists(LogicalPlan):
    def __init__(self, db_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class TableExists(LogicalPlan):
    def __init__(self, table_name: str, db_name: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class FunctionExists(LogicalPlan):
    def __init__(self, function_name: str, db_name: str | None = None) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CreateExternalTable(LogicalPlan):
    def __init__(self, table_name: str, path: str, source: str | None = None, schema: DataType | None = None, options: Mapping[str, str] = {}) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CreateTable(LogicalPlan):
    def __init__(self, table_name: str, path: str, source: str | None = None, description: str | None = None, schema: DataType | None = None, options: Mapping[str, str] = {}) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class DropTempView(LogicalPlan):
    def __init__(self, view_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class DropGlobalTempView(LogicalPlan):
    def __init__(self, view_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class RecoverPartitions(LogicalPlan):
    def __init__(self, table_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class IsCached(LogicalPlan):
    def __init__(self, table_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CacheTable(LogicalPlan):
    def __init__(self, table_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class UncacheTable(LogicalPlan):
    def __init__(self, table_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ClearCache(LogicalPlan):
    def __init__(self) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class RefreshTable(LogicalPlan):
    def __init__(self, table_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class RefreshByPath(LogicalPlan):
    def __init__(self, path: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CurrentCatalog(LogicalPlan):
    def __init__(self) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class SetCurrentCatalog(LogicalPlan):
    def __init__(self, catalog_name: str) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class ListCatalogs(LogicalPlan):
    def __init__(self) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class MapPartitions(LogicalPlan):
    """Logical plan object for a mapPartitions-equivalent API: mapInPandas, mapInArrow."""
    def __init__(self, child: LogicalPlan | None, function: UserDefinedFunction, cols: List[str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class GroupMap(LogicalPlan):
    """Logical plan object for a Group Map API: apply, applyInPandas."""
    def __init__(self, child: LogicalPlan | None, grouping_cols: Sequence[Column], function: UserDefinedFunction, cols: List[str]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CoGroupMap(LogicalPlan):
    """Logical plan object for a CoGroup Map API: applyInPandas."""
    def __init__(self, input: LogicalPlan | None, input_grouping_cols: Sequence[Column], other: LogicalPlan | None, other_grouping_cols: Sequence[Column], function: UserDefinedFunction, cols: List[Column]) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...

class CachedRelation(LogicalPlan):
    def __init__(self, plan: proto.Relation) -> None: ...
    def plan(self, session: SparkConnectClient) -> proto.Relation: ...
