import datetime
from sempy._utils._log import log as log
from sempy.fabric._client._rest_client import PowerBIRestClient as PowerBIRestClient
from sempy.fabric._token_provider import TokenProvider as TokenProvider
from sempy.fabric._utils import is_valid_uuid as is_valid_uuid
from sempy.fabric.exceptions import DatasetNotFoundException as DatasetNotFoundException, FabricHTTPException as FabricHTTPException, WorkspaceNotFoundException as WorkspaceNotFoundException

class _PBIRestAPI:
    def __init__(self, token_provider: TokenProvider | None = None) -> None: ...
    def list_workspaces(self, filter: str | None = None, top: int | None = None, skip: int | None = None): ...
    def list_gateways(self): ...
    def list_dataflows(self, workspace_name: str, workspace_id: str): ...
    def list_dataflow_storage_accounts(self): ...
    def list_apps(self): ...
    def get_workspace_name_from_id(self, workspace_id: str) -> str: ...
    def get_workspace_datasets(self, workspace_name: str, workspace_id: str): ...
    @staticmethod
    def create_refresh_body(refresh_type, max_parallelism, commit_mode, retry_count, apply_refresh_policy, effective_date, objects: list | None = None) -> dict: ...
    def refresh_post(self, dataset_id: str, workspace_id: str, workspace_name: str, refresh_type: str = 'automatic', max_parallelism: int = 10, commit_mode: str = 'transactional', retry_count: int = 0, objects: list | None = None, apply_refresh_policy: bool = True, effective_date: datetime.date = ..., verbose: int = 0) -> str: ...
    def get_refresh_execution_details(self, dataset_id: str, request_id: str, workspace_id: str, workspace_name: str) -> dict: ...
    def list_refresh_history(self, dataset_id: str, workspace_id: str, workspace_name: str, top_n: int | None = None) -> dict: ...
    def get_dataset_name_from_id(self, dataset_id: str, workspace_name: str) -> str: ...
    def get_dataset_model_id(self, dataset_id: str): ...
    def get_dataset_schema_entitites(self, dataset_id: str): ...
    def execute_dax_query(self, dataset_id: str, query: str, num_rows: int | None = None): ...
    def calculate_measure(self, dataset_id: str, measure: list[dict[str, str]], groupby_columns: list[dict[str, str]], filters: list[dict[str, list]], num_rows: int | None, verbose: int) -> tuple[list[dict], list[list]]: ...
    def upload_pbix(self, dataset_name: str, pbix: bytes, workspace_id: str, workspace_name: str, skip_report: bool = True): ...
    def update_dataset_storage_mode(self, dataset_id: str, target_storage_mode: int): ...
    def update_dataset_export_to_onelake(self, dataset_id: str, export: bool = True): ...
    def list_reports(self, workspace_name: str, workspace_id: str): ...
