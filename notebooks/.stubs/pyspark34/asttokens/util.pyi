import ast
import tokenize
from .astroid_compat import NodeNG as NodeNG
from _typeshed import Incomplete
from abc import ABCMeta
from ast import AST, Module, expr
from typing import Any, Callable, Iterable, Iterator, List, NamedTuple, Tuple

class EnhancedAST(AST):
    first_token: Token
    last_token: Token
    lineno: int
AstNode = EnhancedAST | NodeNG
TokenInfo = tokenize.TokenInfo

def token_repr(tok_type: int, string: str | None) -> str:
    """Returns a human-friendly representation of a token with the given type and string."""

class Token(NamedTuple('Token', [('type', Incomplete), ('string', Incomplete), ('start', Incomplete), ('end', Incomplete), ('line', Incomplete), ('index', Incomplete), ('startpos', Incomplete), ('endpos', Incomplete)])):
    """
  TokenInfo is an 8-tuple containing the same 5 fields as the tokens produced by the tokenize
  module, and 3 additional ones useful for this module:

  - [0] .type     Token type (see token.py)
  - [1] .string   Token (a string)
  - [2] .start    Starting (row, column) indices of the token (a 2-tuple of ints)
  - [3] .end      Ending (row, column) indices of the token (a 2-tuple of ints)
  - [4] .line     Original line (string)
  - [5] .index    Index of the token in the list of tokens that it belongs to.
  - [6] .startpos Starting character offset into the input text.
  - [7] .endpos   Ending character offset into the input text.
  """

AstConstant: Incomplete

def match_token(token: Token, tok_type: int, tok_str: str | None = None) -> bool:
    """Returns true if token is of the given type and, if a string is given, has that string."""
def expect_token(token: Token, tok_type: int, tok_str: str | None = None) -> None:
    """
  Verifies that the given token is of the expected type. If tok_str is given, the token string
  is verified too. If the token doesn't match, raises an informative ValueError.
  """
def is_non_coding_token(token_type: int) -> bool:
    """
    These are considered non-coding tokens, as they don't affect the syntax tree.
    """
def generate_tokens(text: str) -> Iterator[TokenInfo]:
    """
  Generates standard library tokens for the given code.
  """
def iter_children_func(node: AST) -> Callable:
    """
  Returns a function which yields all direct children of a AST node,
  skipping children that are singleton nodes.
  The function depends on whether ``node`` is from ``ast`` or from the ``astroid`` module.
  """
def iter_children_astroid(node: NodeNG, include_joined_str: bool = False) -> Iterator | List: ...

SINGLETONS: Incomplete

def iter_children_ast(node: AST, include_joined_str: bool = False) -> Iterator[AST | expr]: ...

stmt_class_names: Incomplete
expr_class_names: Incomplete

def is_expr(node: AstNode) -> bool:
    """Returns whether node is an expression node."""
def is_stmt(node: AstNode) -> bool:
    """Returns whether node is a statement node."""
def is_module(node: AstNode) -> bool:
    """Returns whether node is a module node."""
def is_joined_str(node: AstNode) -> bool:
    """Returns whether node is a JoinedStr node, used to represent f-strings."""
def is_starred(node: AstNode) -> bool:
    """Returns whether node is a starred expression node."""
def is_slice(node: AstNode) -> bool:
    """Returns whether node represents a slice, e.g. `1:2` in `x[1:2]`"""
def is_empty_astroid_slice(node: AstNode) -> bool: ...
def visit_tree(node: Module, previsit: Callable[[AstNode, Token | None], Tuple[Token | None, Token | None]], postvisit: Callable[[AstNode, Token | None, Token | None], None] | None) -> None:
    """
  Scans the tree under the node depth-first using an explicit stack. It avoids implicit recursion
  via the function call stack to avoid hitting 'maximum recursion depth exceeded' error.

  It calls ``previsit()`` and ``postvisit()`` as follows:

  * ``previsit(node, par_value)`` - should return ``(par_value, value)``
        ``par_value`` is as returned from ``previsit()`` of the parent.

  * ``postvisit(node, par_value, value)`` - should return ``value``
        ``par_value`` is as returned from ``previsit()`` of the parent, and ``value`` is as
        returned from ``previsit()`` of this node itself. The return ``value`` is ignored except
        the one for the root node, which is returned from the overall ``visit_tree()`` call.

  For the initial node, ``par_value`` is None. ``postvisit`` may be None.
  """
def walk(node: AST, include_joined_str: bool = False) -> Iterator[Module | AstNode]:
    """
  Recursively yield all descendant nodes in the tree starting at ``node`` (including ``node``
  itself), using depth-first pre-order traversal (yieling parents before their children).

  This is similar to ``ast.walk()``, but with a different order, and it works for both ``ast`` and
  ``astroid`` trees. Also, as ``iter_children()``, it skips singleton nodes generated by ``ast``.

  By default, ``JoinedStr`` (f-string) nodes and their contents are skipped
  because they previously couldn't be handled. Set ``include_joined_str`` to True to include them.
  """
def replace(text: str, replacements: List[Tuple[int, int, str]]) -> str:
    '''
  Replaces multiple slices of text with new values. This is a convenience method for making code
  modifications of ranges e.g. as identified by ``ASTTokens.get_text_range(node)``. Replacements is
  an iterable of ``(start, end, new_text)`` tuples.

  For example, ``replace("this is a test", [(0, 4, "X"), (8, 9, "THE")])`` produces
  ``"X is THE test"``.
  '''

class NodeMethods:
    """
  Helper to get `visit_{node_type}` methods given a node's class and cache the results.
  """
    def __init__(self) -> None: ...
    def get(self, obj: Any, cls: ABCMeta | type) -> Callable:
        """
    Using the lowercase name of the class as node_type, returns `obj.visit_{node_type}`,
    or `obj.visit_default` if the type-specific method is not found.
    """

def patched_generate_tokens(original_tokens: Iterable[TokenInfo]) -> Iterator[TokenInfo]:
    """
    Fixes tokens yielded by `tokenize.generate_tokens` to handle more non-ASCII characters in identifiers.
    Workaround for https://github.com/python/cpython/issues/68382.
    Should only be used when tokenizing a string that is known to be valid syntax,
    because it assumes that error tokens are not actually errors.
    Combines groups of consecutive NAME, NUMBER, and/or ERRORTOKEN tokens into a single NAME token.
    """
def combine_tokens(group: List[tokenize.TokenInfo]) -> List[tokenize.TokenInfo]: ...
def last_stmt(node: ast.AST) -> ast.AST:
    """
  If the given AST node contains multiple statements, return the last one.
  Otherwise, just return the node.
  """
def fstring_positions_work() -> bool:
    """
    The positions attached to nodes inside f-string FormattedValues have some bugs
    that were fixed in Python 3.9.7 in https://github.com/python/cpython/pull/27729.
    This checks for those bugs more concretely without relying on the Python version.
    Specifically this checks:
     - Values with a format spec or conversion
     - Repeated (i.e. identical-looking) expressions
     - f-strings implicitly concatenated over multiple lines.
     - Multiline, triple-quoted f-strings.
    """
def annotate_fstring_nodes(tree: ast.AST) -> None:
    """
    Add a special attribute `_broken_positions` to nodes inside f-strings
    if the lineno/col_offset cannot be trusted.
    """
