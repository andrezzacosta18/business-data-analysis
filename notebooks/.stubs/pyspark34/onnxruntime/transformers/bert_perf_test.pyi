from _typeshed import Incomplete
from dataclasses import dataclass

@dataclass
class TestSetting:
    batch_size: int
    sequence_length: int
    test_cases: int
    test_times: int
    use_gpu: bool
    use_io_binding: bool
    provider: str
    intra_op_num_threads: int
    seed: int
    verbose: bool
    log_severity: int
    average_sequence_length: int
    random_sequence_length: bool
    def __init__(self, batch_size, sequence_length, test_cases, test_times, use_gpu, use_io_binding, provider, intra_op_num_threads, seed, verbose, log_severity, average_sequence_length, random_sequence_length) -> None: ...

@dataclass
class ModelSetting:
    model_path: str
    input_ids_name: str
    segment_ids_name: str
    input_mask_name: str
    opt_level: int
    input_tuning_results: str | None
    output_tuning_results: str | None
    mask_type: int
    def __init__(self, model_path, input_ids_name, segment_ids_name, input_mask_name, opt_level, input_tuning_results, output_tuning_results, mask_type) -> None: ...

def create_session(model_path, use_gpu, provider, intra_op_num_threads, graph_optimization_level: Incomplete | None = None, log_severity: int = 2, tuning_results_path: Incomplete | None = None): ...
def numpy_type(torch_type): ...
def create_input_output_tensors(inputs, outputs, device): ...
def create_io_binding(sess, input_tensors, output_tensors): ...
def onnxruntime_inference_with_io_binding(session, all_inputs, output_names, test_setting): ...
def onnxruntime_inference(session, all_inputs, output_names): ...
def to_string(model_path, session, test_setting): ...
def run_one_test(model_setting, test_setting, perf_results, all_inputs, intra_op_num_threads) -> None: ...
def launch_test(model_setting, test_setting, perf_results, all_inputs, intra_op_num_threads) -> None: ...
def run_perf_tests(model_setting, test_setting, perf_results, all_inputs) -> None: ...
def run_performance(model_setting, test_setting, perf_results) -> None: ...
def parse_arguments(): ...
def main(): ...
