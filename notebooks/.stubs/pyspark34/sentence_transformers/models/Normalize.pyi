from torch import Tensor as Tensor, nn
from typing import Dict

class Normalize(nn.Module):
    """
    This layer normalizes embeddings to unit length
    """
    def __init__(self) -> None: ...
    def forward(self, features: Dict[str, Tensor]): ...
    def save(self, output_path) -> None: ...
    @staticmethod
    def load(input_path): ...
