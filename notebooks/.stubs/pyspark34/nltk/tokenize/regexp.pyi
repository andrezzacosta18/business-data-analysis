from _typeshed import Incomplete
from collections.abc import Generator
from nltk.tokenize.api import TokenizerI as TokenizerI
from nltk.tokenize.util import regexp_span_tokenize as regexp_span_tokenize

class RegexpTokenizer(TokenizerI):
    """
    A tokenizer that splits a string using a regular expression, which
    matches either the tokens or the separators between tokens.

        >>> tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')

    :type pattern: str
    :param pattern: The pattern used to build this tokenizer.
        (This pattern must not contain capturing parentheses;
        Use non-capturing parentheses, e.g. (?:...), instead)
    :type gaps: bool
    :param gaps: True if this tokenizer's pattern should be used
        to find separators between tokens; False if this
        tokenizer's pattern should be used to find the tokens
        themselves.
    :type discard_empty: bool
    :param discard_empty: True if any empty tokens `''`
        generated by the tokenizer should be discarded.  Empty
        tokens can only be generated if `_gaps == True`.
    :type flags: int
    :param flags: The regexp flags used to compile this
        tokenizer's pattern.  By default, the following flags are
        used: `re.UNICODE | re.MULTILINE | re.DOTALL`.

    """
    def __init__(self, pattern, gaps: bool = False, discard_empty: bool = True, flags=...) -> None: ...
    def tokenize(self, text): ...
    def span_tokenize(self, text) -> Generator[Incomplete, None, None]: ...

class WhitespaceTokenizer(RegexpTokenizer):
    '''
    Tokenize a string on whitespace (space, tab, newline).
    In general, users should use the string ``split()`` method instead.

        >>> from nltk.tokenize import WhitespaceTokenizer
        >>> s = "Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks."
        >>> WhitespaceTokenizer().tokenize(s) # doctest: +NORMALIZE_WHITESPACE
        [\'Good\', \'muffins\', \'cost\', \'$3.88\', \'in\', \'New\', \'York.\',
        \'Please\', \'buy\', \'me\', \'two\', \'of\', \'them.\', \'Thanks.\']
    '''
    def __init__(self) -> None: ...

class BlanklineTokenizer(RegexpTokenizer):
    """
    Tokenize a string, treating any sequence of blank lines as a delimiter.
    Blank lines are defined as lines containing no characters, except for
    space or tab characters.
    """
    def __init__(self) -> None: ...

class WordPunctTokenizer(RegexpTokenizer):
    '''
    Tokenize a text into a sequence of alphabetic and
    non-alphabetic characters, using the regexp ``\\w+|[^\\w\\s]+``.

        >>> from nltk.tokenize import WordPunctTokenizer
        >>> s = "Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks."
        >>> WordPunctTokenizer().tokenize(s) # doctest: +NORMALIZE_WHITESPACE
        [\'Good\', \'muffins\', \'cost\', \'$\', \'3\', \'.\', \'88\', \'in\', \'New\', \'York\',
        \'.\', \'Please\', \'buy\', \'me\', \'two\', \'of\', \'them\', \'.\', \'Thanks\', \'.\']
    '''
    def __init__(self) -> None: ...

def regexp_tokenize(text, pattern, gaps: bool = False, discard_empty: bool = True, flags=...):
    """
    Return a tokenized copy of *text*.  See :class:`.RegexpTokenizer`
    for descriptions of the arguments.
    """

blankline_tokenize: Incomplete
wordpunct_tokenize: Incomplete
