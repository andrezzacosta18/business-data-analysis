from synapse.ml.core.serialize.java_params_patch import *
from .FeatureUsagePayload import FeatureUsagePayload as FeatureUsagePayload
from .UsageConstants import FeatureActivityName as FeatureActivityName, UsageFeatureNames as UsageFeatureNames
from .Usage_Utils import reportUsage as reportUsage
from _typeshed import Incomplete
from pyspark.ml import Transformer
from pyspark.ml.param.shared import HasInputCols, HasOutputCol
from typing import Callable, List

class MLFlowTransformer(Transformer, HasInputCols, HasOutputCol):
    """
    Args:
        inputCols (str):  Columns to feed to the model
        outputCol (str): The column to add output predictions to
        modelName (str):  The name of the model in the model registry
        modelVersion (str):  The version of the model in the model registry
        trackingUri (str):  The location of the MLFlow tracking server
        registerModel (bool): Whether to register the model with the PREDICT SQL command
    """
    modelName: Incomplete
    modelVersion: Incomplete
    trackingUri: Incomplete
    registerModel: Incomplete
    modelInfo: Incomplete
    modelUdf: Incomplete
    def __init__(self, inputCols: List[str] = None, outputCol: str = None, modelName: str = None, modelVersion: str = 'latest', trackingUri: str = None, registerModel: bool = True) -> None:
        """
        Parameters
        ----------

        """
    def setInputCols(self, value): ...
    def setOutputCol(self, value): ...
    def setModelName(self, value): ...
    def getModelName(self): ...
    def setModelVersion(self, value): ...
    def getModelVersion(self): ...
    def setTrackingUri(self, value): ...
    def getTrackingUri(self): ...
    def setRegisterModel(self, value): ...
    def getRegisterModel(self): ...
    def report_metrics(self, metrics_name: str, metrics_value: float = 1.0): ...
    def register(self) -> None: ...
    def to_udf(self, old_style: bool = False) -> Callable: ...
    def get_pandas_udf(self, f, output_sig, tensor_input): ...
