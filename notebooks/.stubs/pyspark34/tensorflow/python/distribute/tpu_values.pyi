from _typeshed import Incomplete
from tensorflow.python.distribute import tpu_replicated_variable as tpu_replicated_variable, tpu_util as tpu_util, values as values, values_util as values_util
from tensorflow.python.eager import context as context, tape as tape
from tensorflow.python.framework import ops as ops
from tensorflow.python.ops import gen_resource_variable_ops as gen_resource_variable_ops, math_ops as math_ops, variable_scope as variable_scope

class TPUVariableMixin:
    """Mixin for TPU variables."""
    def __init__(self, *args, **kwargs) -> None: ...
    def __getattr__(self, name): ...
    def get(self): ...
    @property
    def handle(self):
        """The handle by which this variable can be accessed."""
    @property
    def device(self): ...
    def read_value(self): ...
    def value(self): ...
    @property
    def op(self): ...

class TPUDistributedVariable(TPUVariableMixin, values.DistributedVariable):
    """DistributedVariable subclass for TPUStrategy."""
    def assign_sub(self, value, use_locking: bool = False, name: Incomplete | None = None, read_value: bool = True): ...
    def assign_add(self, value, use_locking: bool = False, name: Incomplete | None = None, read_value: bool = True): ...
    def assign(self, value, use_locking: bool = False, name: Incomplete | None = None, read_value: bool = True): ...
    def scatter_sub(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_add(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_mul(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_div(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_min(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_max(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_update(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...

class TPUMirroredVariable(TPUVariableMixin, values.MirroredVariable):
    """Holds a map from replica to TPU variables whose values are kept in sync."""
    @property
    def device(self): ...
    def assign_sub(self, value, use_locking: bool = False, name: Incomplete | None = None, read_value: bool = True): ...
    def assign_add(self, value, use_locking: bool = False, name: Incomplete | None = None, read_value: bool = True): ...
    def assign(self, value, use_locking: bool = False, name: Incomplete | None = None, read_value: bool = True): ...
    def scatter_sub(self, *args, **kwargs): ...
    def scatter_add(self, *args, **kwargs): ...
    def scatter_max(self, *args, **kwargs): ...
    def scatter_min(self, *args, **kwargs): ...
    def scatter_mul(self, *args, **kwargs): ...
    def scatter_div(self, *args, **kwargs): ...
    def scatter_update(self, *args, **kwargs): ...

class TPUSyncOnReadVariable(TPUVariableMixin, values.SyncOnReadVariable):
    """Holds a map from replica to variables whose values are reduced on save."""
    def assign_sub(self, *args, **kwargs): ...
    def assign_add(self, *args, **kwargs): ...
    def assign(self, *args, **kwargs): ...

def assign_sub(var, value, use_locking: bool = False, name: Incomplete | None = None, read_value: bool = True): ...
def assign_add(var, value, use_locking: bool = False, name: Incomplete | None = None, read_value: bool = True): ...
def assign(var, value, use_locking: bool = False, name: Incomplete | None = None, read_value: bool = True): ...

class TPUOnWritePolicy(values.OnWritePolicy):
    """Policy defined for `tf.VariableSynchronization.ON_WRITE` synchronization.

  This policy is created when `synchronization` is set to
  `tf.VariableSynchronization.AUTO` or `tf.VariableSynchronization.ON_WRITE`.
  """
    def assign_sub(self, var, value, use_locking: bool = False, name: Incomplete | None = None, read_value: bool = True): ...
    def assign_add(self, var, value, use_locking: bool = False, name: Incomplete | None = None, read_value: bool = True): ...
    def assign(self, var, value, use_locking: bool = False, name: Incomplete | None = None, read_value: bool = True): ...
    def scatter_sub(self, var, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_add(self, var, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_max(self, var, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_min(self, var, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_mul(self, var, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_div(self, var, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_update(self, var, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...

class TPUOnReadPolicy(values.OnReadPolicy):
    """Policy defined for `tf.VariableSynchronization.ON_READ` synchronization.

  This policy is created when `synchronization` is set to
  `tf.VariableSynchronization.ON_READ` and `aggregation` is set to any of the
  values allowed by the `tf.VariableAggregation` enum such as `NONE`, `SUM`,
  `MEAN` or `ONLY_FIRST_REPLICA`when creating a `tf.Variable` in `tf.distribute`
  scope.
  """
    def assign_sub(self, var, *args, **kwargs): ...
    def assign_add(self, var, *args, **kwargs): ...
    def assign(self, var, *args, **kwargs): ...
    def scatter_sub(self, *args, **kwargs) -> None: ...
    def scatter_add(self, *args, **kwargs) -> None: ...
    def scatter_max(self, *args, **kwargs) -> None: ...
    def scatter_min(self, *args, **kwargs) -> None: ...
    def scatter_mul(self, *args, **kwargs) -> None: ...
    def scatter_div(self, *args, **kwargs) -> None: ...
    def scatter_update(self, *args, **kwargs) -> None: ...
