from _typeshed import Incomplete
from collections.abc import Generator
from tensorflow.python.distribute import distribute_lib as distribute_lib, distribute_utils as distribute_utils, values as values, values_util as values_util
from tensorflow.python.distribute.coordinator import coordinator_context as coordinator_context
from tensorflow.python.eager import context as context
from tensorflow.python.framework import dtypes as dtypes, ops as ops, tensor_conversion_registry as tensor_conversion_registry, tensor_spec as tensor_spec
from tensorflow.python.ops import array_ops as array_ops, lookup_ops as lookup_ops, resource_variable_ops as resource_variable_ops
from tensorflow.python.saved_model import save_context as save_context
from tensorflow.python.types import core as core
from tensorflow.python.util.lazy_loader import LazyLoader as LazyLoader

load_context: Incomplete
TRACKABLE_RESOURCE_METHODS: Incomplete

class AggregatingVariable(resource_variable_ops.BaseResourceVariable, core.Tensor):
    """A wrapper around a variable that aggregates updates across replicas."""
    def __init__(self, strategy, v, aggregation) -> None: ...
    def __deepcopy__(self, memo):
        """Perform a deepcopy of the `AggregatingVariable`.

    Unlike the deepcopy of a regular tf.Variable, this keeps the original
    strategy and devices of the `AggregatingVariable`.  To avoid confusion
    with the behavior of deepcopy on a regular `Variable` (which does
    copy into new devices), we only allow a deepcopy of a `AggregatingVariable`
    within its originating strategy scope.

    Args:
      memo: The memoization object for `deepcopy`.

    Returns:
      A deep copy of the current `AggregatingVariable`.

    Raises:
      RuntimeError: If trying to deepcopy into a different strategy.
    """
    def get(self): ...
    @property
    def distribute_strategy(self): ...
    def __getattr__(self, name): ...
    def assign_sub(self, *args, **kwargs): ...
    def assign_add(self, *args, **kwargs): ...
    def assign(self, *args, **kwargs): ...
    @property
    def initializer(self): ...
    def initialized_value(self): ...
    @property
    def initial_value(self): ...
    @property
    def op(self): ...
    def value(self): ...
    def read_value(self): ...
    def sparse_read(self, indices, name: Incomplete | None = None): ...
    def eval(self, session: Incomplete | None = None): ...
    @property
    def graph(self): ...
    @property
    def device(self): ...
    @property
    def shape(self): ...
    @property
    def aggregation(self): ...
    @property
    def synchronization(self): ...
    @property
    def name(self): ...
    @property
    def trainable(self): ...
    @property
    def dtype(self): ...
    def __add__(self, o): ...
    def __radd__(self, o): ...
    def __sub__(self, o): ...
    def __rsub__(self, o): ...
    def __mul__(self, o): ...
    def __rmul__(self, o): ...
    def __truediv__(self, o): ...
    def __rtruediv__(self, o): ...
    def __floordiv__(self, o): ...
    def __rfloordiv__(self, o): ...
    def __mod__(self, o): ...
    def __rmod__(self, o): ...
    def __lt__(self, o): ...
    def __le__(self, o): ...
    def __gt__(self, o): ...
    def __ge__(self, o): ...
    def __and__(self, o): ...
    def __rand__(self, o): ...
    def __or__(self, o): ...
    def __ror__(self, o): ...
    def __xor__(self, o): ...
    def __rxor__(self, o): ...
    def __getitem__(self, o): ...
    def __pow__(self, o, modulo: Incomplete | None = None): ...
    def __rpow__(self, o): ...
    def __invert__(self): ...
    def __neg__(self): ...
    def __abs__(self): ...
    def __div__(self, o): ...
    def __rdiv__(self, o): ...
    def __matmul__(self, o): ...
    def __rmatmul__(self, o): ...

class CachingVariable(resource_variable_ops.BaseResourceVariable, core.Tensor):
    """A wrapper around a variable that caches read value locally."""
    def __init__(self, v) -> None: ...
    def get(self): ...
    def __getattr__(self, name): ...
    def read_value(self): ...
    def sparse_read(self, indices, name: Incomplete | None = None): ...
    def cached_read_value(self): ...
    def assign_sub(self, *args, **kwargs): ...
    def assign_add(self, *args, **kwargs): ...
    def assign(self, *args, **kwargs): ...
    @property
    def initializer(self): ...
    def initialized_value(self): ...
    @property
    def initial_value(self): ...
    @property
    def op(self): ...
    def value(self): ...
    def eval(self, session: Incomplete | None = None): ...
    @property
    def graph(self): ...
    @property
    def device(self): ...
    @property
    def shape(self): ...
    @property
    def synchronization(self): ...
    @property
    def name(self): ...
    @property
    def trainable(self): ...
    @property
    def dtype(self): ...
    @property
    def constraint(self): ...
    def __array__(self, dtype: Incomplete | None = None): ...
    def __complex__(self) -> complex: ...
    def __int__(self) -> int: ...
    def __float__(self) -> float: ...
    def numpy(self): ...

class DistributedTable(lookup_ops.StaticHashTable):
    """A distributed StaticHashTable for ParameterServerStrategy.

  An instance of DistributedTable has copies of a StaticHashTable and its
  resource handle on the coordinator of each worker, created at the
  DistributedTable instance initialization time with initializers on each
  worker. Users can call methods on a DistributedTable as if it were a
  StaticHashTable, which leads to execution with the resource local to the
  consumer worker (or the coordinator, if calling from the coordinator). This
  implementation relies on the fact that the methods of StaticHashTable are
  queried with the resource handle (instead of the python object).

  Currently, at saving time, a DistributedTable is saved as a StaticHashTable on
  the coordinator, and restoring a DistributedTable from SavedModel is not
  supported.
  """
    def __init__(self, strategy, wrapped_creator) -> None: ...
    def __getattr__(self, attr): ...
    def resource_handle_call_time_value(self):
        """Returns a closure to run for a resource handle at call time and its spec.

    This function is called in self.resource_handle to create a placeholder
    which returns a resource handle on some worker or on the coordinator.
    """
    @property
    def resource_handle(self): ...
    @property
    def is_distributed_table(self): ...
    def __tf_experimental_restore_capture__(self, concrete_function, internal_capture): ...

def get_current_local_resource_restore_context(): ...
def with_local_resource_restore_context(instance) -> Generator[None, None, None]: ...

class LocalResourceRestoreContext:
    """Class holding information of a distributed instance, e.g. StaticHashTable.

  Pairing use with context manager `with_local_resource_restore_context` allows
  operations under this context manager to conveniently gets information of a
  component of the `RestoredDistributedTable` (and other restored distributed
  `CapturableResource` if we're supporting their distribution in the future),
  instead of looking it up from the mapping of the worker-to-resource handle.
  This is especially useful when we know which instance the operations should
  execute with and the mapping is not available yet.
  """
    instance: Incomplete
    def __init__(self, instance) -> None: ...

class RestoredDistributedTable(DistributedTable):
    """A restored and distributed StaticHashTable for ParameterServerStrategy."""
    def __init__(self, strategy, wrapped_creator) -> None: ...
    def resource_handle_call_time_value(self):
        """Returns a closure to run for a resource handle at call time and its spec.

    This function is called in self.resource_handle to create a placeholder
    which returns a resource handle on some worker or on the coordinator.
    """
    def __setattr__(self, name, value): ...
