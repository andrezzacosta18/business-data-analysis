from _typeshed import Incomplete
from tensorflow.python.distribute import device_util as device_util, tpu_util as tpu_util, values_util as values_util
from tensorflow.python.eager import context as context
from tensorflow.python.framework import ops as ops, tensor_conversion_registry as tensor_conversion_registry
from tensorflow.python.ops import control_flow_ops as control_flow_ops, resource_variable_ops as resource_variable_ops

class DistributedVariable(resource_variable_ops.BaseResourceVariable):
    """Represents variables that are replicated.

  It behaves exactly as a normal variable, but uses corresponding variable
  handle based on the context.
  - In each replica, it uses the handle from that replica.
  - In tpu.replicate(), it uses the replicated handle.
  - Otherwise, it uses the handle from the primary replica.

  Note that it doesn't synchronize automatically as the old DistributedVariable
  in values.py.
  """
    def __init__(self, variables, *, enable_packed_handle: bool = False) -> None: ...
    @property
    def handle(self): ...
    @property
    def name(self): ...
    @property
    def initializer(self): ...
    def value(self): ...
    def read_value(self): ...
    def assign_sub(self, delta, use_locking: Incomplete | None = None, name: Incomplete | None = None, read_value: bool = True): ...
    def assign_add(self, delta, use_locking: Incomplete | None = None, name: Incomplete | None = None, read_value: bool = True): ...
    def assign(self, value, use_locking: Incomplete | None = None, name: Incomplete | None = None, read_value: bool = True): ...
    def scatter_sub(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_add(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_mul(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_div(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_min(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_max(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_update(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def batch_scatter_update(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_nd_sub(self, indices, updates, name: Incomplete | None = None): ...
    def scatter_nd_add(self, indices, updates, name: Incomplete | None = None): ...
    def scatter_nd_update(self, indices, updates, name: Incomplete | None = None): ...
    def sparse_read(self, indices, name: Incomplete | None = None): ...
    def gather_nd(self, indices, name: Incomplete | None = None): ...
    def to_proto(self, export_scope: Incomplete | None = None) -> None: ...
    @staticmethod
    def from_proto(variable_def, import_scope: Incomplete | None = None) -> None: ...
    def __deepcopy__(self, memo): ...
