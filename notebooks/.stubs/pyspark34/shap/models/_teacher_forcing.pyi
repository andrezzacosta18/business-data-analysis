from .. import models as models
from .._serializable import Deserializer as Deserializer, Serializer as Serializer
from ..utils import safe_isinstance as safe_isinstance
from ..utils.transformers import getattr_silent as getattr_silent, parse_prefix_suffix_for_tokenizer as parse_prefix_suffix_for_tokenizer
from ._model import Model as Model
from _typeshed import Incomplete

class TeacherForcing(Model):
    """ Generates scores (log odds) for output text explanation algorithms using Teacher Forcing technique.

    This class supports generation of log odds for transformer models as well as functions. In model agnostic
    cases (model is function) it expects a similarity_model and similarity_tokenizer to approximate log odd scores
    for target sentence generated by the model.
    """
    tokenizer: Incomplete
    device: Incomplete
    batch_size: Incomplete
    text_generate: Incomplete
    similarity_model: Incomplete
    similarity_tokenizer: Incomplete
    model_agnostic: bool
    output: Incomplete
    output_names: Incomplete
    similarity_model_type: Incomplete
    def __init__(self, model, tokenizer: Incomplete | None = None, similarity_model: Incomplete | None = None, similarity_tokenizer: Incomplete | None = None, batch_size: int = 128, device: Incomplete | None = None) -> None:
        """ Build a teacher forcing model from the given text generation model.

        Parameters
        ----------
        model: object or function
            A object of any pretrained transformer model or function which is to be explained.

        tokenizer: object
            A tokenizer object(PreTrainedTokenizer/PreTrainedTokenizerFast) which is used to tokenize source and target sentence.

        similarity_model: object
            A pretrained transformer model object which is used in model agnostic scenario to approximate log odds.

        similarity_tokenizer: object
            A tokenizer object(PreTrainedTokenizer/PreTrainedTokenizerFast) which is used to tokenize sentence in model agnostic scenario.

        batch_size: int
            Batch size for model inferencing and computing logodds (default=128).

        device: str
            By default, it infers if system has a gpu and accordingly sets device. Should be 'cpu' or 'cuda' or pytorch models.

        Returns
        -------
        numpy.ndarray
            The scores (log odds) of generating target sentence ids using the model.
        """
    def __call__(self, X, Y):
        """ Computes log odds scores of generating output(text) for a given batch of input(text/image) .

        Parameters
        ----------
        X: numpy.ndarray
            An array containing a list of masked inputs.

        Y: numpy.ndarray
            An array containing a list of target sentence/ids.

        Returns
        -------
        numpy.ndarray
            A numpy array of log odds scores for every input pair (masked_X, X)
        """
    def update_output_names(self, output) -> None:
        """ The function updates output tokens.

        It mimics the caching mechanism to update the output tokens for every
        new row of explanation that are to be explained.

        Parameters
        ----------
        output: numpy.ndarray
            Output(sentence/sentence ids) for an explanation row.
        """
    def get_output_names(self, output):
        """ Gets the output tokens by computing the output sentence ids and output names using the similarity_tokenizer.

        Parameters
        ----------
        output: numpy.ndarray
            Output(sentence/sentence ids) for an explanation row.

        Returns
        -------
        list
            A list of output tokens.
        """
    def get_outputs(self, X):
        """ The function tokenizes output sentences and returns ids.

        Parameters
        ----------
        X: numpy.ndarray
            Output(sentence/sentence ids) for an explanation row.

        Returns
        -------
        numpy.ndarray
            An array of output(target sentence) ids.
        """
    def get_inputs(self, X, padding_side: str = 'right'):
        ''' The function tokenizes source sentences.

        In model agnostic case, the function calls model(X) which is expected to
        return a batch of output sentences which is tokenized to compute inputs.

        Parameters
        ----------
        X: numpy.ndarray
            X could be a batch of text or images(model agnostic case).

        Returns
        -------
        dict
            Dictionary of padded source sentence ids and attention mask as tensors("pt" or "tf" based on similarity_model_type).
        '''
    def get_logodds(self, logits):
        """ Calculates log odds from logits.

        This function passes the logits through softmax and then computes log odds for the output(target sentence) ids.

        Parameters
        ----------
        logits: numpy.ndarray
            An array of logits generated from the model.

        Returns
        -------
        numpy.ndarray
            Computes log odds for corresponding output ids.
        """
    def model_inference(self, inputs, output_ids):
        """ This function performs model inference for tensorflow and pytorch models.

        Parameters
        ----------
        inputs: dict
            Dictionary of padded source sentence ids and attention mask as tensors.

        output_ids: numpy.ndarray
            An array of decoder output ids.

        Returns
        -------
        numpy.ndarray
            Returns output logits from the model.
        """
    def get_teacher_forced_logits(self, X, Y):
        """ The function generates logits for transformer models.

        It generates logits for encoder-decoder models as well as decoder only models by using the teacher forcing technique.

        Parameters
        ----------
        X: numpy.ndarray
            An array containing a list of masked inputs.

        Y: numpy.ndarray
            An array containing a list of target sentence/ids.

        Returns
        -------
        numpy.ndarray
            Decoder output logits for output(target sentence) ids.
        """
    def save(self, out_file) -> None: ...
    @classmethod
    def load(cls, in_file, instantiate: bool = True): ...
