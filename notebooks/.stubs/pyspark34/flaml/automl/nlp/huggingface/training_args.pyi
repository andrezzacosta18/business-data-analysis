from dataclasses import dataclass
from flaml.automl.task.task import NLG_TASKS as NLG_TASKS
from transformers import TrainingArguments
from typing import List

TrainingArguments = object

@dataclass
class TrainingArgumentsForAuto(TrainingArguments):
    '''FLAML custom TrainingArguments.

    Args:
        task (str): the task name for NLP tasks, e.g., seq-classification, token-classification
        output_dir (str): data root directory for outputing the log, etc.
        model_path (str, optional, defaults to "facebook/muppet-roberta-base"): A string,
            the path of the language model file, either a path from huggingface
            model card huggingface.co/models, or a local path for the model.
        fp16 (bool, optional, defaults to "False"): A bool, whether to use FP16.
        max_seq_length (int, optional, defaults to 128): An integer, the max length of the sequence.
            For token classification task, this argument will be ineffective.
        pad_to_max_length (bool, optional, defaults to "False"):
            whether to pad all samples to model maximum sentence length.
            If False, will pad the samples dynamically when batching to the maximum length in the batch.
        per_device_eval_batch_size (int, optional, defaults to 1): An integer, the per gpu evaluation batch size.
        label_list (List[str], optional, defaults to None): A list of string, the string list of the label names.
            When the task is sequence labeling/token classification, there are two formats of the labels:
            (1) The token labels, i.e., [B-PER, I-PER, B-LOC]; (2) Id labels. For (2), need to pass the label_list (e.g., [B-PER, I-PER, B-LOC])
            to convert the Id to token labels when computing the metric with metric_loss_score.
            See the example in [a simple token classification example](/docs/Examples/AutoML-NLP#a-simple-token-classification-example).
    '''
    task: str = ...
    output_dir: str = ...
    model_path: str = ...
    fp16: bool = ...
    max_seq_length: int = ...
    label_all_tokens: bool = ...
    pad_to_max_length: bool = ...
    per_device_eval_batch_size: int = ...
    label_list: List[str] | None = ...
    eval_steps: int = ...
    save_steps: int = ...
    logging_steps: int = ...
    @staticmethod
    def load_args_from_console(): ...
    def __init__(self, *selftaskoutput_dirmodel_pathfp16max_seq_lengthlabel_all_tokenspad_to_max_lengthper_device_eval_batch_sizelabel_listeval_stepssave_stepslogging_steps_, task, output_dir, model_path, fp16, max_seq_length, label_all_tokens, pad_to_max_length, per_device_eval_batch_size, label_list, eval_steps, save_steps, logging_steps, **selftaskoutput_dirmodel_pathfp16max_seq_lengthlabel_all_tokenspad_to_max_lengthper_device_eval_batch_sizelabel_listeval_stepssave_stepslogging_steps__) -> None: ...

@dataclass
class Seq2SeqTrainingArgumentsForAuto(TrainingArgumentsForAuto):
    model_path: str = ...
    sortish_sampler: bool = ...
    predict_with_generate: bool = ...
    generation_max_length: int | None = ...
    generation_num_beams: int | None = ...
    def __post_init__(self) -> None: ...
    def __init__(self, *selftaskoutput_dirmodel_pathfp16max_seq_lengthlabel_all_tokenspad_to_max_lengthper_device_eval_batch_sizelabel_listeval_stepssave_stepslogging_stepssortish_samplerpredict_with_generategeneration_max_lengthgeneration_num_beams_, task, output_dir, model_path, fp16, max_seq_length, label_all_tokens, pad_to_max_length, per_device_eval_batch_size, label_list, eval_steps, save_steps, logging_steps, sortish_sampler, predict_with_generate, generation_max_length, generation_num_beams, **selftaskoutput_dirmodel_pathfp16max_seq_lengthlabel_all_tokenspad_to_max_lengthper_device_eval_batch_sizelabel_listeval_stepssave_stepslogging_stepssortish_samplerpredict_with_generategeneration_max_lengthgeneration_num_beams__) -> None: ...
