import enum
import inspect
import numpy as np
import types
import typing
from . import jax_jit as jax_jit, mlir as mlir, ops as ops, outfeed_receiver as outfeed_receiver, pmap_lib as pmap_lib, profiler as profiler, pytree as pytree, transfer_guard_lib as transfer_guard_lib
from _typeshed import Incomplete
from typing import Any, Callable, ClassVar, Dict, List, Sequence, Tuple, Type, overload

class XlaRuntimeError(RuntimeError): ...

class PrimitiveType(enum.IntEnum):
    PRIMITIVE_TYPE_INVALID: PrimitiveType
    PRED: PrimitiveType
    S8: PrimitiveType
    S16: PrimitiveType
    S32: PrimitiveType
    S64: PrimitiveType
    U8: PrimitiveType
    U16: PrimitiveType
    U32: PrimitiveType
    U64: PrimitiveType
    F8_E4M3FN: PrimitiveType
    F8_E4M3B11FNUZ: PrimitiveType
    F8_E4M3FNUZ: PrimitiveType
    F8_E5M2: PrimitiveType
    F8_E5M2FNUZ: PrimitiveType
    BF16: PrimitiveType
    F16: PrimitiveType
    F32: PrimitiveType
    F64: PrimitiveType
    C64: PrimitiveType
    C128: PrimitiveType
    TUPLE: PrimitiveType
    OPAQUE_TYPE: PrimitiveType
    TOKEN: PrimitiveType

class Shape:
    def __init__(self, s: str) -> None: ...
    @staticmethod
    def tuple_shape(shapes: Sequence[Shape]) -> Shape: ...
    @staticmethod
    def array_shape(type: np.dtype | PrimitiveType, dims_seq: Any = ..., layout_seq: Any = ..., dynamic_dimensions: List[bool] | None = ...) -> Shape: ...
    @staticmethod
    def token_shape() -> Shape: ...
    @staticmethod
    def scalar_shape(type: np.dtype | PrimitiveType) -> Shape: ...
    def dimensions(self) -> Tuple[int, ...]: ...
    def xla_element_type(self) -> PrimitiveType: ...
    def element_type(self) -> np.dtype: ...
    def numpy_dtype(self) -> np.dtype: ...
    def is_tuple(self) -> bool: ...
    def is_array(self) -> bool: ...
    def is_token(self) -> bool: ...
    def is_static(self) -> bool: ...
    def is_dynamic(self) -> bool: ...
    def is_dynamic_dimension(self, dimension: int) -> bool: ...
    def set_dynamic_dimension(self, dimension: int, is_dynamic: bool) -> None: ...
    def rank(self) -> int: ...
    def to_serialized_proto(self) -> bytes: ...
    def tuple_shapes(self) -> List[Shape]: ...
    def leaf_count(self) -> int: ...
    def with_major_to_minor_layout_if_absent(self) -> Shape: ...
    def __eq__(self, other: Shape) -> bool: ...
    def __ne__(self, other: Shape) -> bool: ...
    def __hash__(self) -> int: ...

class Layout:
    def minor_to_major(self) -> Tuple[int, ...]: ...
    def to_string(self) -> str: ...
    def __eq__(self, other: Layout) -> bool: ...
    def __ne__(self, other: Layout) -> bool: ...
    def __hash__(self) -> int: ...

class ProgramShape:
    def __init__(self, params: Sequence[Shape], result: Shape) -> None: ...
    def parameter_shapes(self) -> List[Shape]: ...
    def result_shape(self) -> Shape: ...

class ShapeIndex:
    def __init__(self, indices: List[int]) -> None: ...
    def __eq__(self, other: Shape) -> bool: ...
    def __ne__(self, other: Shape) -> bool: ...
    def __hash__(self) -> int: ...

class Literal: ...

class XlaComputation:
    def __init__(self, serialized_hlo_module_proto: bytes) -> None: ...
    def get_hlo_module(self) -> HloModule: ...
    def program_shape(self) -> ProgramShape: ...
    def as_serialized_hlo_module_proto(self) -> bytes: ...
    def as_hlo_text(self, print_large_constants: bool = False) -> str: ...
    def as_hlo_dot_graph(self) -> str: ...
    def hash(self) -> int: ...
    def as_hlo_module(self) -> HloModule: ...

class HloPrintOptions:
    def __init__(self) -> None: ...
    @staticmethod
    def short_parsable() -> HloPrintOptions: ...
    @staticmethod
    def canonical() -> HloPrintOptions: ...
    @staticmethod
    def fingerprint() -> HloPrintOptions: ...
    print_large_constants: bool
    print_metadata: bool
    print_backend_config: bool
    print_result_shape: bool
    print_operand_shape: bool
    print_operand_names: bool
    print_ids: bool
    print_extra_attributes: bool
    print_program_shape: bool
    print_percent: bool
    print_control_dependencies: bool
    compact_operands: bool
    include_layout_in_shapes: bool
    canonicalize_instruction_names: bool
    canonicalize_computations: bool
    indent_amount: int
    is_in_nested_computation: bool

class HloModule:
    spmd_output_sharding: OpSharding | None
    spmd_parameters_shardings: List[OpSharding] | None
    @property
    def name(self) -> str: ...
    def to_string(self, options: HloPrintOptions = ...) -> str: ...
    def as_serialized_hlo_module_proto(self) -> bytes: ...
    @staticmethod
    def from_serialized_hlo_module_proto(serialized_hlo_module_proto: bytes) -> HloModule: ...

class HloModuleGroup:
    def __init__(self, name: str, modules: List[HloModule]) -> None: ...
    @property
    def name(self) -> str: ...
    def to_string(self) -> str: ...
    def to_modules(self) -> List[HloModule]: ...

def hlo_module_to_dot_graph(hlo_module: HloModule) -> str: ...
def hlo_module_from_text(hlo_module_text: str) -> HloModule: ...
def hlo_module_cost_analysis(client: Client, module: HloModule) -> Dict[str, float]: ...

class XlaOp: ...

class XlaBuilder:
    def __init__(self, name: str) -> None: ...
    def Build(self, root: XlaOp | None = ...) -> XlaComputation: ...
    def GetShape(self, __op: XlaOp) -> Shape: ...
    build = Build
    def clear_op_metadata(self) -> None: ...
    get_shape = GetShape
    def get_program_shape(self, root: XlaOp | None = ...) -> ProgramShape: ...
    def is_constant(self, __op: XlaOp) -> bool: ...
    def set_op_metadata(self, metadata: _XlaOpMetadata) -> None: ...
    def set_sharding(self, sharding: OpSharding_Type) -> None: ...
    def clear_sharding(self) -> None: ...
    def setup_alias(self, __output_index: Sequence[int], __param_number: int, __param_index: Sequence[int]) -> None: ...

class DeviceAssignment:
    @staticmethod
    def create(array: np.ndarray) -> DeviceAssignment: ...
    def replica_count(self) -> int: ...
    def computation_count(self) -> int: ...
    def serialize(self) -> bytes: ...

class CompileOptions:
    @staticmethod
    def ParseFromString(s: bytes) -> CompileOptions: ...
    def __init__(self) -> None: ...
    def SerializeAsString(self) -> bytes: ...
    argument_layouts: List[Shape] | None
    parameter_is_tupled_arguments: bool
    executable_build_options: ExecutableBuildOptions
    tuple_arguments: bool
    num_replicas: int
    num_partitions: int
    profile_version: int
    device_assignment: DeviceAssignment | None
    compile_portable_executable: bool
    env_option_overrides: List[Tuple[str, str]]

def register_custom_call_target(fn_name: str, capsule: Any, platform: str) -> _Status: ...
def register_custom_call_partitioner(name: str, prop_user_sharding: Callable, partition: Callable, infer_sharding_from_operands: Callable, can_side_effecting_have_replicated_sharding: bool) -> None: ...
def encode_inspect_sharding_callback(handler: Any) -> bytes: ...

class DebugOptions:
    xla_cpu_enable_fast_math: bool
    xla_cpu_fast_math_honor_infs: bool
    xla_cpu_fast_math_honor_nans: bool
    xla_cpu_fast_math_honor_division: bool
    xla_cpu_fast_math_honor_functions: bool
    xla_gpu_enable_fast_min_max: bool
    xla_backend_optimization_level: int
    xla_cpu_enable_xprof_traceme: bool
    xla_llvm_disable_expensive_passes: bool
    xla_test_all_input_layouts: bool
    xla_disable_hlo_passes: str
    xla_enable_hlo_passes_only: str

class CompiledMemoryStats:
    generated_code_size_in_bytes: int
    argument_size_in_bytes: int
    output_size_in_bytes: int
    alias_size_in_bytes: int
    temp_size_in_bytes: int
    serialized_hlo_proto: bytes

class ExecutableBuildOptions:
    def __init__(self) -> None: ...
    result_layout: Shape | None
    fdo_profile: bytes | None
    num_replicas: int
    num_partitions: int
    debug_options: DebugOptions
    device_assignment: DeviceAssignment | None
    use_spmd_partitioning: bool
    use_auto_spmd_partitioning: bool
    auto_spmd_partitioning_mesh_shape: List[int]
    auto_spmd_partitioning_mesh_ids: List[int]

class PrecisionConfig_Precision(enum.IntEnum):
    DEFAULT: int
    HIGH: int
    HIGHEST: int

class OpSharding_Type(enum.IntEnum):
    REPLICATED: int
    MAXIMAL: int
    TUPLE: int
    OTHER: int
    MANUAL: int

class OpSharding:
    Type: typing.Type[OpSharding_Type]
    type: OpSharding_Type
    replicate_on_last_tile_dim: bool
    last_tile_dims: Sequence[Type]
    tile_assignment_dimensions: Sequence[int]
    tile_assignment_devices: Sequence[int]
    iota_reshape_dims: Sequence[int]
    iota_transpose_perm: Sequence[int]
    tuple_shardings: Sequence[OpSharding]
    def ParseFromString(self, s: bytes) -> None: ...
    def SerializeToString(self) -> bytes: ...
    def clone(self) -> OpSharding: ...

class HloSharding:
    @staticmethod
    def from_proto(proto: OpSharding) -> HloSharding: ...
    @staticmethod
    def from_string(sharding: str) -> HloSharding: ...
    @staticmethod
    def tuple_sharding(shape: Shape, shardings: Sequence[HloSharding]) -> HloSharding: ...
    @staticmethod
    def iota_tile(dims: Sequence[int], reshape_dims: Sequence[int], transpose_perm: Sequence[int], subgroup_types: Sequence[OpSharding.Type]) -> HloSharding: ...
    @staticmethod
    def replicate() -> HloSharding: ...
    @staticmethod
    def manual() -> HloSharding: ...
    def __eq__(self, other: HloSharding) -> bool: ...
    def __hash__(self) -> int: ...
    def tile(self, shape: Shape) -> Shape: ...
    def is_replicated(self) -> bool: ...
    def is_manual(self) -> bool: ...
    def is_tiled(self) -> bool: ...
    def tuple_elements(self) -> List[HloSharding]: ...
    def num_devices(self) -> int: ...
    def num_dimensions(self) -> int: ...
    def tile_assignment_dimensions(self) -> Sequence[int]: ...
    def tile_assignment_devices(self) -> Sequence[int]: ...
    def subgroup_types(self) -> Sequence[OpSharding.Type]: ...
    def replicate_on_last_tile_dim(self) -> bool: ...
    def to_proto(self) -> OpSharding: ...

class FftType(enum.IntEnum):
    FFT: int
    IFFT: int
    RFFT: int
    IRFFT: int

class Device:
    id: int
    host_id: int
    process_index: int
    platform: str
    device_kind: str
    client: Client
    def transfer_to_infeed(self, literal: _LiteralSlice): ...
    def transfer_from_outfeed(self, shape: Shape): ...
    def memory(self, kind: str) -> Memory: ...
    def default_memory(self) -> Memory: ...
    def addressable_memories(self) -> List[Memory]: ...
    def live_buffers(self) -> List[Any]: ...
    def memory_stats(self) -> Dict[str, int] | None: ...
    def __getattr__(self, name: str) -> Any: ...

class Memory:
    process_index: int
    platform: str
    kind: str
    def attached_devices(self) -> List[Device]: ...

class _GpuAllocatorKind(enum.IntEnum):
    DEFAULT: int
    PLATFORM: int
    BFC: int
    CUDA_ASYNC: int

class GpuAllocatorConfig:
    Kind: Incomplete
    def __init__(self, kind: _GpuAllocatorKind = ..., memory_fraction: float = ..., preallocate: bool = ...) -> None: ...

class HostBufferSemantics(enum.IntEnum):
    IMMUTABLE_ONLY_DURING_CALL: HostBufferSemantics
    IMMUTABLE_UNTIL_TRANSFER_COMPLETES: HostBufferSemantics
    ZERO_COPY: HostBufferSemantics

class Client:
    platform: str
    platform_version: str
    runtime_type: str
    def device_count(self) -> int: ...
    def local_device_count(self) -> int: ...
    def devices(self) -> List[Device]: ...
    def local_devices(self) -> List[Device]: ...
    def live_buffers(self) -> List[Any]: ...
    def live_arrays(self) -> List[ArrayImpl]: ...
    def live_executables(self) -> List[LoadedExecutable]: ...
    def host_id(self) -> int: ...
    def process_index(self) -> int: ...
    @overload
    def get_default_device_assignment(self, num_replicas: int, num_partitions: int) -> List[List[Device]]: ...
    @overload
    def get_default_device_assignment(self, num_replicas: int) -> List[Device]: ...
    def buffer_from_pyval(self, argument: Any, device: Device | None = ..., force_copy: bool = ..., host_buffer_semantics: HostBufferSemantics = ...) -> ArrayImpl: ...
    def make_cross_host_receive_buffers(self, shapes: Sequence[Shape], device: Device) -> List[Tuple[ArrayImpl, bytes]]: ...
    def compile(self, computation: str | bytes, compile_options: CompileOptions = ..., host_callbacks: Sequence[Any] = ...) -> LoadedExecutable: ...
    def serialize_executable(self, executable: LoadedExecutable) -> bytes: ...
    def deserialize_executable(self, serialized: bytes, options: CompileOptions | None, host_callbacks: Sequence[Any] = ...) -> LoadedExecutable: ...
    def heap_profile(self) -> bytes: ...
    def defragment(self) -> _Status: ...
    def get_emit_python_callback_descriptor(self, callable: Callable, operand_shapes: Sequence[Shape], results_shapes: Sequence[Shape]) -> Tuple[Any, Any]: ...
    def emit_python_callback(self, callable: Callable, builder: XlaBuilder, operands: Sequence[XlaOp], results_shapes: Sequence[Shape], operand_layouts: Sequence[Shape] | None = ..., has_side_effects: bool = ...) -> Tuple[XlaOp, Any]: ...
    def make_python_callback_from_host_send_and_recv(self, callable: Callable, operand_shapes: Sequence[Shape], result_shapes: Sequence[Shape], send_channel_ids: Sequence[int], recv_channel_ids: Sequence[int], serializer: Callable | None = ...) -> Any: ...
    def get_python_callback_from_host_send(callable: Any, operand_shapes: Any, send_channel_ids: Any, recv_channel_ids: Any) -> Any: ...

def get_tfrt_cpu_client(asynchronous: bool = ...) -> Client: ...
def get_interpreter_client() -> Client: ...
def get_gpu_client(asynchronous: bool = ..., allocator_config: GpuAllocatorConfig = ..., distributed_client: DistributedRuntimeClient | None = ..., node_id: int = ..., allowed_devices: Any | None = ..., platform_name: str | None = ...) -> Client: ...
def get_tpu_client(max_inflight_computations: int = ...) -> Client: ...
def get_c_api_client(platform_name: str, options: Dict[str, str | int | List[int] | float]) -> Client: ...
def get_default_c_api_topology(platform_name: str, topology_name: str, options: Dict[str, str | int | List[int] | float]) -> DeviceTopology: ...
def load_pjrt_plugin(platform_name: str, library_path: str) -> _Status: ...
def pjrt_plugin_loaded(plugin_name: str) -> bool: ...
ArrayImpl = Any

def copy_array_to_devices_with_sharding(self, devices: List[Device], sharding: Any) -> ArrayImpl: ...
def batched_device_put(aval: Any, sharding: Any, shards: Sequence[Any], devices: List[Device], committed: bool = True) -> ArrayImpl: ...
def array_result_handler(aval: Any, sharding: Any, committed: bool, _skip_checks: bool = ...) -> Callable: ...

class Token:
    def block_until_ready(self) -> None: ...

class ShardedToken:
    def block_until_ready(self) -> None: ...
    def get_token(self, device_id: int): ...

class ExecuteResults:
    def __len__(self) -> int: ...
    def disassemble_into_single_device_arrays(self) -> List[List[ArrayImpl]]: ...
    def disassemble_prefix_into_single_device_arrays(self, n: int) -> List[List[ArrayImpl]]: ...
    def consume_with_handlers(self, handlers: List[Callable]) -> List[Any]: ...
    def consume_token(self) -> ShardedToken: ...

class LoadedExecutable:
    client: Client
    def local_logical_device_ids(self) -> List[Tuple[int, int]]: ...
    def local_devices(self) -> List[Device]: ...
    def size_of_generated_code_in_bytes(self) -> int: ...
    def delete(self) -> None: ...
    def execute(self, arguments: Sequence[ArrayImpl]) -> List[ArrayImpl]: ...
    def execute_with_token(self, arguments: Sequence[ArrayImpl]) -> Tuple[List[ArrayImpl], Token]: ...
    def execute_sharded_on_local_devices(self, arguments: Sequence[List[ArrayImpl]]) -> List[List[ArrayImpl]]: ...
    def execute_sharded_on_local_devices_with_tokens(self, arguments: Sequence[List[ArrayImpl]]) -> Tuple[List[List[ArrayImpl]], ShardedToken]: ...
    def execute_sharded(self, arguments: Sequence[List[ArrayImpl]], with_tokens: bool = ...) -> ExecuteResults: ...
    def hlo_modules(self) -> List[HloModule]: ...
    def get_output_memory_kinds(self) -> List[List[str]]: ...
    def get_compiled_memory_stats(self) -> CompiledMemoryStats: ...
    def keep_alive(self) -> None: ...
    def compile_options(self) -> CompileOptions: ...
    def cost_analysis(self) -> Dict[str, Any]: ...
    traceback: Traceback
    fingerprint: bytes | None

class Executable:
    def hlo_modules(self) -> List[HloModule]: ...
    def get_output_memory_kinds(self) -> List[List[str]]: ...
    def get_output_shardings(self) -> List[OpSharding] | None: ...
    def get_parameter_shardings(self) -> List[OpSharding] | None: ...
    def get_compiled_memory_stats(self) -> CompiledMemoryStats: ...
    def serialize(self) -> str: ...
    def compile_options(self) -> CompileOptions: ...

class DeviceTopology:
    platform: str
    platform_version: str

def buffer_to_dlpack_managed_tensor(buffer: ArrayImpl, take_ownership: bool = ...) -> Any: ...
def dlpack_managed_tensor_to_buffer(tensor: Any, cpu_backend: Client | None = ..., gpu_backend: Client | None = ...) -> ArrayImpl: ...

class Frame:
    file_name: str
    function_name: str
    function_line_start: int
    line_num: int

class Traceback:
    enabled: ClassVar[bool]
    @staticmethod
    def get_traceback() -> Traceback: ...
    frames: Sequence[Frame]
    def as_python_traceback(self) -> Any: ...
    def raw_frames(self) -> Tuple[List[types.CodeType], List[int]]: ...
    @staticmethod
    def code_addr2line(code: types.CodeType, lasti: int) -> int: ...
    @staticmethod
    def code_addr2location(code: types.CodeType, lasti: int) -> Tuple[int, int, int, int]: ...

def replace_thread_exc_traceback(traceback: Any): ...

class DistributedRuntimeService:
    def shutdown(self) -> None: ...

class DistributedRuntimeClient:
    def connect(self) -> _Status: ...
    def shutdown(self) -> _Status: ...
    def blocking_key_value_get(self, key: str, timeout_in_ms: int) -> _Status: ...
    def blocking_key_value_get_bytes(self, key: str, timeout_in_ms: int) -> _Status: ...
    def key_value_dir_get(self, key: str) -> _Status: ...
    def key_value_dir_get_bytes(self, key: str) -> _Status: ...
    def key_value_set(self, key: str, value: str) -> _Status: ...
    def key_value_delete(self, key: str) -> _Status: ...
    def wait_at_barrier(self, barrier_id: str, timeout_in_ms: int) -> _Status: ...

def get_distributed_runtime_service(address: str, num_nodes: int, use_coordination_service: bool = ..., heartbeat_interval: int | None = ..., max_missing_heartbeats: int | None = ..., enumerate_devices_timeout: int | None = ..., shutdown_timeout: int | None = ...) -> DistributedRuntimeService: ...
def get_distributed_runtime_client(address: str, node_id: int, use_coordination_service: bool = ..., rpc_timeout: int | None = ..., init_timeout: int | None = ..., shutdown_timeout: int | None = ..., heartbeat_interval: int | None = ..., max_missing_heartbeats: int | None = ..., missed_heartbeat_callback: Any | None = ..., shutdown_on_destruction: bool | None = ...) -> DistributedRuntimeClient: ...

class PreemptionSyncManager:
    def initialize(self, client: DistributedRuntimeClient) -> _Status: ...
    def reached_sync_point(self, step_counter: int) -> bool: ...

def create_preemption_sync_manager() -> PreemptionSyncManager: ...
def collect_garbage() -> None: ...
def is_optimized_build() -> bool: ...
def json_to_pprof_profile(json: str) -> bytes: ...
def pprof_profile_to_json(proto: bytes) -> str: ...
CompiledFunction = Any

class PmapFunction:
    def __call__(self, *args, **kwargs) -> Any: ...
    __signature__: inspect.Signature

def weakref_lru_cache(cache_context_fn: Callable, call: Callable, maxsize=...): ...

class Sharding: ...
class XLACompatibleSharding(Sharding): ...

class NamedSharding(XLACompatibleSharding):
    def __init__(self, mesh: Any, spec: Any, *, memory_kind: str | None = None, _parsed_pspec: Any = None) -> None: ...
    mesh: Any
    spec: Any
    memory_kind: str | None

class SingleDeviceSharding(XLACompatibleSharding):
    def __init__(self, device: Device, *, memory_kind: str | None = None) -> None: ...

class PmapSharding(XLACompatibleSharding):
    def __init__(self, devices: Sequence[Any], sharding_spec: pmap_lib.ShardingSpec) -> None: ...
    devices: List[Any]
    sharding_spec: pmap_lib.ShardingSpec

class GSPMDSharding(XLACompatibleSharding):
    def __init__(self, devices: Sequence[Device], op_sharding: OpSharding | HloSharding, *, memory_kind: str | None = None) -> None: ...

class PjitFunction:
    def __call__(self, *args, **kwargs) -> Any: ...

class PjitFunctionCache:
    def __init__(self, capacity: int = ...) -> None: ...
    def size(self) -> int: ...
    def capacity(self) -> int: ...
    def clear(self) -> None: ...
    @staticmethod
    def clear_all() -> None: ...

def pjit(function_name: str, fun: Callable | None, cache_miss: Callable, static_argnums: Sequence[int], static_argnames: Sequence[str], donate_argnums: Sequence[int], pytree_registry: pytree.PyTreeRegistry, cache: PjitFunctionCache | None = ...) -> PjitFunction: ...

class HloPassInterface:
    @property
    def name(self) -> str: ...
    def is_pass_pipeline(self) -> bool: ...
    def run(self, module: HloModule) -> bool: ...
    def run_on_module_group(self, module_group: HloModuleGroup) -> bool: ...

class HloDCE(HloPassInterface):
    def __init__(self) -> None: ...

class CallInliner(HloPassInterface):
    def __init__(self) -> None: ...

class FlattenCallGraph(HloPassInterface):
    def __init__(self) -> None: ...

class TupleSimplifer(HloPassInterface):
    def __init__(self) -> None: ...

def is_asan() -> bool: ...
def is_msan() -> bool: ...
def is_tsan() -> bool: ...
def is_sanitized() -> bool: ...
