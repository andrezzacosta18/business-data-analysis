import torch
from _typeshed import Incomplete
from torch import Tensor as Tensor
from torch._decomp import global_decomposition_table as global_decomposition_table, meta_table as meta_table
from torch._ops import OpOverload as OpOverload
from torch._prims import ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND as ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND as ELEMENTWISE_TYPE_PROMOTION_KIND, IntLike as IntLike, check as check, corresponding_complex_dtype as corresponding_complex_dtype, corresponding_real_dtype as corresponding_real_dtype, elementwise_dtypes as elementwise_dtypes, make_contiguous_strides_for as make_contiguous_strides_for
from torch._prims_common.wrappers import out_wrapper as out_wrapper
from torch._subclasses.fake_tensor import check_no_bool_index_tensors as check_no_bool_index_tensors
from torch.utils._pytree import tree_map as tree_map
from typing import List

aten: Incomplete

def register_meta(op): ...
def toRealValueType(dtype): ...
def meta_fft_c2c(self, dim, normalization, forward): ...
def meta_fft_r2c(self, dim, normalization, onesided): ...
def meta_randperm(n, *, generator: Incomplete | None = None, out): ...
def meta_randint(high, size, *, dtype=..., layout: Incomplete | None = None, device: Incomplete | None = None, pin_memory: Incomplete | None = None): ...
def meta_randint_low(low, high, size, *, dtype=..., layout: Incomplete | None = None, device: Incomplete | None = None, pin_memory: Incomplete | None = None): ...
def meta_rand_default(size, *, dtype: Incomplete | None = None, layout: Incomplete | None = None, device: Incomplete | None = None, pin_memory: Incomplete | None = None): ...
def meta_fft_c2r(self, dim, normalization, lastdim): ...
def meta_copy_(self, src, non_blocking: bool = False): ...
def inferUnsqueezeGeometry(tensor, dim): ...
def meta_unsqueeze_(self, dim): ...
def meta_index_select(self, dim, index): ...
def meta_index_select_out(self, dim, index, out): ...
def meta_max(self): ...
def meta_max_dim(self, dim, keepdim: bool = False): ...
def meta_min(self): ...
def meta_angle(self): ...
def meta_angle_out(self, out): ...
def squareCheckInputs(self, f_name: str): ...
def checkFloatingOrComplex(t: Tensor, f_name: str, allow_low_precision_dtypes: bool = True): ...
def checkIsMatrix(A: Tensor, f_name: str, arg_name: str = 'A'): ...
def checkUplo(uplo: str): ...
def meta_linalg_eigh(self, uplo: str = 'L'): ...
def linalg_cholesky_ex(A: Tensor, upper: bool = False, check_errors: bool = False): ...
def linalg_inv_ex_meta(A: Tensor, check_errors: bool = False): ...
def meta_pad2d_backward(grad_output, self, padding): ...
def meta_pad2d(self, padding): ...
def meta_bernoulli(self, *, generator: Incomplete | None = None): ...
def meta_bernoulli_(self, p: float = 0.5, generator: Incomplete | None = None): ...
def meta_bernoulli_p(self, p: float = 0.5, generator: Incomplete | None = None): ...
def meta__fused_moving_avg_obs_fq_helper(self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant: bool = False, symmetric_quant: bool = False): ...
def dot_check(self, other): ...
def meta_dot(self, tensor): ...
def meta_mm(a, b): ...
def device_hint(tensor) -> str: ...
def calc_conv_nd_return_shape(input_tensor: torch.Tensor, weight: torch.Tensor, stride: List[int] | int, padding: List[int] | int, dilation: List[int] | int, is_transposed: bool, groups: int, output_padding: List[int] | int | None = None): ...
def is_channels_last(ten): ...
def meta_conv(input_tensor: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, stride: List[int], padding: List[int], dilation: List[int], is_transposed: bool, output_padding: List[int], groups: int): ...
def pick_mkldnn_conv_memory_format(input_tensor, weight): ...
def meta_mkldnn_convolution_default(input_tensor, weight, bias, padding, stride, dilation, groups, attr, scalars, algorithm): ...
def meta_mkldnn_convolution_binary(input_tensor, other, weight, bias, padding, stride, dilation, groups, binary_attr, alpha, unary_attr, unary_scalars, unary_algorithm): ...
def meta_mkldnn_convolution_binary_inplace(input_tensor, other, weight, bias, padding, stride, dilation, groups, binary_attr, alpha, unary_attr, unary_scalars, unary_algorithm): ...
def meta_linear_pointwise_default(input_tensor, weight, bias, attr, scalars, algorithm): ...
def meta_linear_pointwise_binary(input_tensor, other, weight, bias, attr): ...
def meta_mkl_linear(input_tensor, packed_weight, orig_weight, bias, batch_size): ...
def check_dim_size(tensor, dim, dim_size, size): ...
def meta_avg_pool2d(input, kernel_size, stride=(), padding=(0,), ceil_mode: bool = False, count_include_pad: bool = True, divisor_override: Incomplete | None = None): ...
def avg_pool2d_backward_shape_check(input, gradOutput, nbatch, kH, kW, dH, dW, padH, padW, nInputPlane, inputHeight, inputWidth, outputHeight, outputWidth, mem_format) -> None: ...
def meta_avg_pool2d_backward(gradOutput_, input, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override): ...
def meta_adaptive_avg_pool2d(self, output_size): ...
def meta_adaptive_avg_pool3d(self, output_size): ...
def meta__adaptive_avg_pool2d_backward(grad_out, self): ...
def meta_repeat_interleave_Tensor(repeats, output_size: Incomplete | None = None): ...
def meta_complex(real, imag): ...
def vdot(self, other): ...
def meta_index_Tensor(self, indices): ...
def meta_convolution_backward(grad_output_, input_, weight_, bias_sizes_opt, stride, padding, dilation, transposed, output_padding, groups, output_mask): ...
def meta_addbmm(self, batch1, batch2, *, beta: int = 1, alpha: int = 1): ...
def meta_cdist_forward(x1, x2, p, compute_mode): ...
def meta_embedding_bag(weight, indices, offsets, scale_grad_by_freq: bool = False, mode: int = 0, sparse: bool = False, per_sample_weights: Incomplete | None = None, include_last_offset: bool = False, padding_idx: int = -1): ...
def meta_embedding_bag_forward_only(weight, indices, offsets, *args): ...
def meta_nansum(input, dims: Incomplete | None = None, keepdim: bool = False, *, dtype: Incomplete | None = None): ...
def meta_nanmedian(input): ...
def meta_nanmedian_dim(input, dim: int = -1, keepdim: bool = False): ...
def meta_logical_not_(self): ...
def meta_repeat(self, repeats): ...
def meta_zero_(self): ...
def meta_binop_inplace(self, other): ...
def meta_binop_inplace_alpha(self, other, alpha: int = 1): ...
def meta_round(self, **kwargs): ...
def meta_zero(self): ...
def meta_fill_(self, val): ...
def meta_fill(self, val): ...
def meta_relu_(self): ...
def meta_index_put(self, indices, values, accumulate: bool = False): ...
def meta_masked_fill_(self, mask, value): ...
def meta_index_put_(self, indices, values, accumulate: bool = False): ...
def meta_alias(self): ...
def common_meta_baddbmm_bmm(batch1, batch2, is_bmm, self_baddbmm: Incomplete | None = None): ...
def meta_bmm(self, mat2): ...
def div_rtn(x, y): ...
def pooling_output_shape_pad_lr(inputSize, kernelSize, pad_l, pad_r, stride, dilation, ceil_mode): ...
def pooling_output_shape(inputSize, kernelSize, pad, stride, dilation, ceil_mode): ...
def pool2d_shape_check(input, kH, kW, dH, dW, padH, padW, dilationH, dilationW, nInputPlane, inputHeight, inputWidth, outputHeight, outputWidth, memory_format): ...
def max_pool2d_checks_and_compute_shape(input, kernel_size, stride, padding, dilation, ceil_mode): ...
def meta_max_pool2d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices): ...
def meta_max_pool2d_with_indices(input, kernel_size, stride=(), padding=(0,), dilation=(1,), ceil_mode: bool = False): ...
def grid_sampler_2d_backward_meta(grad_output, input, grid, interpolation_mode, padding_mode, align_corners, output_mask): ...
def full(size, fill_value, *args, **kwargs): ...
def meta_like(self, *args, **kwargs): ...
def zeros_like(self, dtype: Incomplete | None = None, layout: Incomplete | None = None, device: Incomplete | None = None, pin_memory: Incomplete | None = None, memory_format: Incomplete | None = None): ...
def meta_select(self, dim, index): ...
def meta_select_scatter(self, src, dim, index): ...
def meta_slice_scatter(self, src, dim: int = 0, start: Incomplete | None = None, end: Incomplete | None = None, step: int = 1): ...
def maybe_wrap_dim(dim: int, dim_post_expr: int, wrap_scalar: bool = True): ...
def ensure_nonempty_size(t, dim): ...
def gather_shape_check(self, dim, index): ...
def meta_gather(self, dim, index, sparse_grad: bool = False): ...
def get_operator_enum(reduce_, use_new_options: bool = False): ...
def scatter_gather_dtype_check(method_name, self, index, src_opt: Incomplete | None = None): ...
def ensure_nonempty_dim(dim): ...
def scatter_shape_check(self, dim, index, src_opt: Incomplete | None = None): ...
def scatter_meta_impl(self, dim, index, src: Incomplete | None = None, reduce_: Incomplete | None = None, use_new_options: bool = False) -> None: ...
def meta_scatter_add(self, dim, index, src): ...
def meta_scatter_add_(self, dim, index, src): ...
def meta_scatter(self, dim, index, src_or_value, reduce: Incomplete | None = None): ...
def meta_scatter_(self, dim, index, src_or_value, reduce: Incomplete | None = None): ...
def meta__scaled_dot_product_flash(query: Tensor, key: Tensor, value: Tensor, dropout_p: float = 0.0, is_causal: bool = False, return_debug_mask: bool = False): ...
def meta__scaled_dot_product_flash_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, cum_seq_q: Tensor, cum_seq_k: Tensor, max_q: int, max_k: int, dropout_p: float, is_causal: bool, philox_seed: int, philox_offset: int): ...
def meta__scaled_dot_product_efficient(query: Tensor, key: Tensor, value: Tensor, compute_log_sumexp: bool, is_causal: bool = False): ...
def meta__scaled_dot_product_efficient_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, is_causal: bool = False, chunk_grad_outputs: bool = False): ...
def meta_scatter_reduce_two(self, dim, index, src, reduce, include_self: bool = True): ...
def meta_scatter_reduce__two(self, dim, index, src, reduce, include_self: bool = True): ...
def multiply_integers(vs): ...
def upsample_common_check(input_size, output_size, num_spatial_dims): ...
def upsample_nearest1d(input, output_size, scales: Incomplete | None = None): ...
def upsample_nearest2d(input, output_size, scales_h: Incomplete | None = None, scales_w: Incomplete | None = None): ...
def upsample_nearest3d(input, output_size, scales_d: Incomplete | None = None, scales_h: Incomplete | None = None, scales_w: Incomplete | None = None): ...
def meta_sort(self, stable: Incomplete | None = None, dim: int = -1, descending: bool = False): ...
def rnn_cell_checkSizes(input_gates, hidden_gates, input_bias, hidden_bias, factor, prev_hidden): ...
def mkldnn_rnn_layer(input, w0, w1, w2, w3, hx_, cx_, reverse, batch_sizes, mode, hidden_size, num_layers, has_biases, bidirectional, batch_first, train): ...
def zero_numel_check_dims(self, dim, fn_name): ...
def check_argmax_argmin(name, self, dim): ...
def argmax_argmin_meta(self, dim: Incomplete | None = None, keepdim: bool = False): ...
def scalar_tensor(s, dtype: Incomplete | None = None, layout: Incomplete | None = None, device: Incomplete | None = None, pin_memory: Incomplete | None = None): ...
def topk_meta(self, k, dim: int = -1, largest: bool = True, sorted: bool = True): ...

legacy_contiguous_memory_format: Incomplete

def checkLSTMBackwardSizes(grad_hy, grad_cy, cx, cy, workspace): ...
def meta_pixel_shuffle(self, upscale_factor): ...
def mkldnn_rnn_layer_backward(input, weight0, weight1, weight2, weight3, hx_, cx_tmp, output, hy_, cy_, grad_output_r_opt, grad_hy_r_opt, grad_cy_r_opt, reverse, mode, hidden_size, num_layers, has_biases, train, bidirectional, batch_sizes, batch_first, workspace): ...
def meta_bucketize(self, boundaries, *, out_int32: bool = False, right: bool = False): ...
def activate_meta() -> None: ...
