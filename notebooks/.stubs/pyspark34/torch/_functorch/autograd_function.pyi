from _typeshed import Incomplete
from torch._C._functorch import TransformType as TransformType, current_level as current_level
from torch._functorch.utils import enable_single_level_autograd_function as enable_single_level_autograd_function
from torch._functorch.vmap import restore_vmap as restore_vmap, unwrap_batched as unwrap_batched, vmap as vmap, wrap_batched as wrap_batched
from torch._ops import PyOperator as PyOperator
from typing import NamedTuple

class CustomFunctionPyOperator(PyOperator):
    def __init__(self) -> None: ...
    def __call__(self, autograd_function, *args, **kwargs): ...

custom_function_call: Incomplete

def custom_function_call_grad(interpreter, autograd_function, *operands): ...
def generate_single_level_function(interpreter, autograd_function): ...

NO_OUT_DIMS: str

def wrap_outputs_maintaining_identity(outputs, unwrapped_inputs, orig_inputs, wrap_fn, out_dims=...): ...

class VmapInfo(NamedTuple):
    batch_size: int
    randomness: str

def has_overriden_vmap_rule(autograd_function): ...
def validate_vmap_returns_tuple_of_two_elements(result) -> None: ...
def custom_function_call_vmap(interpreter, autograd_function, *operands): ...
def custom_function_call_vmap_generate_rule(interpreter, autograd_function, *operands): ...
def custom_function_call_functionalize(interpreter, autograd_function, generate_vmap_rule, *operands) -> None: ...
def vmapify_autograd_function(autograd_function, in_dims, batch_size, randomness): ...
def get_tangents_in_dims(input_dims, tangents): ...

class WrappedCtx:
    def __init__(self, ctx) -> None: ...
    def __getattr__(self, name): ...
    def __setattr__(self, name, value): ...

class CtxWithSavedTensors(WrappedCtx):
    def __init__(self, ctx, new_saved_tensors) -> None: ...
    @property
    def saved_tensors(self): ...

class CtxCustomSave(WrappedCtx):
    def __init__(self, ctx, current_level) -> None: ...
    def save_for_backward(self, *tensors) -> None: ...
    def save_for_forward(self, *tensors) -> None: ...

def reductify(grad_input, grad_input_bdim, input_bdim, batch_size, target_shape_without_bdim_to_reduce_to: Incomplete | None = None): ...
def reductify_leaf(grad_input, grad_input_bdim, input_bdim, batch_size, target_shape_without_bdim_to_reduce_to: Incomplete | None = None): ...
