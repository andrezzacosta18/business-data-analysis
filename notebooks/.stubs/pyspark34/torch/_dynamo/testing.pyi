import torch
import types
from . import config as config, eval_frame as eval_frame, optimize_assert as optimize_assert, reset as reset
from .bytecode_transformation import create_instruction as create_instruction, debug_checks as debug_checks, is_generator as is_generator, transform_code_object as transform_code_object
from .guards import CheckFunctionManager as CheckFunctionManager, GuardedCode as GuardedCode
from .utils import same as same
from _typeshed import Incomplete
from torch import fx as fx

unsupported: Incomplete
three: int
log: Incomplete

def clone_me(x): ...
def skip_if_pytest(fn): ...
def named_parameters_for_optimized_module(mod): ...
def named_buffers_for_optimized_module(mod): ...
def remove_optimized_module_prefix(name): ...
def collect_results(model, prediction, loss, example_inputs): ...
def requires_bwd_pass(out): ...
def reduce_to_scalar_loss(out):
    """Reduce the output of a model to get scalar loss"""
def debug_dir(): ...
def debug_dump(name, code: types.CodeType, extra: str = ''): ...
def debug_insert_nops(frame, cache_size, hooks):
    """used to debug jump updates"""

class CompileCounter:
    frame_count: int
    op_count: int
    def __init__(self) -> None: ...
    def __call__(self, gm: torch.fx.GraphModule, example_inputs): ...
    def clear(self) -> None: ...

class CompileCounterWithBackend:
    frame_count: int
    op_count: int
    backend: Incomplete
    def __init__(self, backend) -> None: ...
    def __call__(self, gm: torch.fx.GraphModule, example_inputs): ...

def standard_test(self, fn, nargs, expected_ops: Incomplete | None = None, expected_ops_dynamic: Incomplete | None = None) -> None: ...
def dummy_fx_compile(gm: fx.GraphModule, example_inputs): ...
def format_speedup(speedup, pvalue, is_correct: bool = True, pvalue_threshold: float = 0.1): ...
def requires_static_shapes(fn): ...
def rand_strided(size, stride, dtype=..., device: str = 'cpu', extra_size: int = 0): ...
def make_test_cls_with_patches(cls, cls_prefix, fn_suffix, *patches): ...
