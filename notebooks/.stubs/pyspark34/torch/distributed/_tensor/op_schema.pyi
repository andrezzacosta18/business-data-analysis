import torch
from dataclasses import dataclass
from torch.distributed._tensor.placement_types import DTensorSpec as DTensorSpec
from typing import Dict, List, Sequence, Tuple

ArgsType = Tuple[object, ...]
KwargsType = Dict[str, object]
OutputSpecType = DTensorSpec | Sequence[DTensorSpec | None] | None

@dataclass
class OpSchema:
    '''
    OpSchema is a data class that describes an operator input schemas, it
    includes DTensor DTensorSpecs and non-tensor args/kwargs (positional order
    preserved). It is mainly used by the dispatching logic below to run things like
    sharding propagation.

    Sharding propagation rules registered could utilize this data class and
    do inplace update some fields (when necessary, i.e shape related ops) to make
    sure the args/kwargs are legit before passing to the local tensor operator.
    This is the main reason that we don\'t freeze this dataclass.

    NOTE: greater access to the operator inputs comes with greater responsibility.
    Here are some basic rules about what can be used and what can be changed.

    Args:
        func_schema: the function schema of the operator
        args_schema: contains args except that the DTensor args have been replaced
            with its DTensorSpec
        kwargs_schema: contains kwargs except that the DTensor kwargs have been replaced
            with its DTensorSpec

    What can be used:
        - every attribute within this class could be read to conduct
          sharding propagation.
    What can be changed:
        - only the args_schema and kwargs_schema could be changed.
        - every non-tensor args could be changed to accomodate for local tensor
          operations (i.e. for ops like view/reshape/...)
        - every "DTensorSpec" attribute inside `args_schema`, `kwargs_schema` and
          `args_spec` SHOULD NOT be updated! DTensorSpec are read only and sharding
          propagation shouldn\'t inplace update them, otherwise the input DTensor
          placements will get implicitly changed and it\'s error-prone.
    '''
    func_schema: torch._C.FunctionSchema
    args_schema: ArgsType
    kwargs_schema: KwargsType
    is_inplace: bool = ...
    is_out_variant: bool = ...
    def __post_init__(self) -> None: ...
    @property
    def args_spec(self) -> Tuple[DTensorSpec, ...]:
        """
        args_spec: Tuple[DTensorSpec, ...]: contains a clean list of args spec list
            with NO non-DTensor positional arguments (i.e. int/float/tuple, etc)
            mainly used by sharding propagation to propagate the output spec
        """
    def __hash__(self) -> int: ...
    def __eq__(self, other: object) -> bool: ...
    def __init__(self, func_schema, args_schema, kwargs_schema, is_inplace, is_out_variant) -> None: ...

@dataclass
class OutputSharding:
    """
    OutputSharding is a data class that is used by the sharding propagation
    rules, it could set the output_spec upon successful propagation, and if
    it failed, output_spec would become None and sharding propagation rules
    could give a list of suggestions for inputs to reshard.

    NOTE: the schema_suggestion generated by sharding propagation should be
    exactly the same as the operator OpSchema, except the DTensor DTensorSpecs
    """
    output_spec: OutputSpecType
    schema_suggestions: List[OpSchema] | None = ...
    failed_reason: str | None = ...
    def __init__(self, output_spec, schema_suggestions, failed_reason) -> None: ...
