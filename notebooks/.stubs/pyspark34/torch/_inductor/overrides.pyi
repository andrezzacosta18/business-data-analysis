import torch
from . import config as config
from .fx_utils import matches_module_function_pattern as matches_module_function_pattern
from .mkldnn import mkldnn_fuse_fx as mkldnn_fuse_fx
from _typeshed import Incomplete
from torch._dynamo.utils import fake_mode_from_tensors as fake_mode_from_tensors
from torch.fx.experimental.optimization import matches_module_pattern as matches_module_pattern, replace_node_module as replace_node_module
from torch.fx.experimental.proxy_tensor import ProxyTorchDispatchMode as ProxyTorchDispatchMode
from torch.fx.passes.shape_prop import ShapeProp as ShapeProp
from torch.nn.utils.fusion import fuse_conv_bn_eval as fuse_conv_bn_eval, fuse_conv_bn_weights as fuse_conv_bn_weights
from torch.overrides import TorchFunctionMode as TorchFunctionMode

log: Incomplete

class AutogradMonkeypatch(TorchFunctionMode):
    def __torch_function__(self, func, types, args=(), kwargs: Incomplete | None = None): ...
patch_functions = AutogradMonkeypatch

def replace_fx(gm: torch.fx.GraphModule): ...
def fuse_fx(gm: torch.fx.GraphModule, example_inputs): ...
def fetch_attr(target: str, mod): ...
def remove_identity(gm: torch.fx.GraphModule):
    """
    Removes all identity layers from the module.
    """
def fuse_conv_bn(gm: torch.fx.GraphModule, inplace: bool = False):
    """
    Fuses Convolution/BN layers for inference purposes.
    """

class NormalizedLinearNode:
    node: Incomplete
    def __init__(self, node: torch.fx.Node) -> None: ...
    def get_input(self) -> torch.fx.Node: ...
    def get_weight(self) -> torch.fx.Node: ...
    def get_bias(self) -> torch.fx.Node: ...

class NormalizedMatmulNode:
    node: Incomplete
    def __init__(self, node: torch.fx.Node) -> None: ...
    def get_input(self) -> torch.fx.Node: ...
    def get_other(self) -> torch.fx.Node: ...

def check_permute(node: torch.fx.Node): ...
def sink_cat_after_pointwise(module: torch.fx.GraphModule) -> torch.fx.GraphModule: ...
def linear_permute_fusion(module: torch.fx.GraphModule) -> torch.fx.GraphModule: ...
def linear_transpose(input: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor: ...
def permute_linear_fusion(module: torch.fx.GraphModule) -> torch.fx.GraphModule: ...
def permute_matmul_fusion(module: torch.fx.GraphModule) -> torch.fx.GraphModule: ...
def transpose_linear(input: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor: ...
def transpose_matmul(A: torch.Tensor, B: torch.Tensor, Atrans: bool, Btrans: bool): ...

philox_rand_like: Incomplete
philox_seed_like: Incomplete

def null_ref() -> None: ...

class PhiloxRandomState:
    next_offset: int
    seed: Incomplete
    last_tracer_ref = null_ref
    @classmethod
    def reset(cls, tracer: Incomplete | None = None) -> None: ...
    @classmethod
    def get_seed_offset(cls, x): ...

class LowmemDropout(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, p): ...
    @staticmethod
    def backward(ctx, grad_output): ...

def lowmem_dropout(input, p: float = 0.5, training: bool = True, inplace: bool = False): ...
def rand_like(x, **kwargs): ...

replacements: Incomplete
replacements_using_triton_random: Incomplete
