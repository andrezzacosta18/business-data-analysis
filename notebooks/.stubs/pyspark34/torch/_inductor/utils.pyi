import sympy
import torch
from . import config as config
from .cuda_properties import get_device_capability as get_device_capability
from _typeshed import Incomplete
from collections.abc import Generator
from torch.fx.immutable_collections import immutable_dict as immutable_dict, immutable_list as immutable_list
from triton.testing import do_bench as do_bench
from typing import Any, Dict, List, NamedTuple

log: Incomplete
VarRanges: Incomplete

def has_triton(): ...
def has_torchvision_roi_align(): ...
def conditional_product(*args): ...
def sympy_product(it): ...
def sympy_dot(seq1, seq2): ...
def unique(it): ...
def ceildiv(numer: int, denom: int): ...
def convert_shape_to_inductor(lst: List[int | torch.SymInt]) -> List[sympy.Expr]:
    """
    Gets the shape and stride of a tensor. For non-symbolic tensors, this is
    trivial. But for symbolic tensors, we need to map from SymIntNode into
    sympy.Expr.
    """
def convert_shape_to_symint(lst: List[int | sympy.Expr]) -> List[int | torch.SymInt]:
    """
    Takes a list of shapes from Inductor and converts them into symints (or just
    ints if all shapes are static).
    """
def gen_gm_and_inputs(target, args, kwargs): ...
def synchronize() -> None: ...
def timed(model, example_inputs, times: int = 1): ...
def print_performance(fn, args=(), times: int = 10, repeat: int = 10, baseline: float = 1.0): ...
def freeze_inputs(f):
    """
    Useful for wrapping lists in tuples for caching purposes
    """
def precompute_method(obj: Any, method: str):
    """Replace obj.method() with a new method that returns a precomputed constant."""
def precompute_methods(obj: Any, methods: List[str]):
    """Replace methods with new methods that returns a precomputed constants."""
def cmp(a, b): ...
def cache_on_self(fn): ...
def get_fused_kernel_name(node_schedule): ...
def gather_origins(args, kwargs): ...
def sympy_str(expr: sympy.Expr):
    """
    Normal sympy str is very slow, this is a lot faster.  The result are
    somewhat worse, as it doesn't do as much simplification.  So don't
    use this for final codegen.
    """
def sympy_symbol(name): ...
def sympy_subs(expr: sympy.Expr, replacements: Dict[Any, Any]):
    """
    xreplace is faster than subs, but is way more picky
    """
def free_symbol_startswith(index: sympy.Expr, prefix: str): ...
def has_incompatible_cudagraph_ops(gm): ...

class instance_descriptor(NamedTuple):
    divisible_by_16: Incomplete
    equal_to_1: Incomplete

def fresh_inductor_cache(cache_entries: Incomplete | None = None) -> Generator[None, None, None]:
    """
    Contextmanager that provides a clean tmp cachedir for inductor.

    Optionally, pass a dict as 'cache_entries' to get a list of filenames and sizes
    generated with this cache instance.
    """
def argsort(seq): ...
def get_dtype_size(dtype): ...

class IndentedBuffer:
    tabwidth: int
    def __init__(self, initial_indent: int = 0) -> None: ...
    def getvalue(self): ...
    def getrawvalue(self): ...
    def clear(self) -> None: ...
    def __bool__(self) -> bool: ...
    def prefix(self): ...
    def writeline(self, line) -> None: ...
    def writelines(self, lines) -> None: ...
    def indent(self, offset: int = 1): ...
    def splice(self, other_code, strip: bool = False) -> None: ...

class DeferredLineBase:
    """A line that can be 'unwritten' at a later time"""
    line: Incomplete
    def __init__(self, line) -> None: ...
    def __call__(self) -> str | None:
        """Returns either self.line or None to indicate the line has been 'unwritten'"""
    def with_prefix(self, prefix): ...
    def lstrip(self): ...
    def __getitem__(self, index): ...
    def __bool__(self) -> bool: ...
    def __len__(self) -> int: ...

def is_big_gpu(index): ...
def use_triton_template(layout): ...

class DebugDirManager:
    counter: Incomplete
    id: Incomplete
    prev_debug_name: Incomplete
    def __init__(self) -> None: ...
    new_name: Incomplete
    def __enter__(self) -> None: ...
    def __exit__(self, *args) -> None: ...

def run_and_get_triton_code(fn, *args, **kwargs): ...
def developer_warning(msg) -> None:
    """
    Warnings that will be actionable for PyTorch developers, but not
    end users.  Allows us to easily disable them in stable releases but
    keep them on for nightly builds.
    """
