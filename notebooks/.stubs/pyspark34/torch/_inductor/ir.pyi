import dataclasses
import sympy
import torch
import torch.fx
from . import config as config, dependencies as dependencies
from .codegen.common import index_prevent_reordering as index_prevent_reordering
from .cuda_properties import get_device_properties as get_device_properties
from .dependencies import extract_read_writes as extract_read_writes, var_builder as var_builder
from .utils import argsort as argsort, cache_on_self as cache_on_self, convert_shape_to_inductor as convert_shape_to_inductor, convert_shape_to_symint as convert_shape_to_symint, developer_warning as developer_warning, sympy_dot as sympy_dot, sympy_product as sympy_product, sympy_subs as sympy_subs, sympy_symbol as sympy_symbol
from .virtualized import V as V, ops as ops
from _typeshed import Incomplete
from enum import Enum
from sympy import Expr
from torch._prims_common import is_boolean_dtype as is_boolean_dtype, is_float_dtype as is_float_dtype, make_channels_last_strides_for as make_channels_last_strides_for, make_contiguous_strides_for as make_contiguous_strides_for
from torch.fx.experimental.symbolic_shapes import FloorDiv as FloorDiv
from typing import Any, Callable, Dict, List, Set, Tuple

log: Incomplete
indent: Incomplete
aten: Incomplete

def validate_ir(node_or_nodes) -> None: ...
def inverse_reorder(order): ...
def same_reorder(order): ...
def fuse_reindexing(reindex1, reindex2): ...
def stride_order2fill_order(order):
    """
    Convert stride order to fill order
    For channel last format,
    stride order = [3, 0, 2, 1] and fill order = [1, 3, 2, 0]
    """
def get_stride_order(seq):
    """
    Convert strides to stride order
    """
def reads_from_conv(buf, var_ranges):
    """
    return:
    if reads_from_conv: boolean
    the new memory_addr: Sympy Expression
    """
def ir_node_to_tensor(x, guard_shape: bool = True): ...
def layout_priority_idx(reads_bufs, memory_addrs, var_ranges):
    """
    if reads from conv that needs to use specific layout
    return:
    priority_idx regarding memory_addrs idx
    memory_addrs - update memory_addrs with the true addr if needed
    """

class ModularIndexing(sympy.Function):
    """
    ModularIndexing(a, b, c) => (a // b) % c
    """
    nargs: Incomplete
    is_integer: bool
    @classmethod
    def eval(cls, base, divisor, modulus): ...

class CleanDiv(FloorDiv):
    """
    Div where we can assume no rounding.
    This is to enable future optimizations.
    """

class CeilDiv(sympy.Function):
    """
    Div used in indexing that rounds up.
    """
    is_integer: bool
    def __new__(cls, base, divisor): ...

def get_device_type(x): ...
def is_triton(x): ...
def is_cpu(x): ...

@dataclasses.dataclass
class IRNode:
    @staticmethod
    def current_origins(origins: Set[torch.fx.Node]): ...
    origins = ...
    def __post_init__(self) -> None: ...
    def common_repr(self): ...
    def str_helper(self, lines): ...
    def is_user_of(self, name): ...
    def get_numel(self): ...

@dataclasses.dataclass
class Loops(IRNode):
    device: torch.device
    dtype: torch.dtype
    inner_fn: Callable
    ranges: List[Expr]
    def get_dtype(self): ...
    def get_device(self): ...
    def get_size(self): ...
    def is_extern(self): ...
    @classmethod
    def create(cls, *args, **kwargs): ...
    def inner_fn_str(self): ...
    def is_zero_elements(self): ...
    def get_reads(self): ...
    def __init__(self, device, dtype, inner_fn, ranges) -> None: ...

class Pointwise(Loops):
    def make_loader(self): ...
    def get_reduction_size(self): ...
    def get_reduction_type(self) -> None: ...
    def store_output(self, output_name, indexer, vars): ...
    def constant_to_device(self, device):
        """Move this to a given device. Requires that all reads are to constants."""

@dataclasses.dataclass
class Scatter(Pointwise):
    output_indexer: Callable[[List[Expr]], Expr]
    scatter_mode: str | None = ...
    def constant_to_device(self, device):
        """Move this to a given device. Requires that all reads are to constants."""
    def store_output(self, output_name, indexer, vars): ...
    def __init__(self, device, dtype, inner_fn, ranges, output_indexer, scatter_mode) -> None: ...

class ReductionHint(Enum):
    INNER: int
    OUTER: int
    OUTER_TINY: int
    DEFAULT: int

class TileHint(Enum):
    SQUARE: int
    DEFAULT: int

@dataclasses.dataclass
class Reduction(Loops):
    reduction_ranges: List[Expr]
    reduction_type: str
    src_dtype: torch.dtype
    reduction_hint: ReductionHint
    def get_reduction_size(self): ...
    def get_reduction_type(self): ...
    def store_reduction(self, output_name, indexer, vars, reduction_vars): ...
    def index_length(self): ...
    def inner_fn_str(self): ...
    def constant_to_device(self, device):
        """Move this to a given device. Requires that all reads are to constants."""
    @staticmethod
    def num_splits(device, dst_dtype, src_dtype, inner_fn, ranges, reduction_ranges, reduction_type, reduction_numel): ...
    @classmethod
    def create(cls, device: torch.device, dst_dtype: torch.dtype, src_dtype: torch.dtype, inner_fn: Callable, ranges: List[Expr], reduction_ranges: List[Expr], reduction_type: str, reduction_hint: ReductionHint = ...): ...
    @staticmethod
    def default_value(reduction_type, dtype): ...
    @classmethod
    def create_multilayer(cls, device: torch.device, dst_dtype: torch.dtype, src_dtype: torch.dtype, inner_fn: Callable, ranges: List[Expr], reduction_ranges: List[Expr], reduction_type: str, split: int, reduction_hint: ReductionHint):
        """
        Break a large reduction up into multiple smaller reductions
        recursively
        """
    def __init__(self, device, dtype, inner_fn, ranges, reduction_ranges, reduction_type, src_dtype, reduction_hint) -> None: ...

def is_storage_and_layout(x): ...
def is_contiguous_storage_and_layout(x): ...
def as_storage_and_layout(x, freeze: bool = True, want_contiguous: bool = False, stride_order: Incomplete | None = None):
    """Try to simplify x into a StorageBox and a Layout"""

as_contiguous_storage_and_layout: Incomplete

def is_stride_order_storage_and_layout(x, stride_order): ...

@dataclasses.dataclass
class BaseView(IRNode):
    data: IRNode
    def get_dtype(self): ...
    def get_device(self): ...
    def get_name(self): ...
    def mark_reuse(self, users): ...
    def has_exceeded_max_reads(self): ...
    def realize(self): ...
    def realize_hint(self): ...
    def get_storage_numel(self): ...
    def is_extern(self): ...
    def get_reads(self): ...
    def unwrap_view(self): ...
    def constant_to_device(self, device):
        """Move this to a given device. Requires that all reads are to constants."""
    def __init__(self, data) -> None: ...

@dataclasses.dataclass
class ExpandView(BaseView):
    size: List[Expr]
    @classmethod
    def create(cls, x, new_size): ...
    def get_size(self): ...
    def make_loader(self): ...
    def __init__(self, data, size) -> None: ...

@dataclasses.dataclass
class PermuteView(BaseView):
    dims: List[Expr]
    @classmethod
    def create(cls, x, dims): ...
    def get_size(self): ...
    def make_loader(self): ...
    def __init__(self, data, dims) -> None: ...

class SqueezeView(BaseView):
    @classmethod
    def create(cls, x, *, dim: Incomplete | None = None): ...
    @staticmethod
    def squeezer(size: Tuple[sympy.Expr, ...]): ...
    def __init__(self, data) -> None: ...

@dataclasses.dataclass
class View(BaseView):
    size: List[Expr]
    reindex: Callable
    def make_indexer(self): ...
    @staticmethod
    def handle_negative_index(idx, size): ...
    def reindex_str(self): ...
    @classmethod
    def create(cls, x, new_size): ...
    @staticmethod
    def resolve_negative_size(old_size, new_size): ...
    @classmethod
    def dynamic_reshape_indexer(cls, old_size, new_size): ...
    def get_size(self): ...
    def make_loader(self): ...
    def __init__(self, data, size, reindex) -> None: ...

@dataclasses.dataclass
class ReinterpretView(BaseView):
    """Pretend our storage has a different layout"""
    layout: Layout
    data = ...
    def __post_init__(self) -> None: ...
    def get_name(self): ...
    def get_device(self): ...
    def get_dtype(self): ...
    def get_size(self): ...
    def get_stride(self): ...
    def make_loader(self): ...
    def make_indexer(self): ...
    def get_layout(self): ...
    def freeze_layout(self) -> None: ...
    def codegen_reference(self): ...
    def __init__(self, data, layout) -> None: ...

class SliceView(View):
    @classmethod
    def create(cls, x, dim, start, end, step: int = 1): ...

class BaseConstant(IRNode):
    def get_size(self): ...
    def get_dtype(self): ...
    def get_device(self): ...
    def mark_reuse(self, users) -> None: ...
    def has_exceeded_max_reads(self): ...
    def get_reads(self): ...
    def is_extern(self): ...

@dataclasses.dataclass
class Constant(BaseConstant):
    value: Any
    dtype: torch.dtype
    device: torch.device
    def make_loader(self): ...
    def realize(self) -> None: ...
    def __init__(self, value, dtype, device) -> None: ...

@dataclasses.dataclass
class IndexingConstant(BaseConstant):
    index: Any
    dtype: torch.dtype
    device: torch.device
    def make_loader(self): ...
    def __init__(self, index, dtype, device) -> None: ...

@dataclasses.dataclass
class Layout(IRNode):
    device = ...
    dtype = ...
    size = ...
    offset = ...
    def __init__(self, device: torch.device, dtype: torch.dtype, size: List[Expr], stride: List[Expr], offset: Expr = ...) -> None: ...
    @property
    def stride(self): ...
    def is_contiguous(self): ...
    def is_channels_last_contiguous(self): ...
    def is_transposed(self): ...
    def is_stride_ordered(self, order): ...
    def is_channels_last_stride_ordered(self): ...
    def as_fixed(self): ...
    def make_indexer(self): ...
    def __eq__(self, other) -> bool: ...

class FixedLayout(Layout):
    """A Tensor layout we cannot change"""
    def __init__(self, device: torch.device, dtype: torch.dtype, size: List[Expr], stride: List[Expr] = None, offset: Expr = ...) -> None: ...
    def make_indexer(self):
        """A closure containing math to read a given element"""

class FlexibleLayout(Layout):
    """A Tensor layout we are allowed to change"""
    allow_indexing: bool
    @staticmethod
    def contiguous_strides(sizes): ...
    @staticmethod
    def fill_ordered(sizes, order):
        """
        Create a stride based on the order the dimensions should be filled in.

        In this format, channels last would be:
            [1, 3, 2, 0]
        """
    @staticmethod
    def stride_ordered(sizes, order):
        """
        Create a stride based on the sorted order of a permuted range.

        In this format, channels last would be:
            [3, 0, 2, 1]
        """
    @staticmethod
    def same_ordered(sizes, stride):
        """
        Create a stride that has the same stride order as given stride

        For example, if given stride is [1000, 1, 100, 10],
        the fill order should be [1, 3, 2, 0]
        """
    def as_stride_order(self, order): ...
    def as_fill_order(self, order): ...
    def as_same_order(self, stride): ...
    def __init__(self, device, dtype, size, stride_order: Incomplete | None = None) -> None: ...

class AliasedLayout(Layout):
    """Shares the same storage as another tensor"""
    view: Incomplete
    def __init__(self, view: ReinterpretView) -> None: ...
    def make_indexer(self): ...
    def maybe_guard_aligned(self): ...

class MutationLayout(Layout):
    target: Incomplete
    def __init__(self, target: IRNode) -> None: ...
    def stride(self): ...
    def real_layout(self): ...
    @classmethod
    def realize_into(cls, src, dst): ...
    def as_fixed(self): ...
    def make_indexer(self): ...

@dataclasses.dataclass
class Buffer(IRNode):
    name: str
    layout: Layout
    def make_indexer(self): ...
    def get_name(self): ...
    def get_device(self): ...
    def get_dtype(self): ...
    def get_size(self): ...
    def get_stride(self): ...
    def get_layout(self): ...
    def get_storage_numel(self): ...
    def is_extern(self): ...
    def freeze_layout(self) -> None: ...
    def freeze_layout_with_stride_order(self, order) -> None: ...
    def freeze_layout_with_fill_order(self, order) -> None: ...
    def freeze_layout_with_same_order(self, stride) -> None: ...
    def make_loader(self): ...
    def is_no_op(self): ...
    def codegen_reference(self): ...
    def decide_layout(self) -> None: ...
    def get_alias_names(self): ...
    def get_mutation_names(self): ...
    def get_read_writes(self): ...
    def get_reads(self): ...
    def realize(self) -> None: ...
    def __init__(self, name, layout) -> None: ...

class InputBuffer(Buffer): ...

class ConstantBuffer(InputBuffer):
    override_device: Incomplete
    def make_loader(self): ...
    def constant_to_device(self, device): ...

class RandSeedBuffer(ConstantBuffer):
    def codegen_reference(self): ...

class NoneAsConstantBuffer(IRNode):
    def codegen_reference(self): ...
    def cpp_wrapper_codegen_reference(self): ...

class ShapeAsConstantBuffer(IRNode):
    shape: Incomplete
    def __init__(self, shape) -> None: ...
    def codegen_reference(self): ...

@dataclasses.dataclass
class ComputedBuffer(Buffer):
    data: Loops
    def get_read_writes(self): ...
    def get_store_function(self): ...
    def get_fill_order(self):
        """
        If our layout is still flexible, try to determine the stride order based on stride orders of reads.

        TODO(jansel): A better algorithm here would look at downstream consumers of this
                      value and try to do global graph-level layout optimization.
                      This is also something just begging to be autotuned.
        """
    def decide_layout(self) -> None: ...
    iter_reordering_reindex = ...
    def simplify_and_reorder(self):
        """
        This is a main place where we do loop transformations in a
        backend-agnostic way.

        Here we:
            1) Remove any 1 dimensions
            2) Fuse contiguous dimensions together
            3) Reorder dimensions based on stride orders
        """
    def get_reduction_size(self): ...
    def get_reduction_type(self): ...
    def is_no_op(self): ...
    def should_allocate(self): ...
    def constant_to_device(self, device):
        """Move this to a given device. Requires that all reads are to constants."""
    def __init__(self, name, layout, data) -> None: ...

class TemplateBuffer(Buffer):
    """
    Represents a Triton (in the futurue other type) of template operator
    that we can fuse an epilogue onto.
    """
    inputs: Incomplete
    make_kernel_render: Incomplete
    name: Incomplete
    def __init__(self, layout, inputs, make_kernel_render) -> None: ...
    def get_read_writes(self): ...
    def normalized_read_writes(self): ...
    def get_reduction_size(self): ...
    def get_reduction_type(self) -> None: ...
    def is_no_op(self): ...
    def should_allocate(self): ...
    def simplify_and_reorder(self): ...

@dataclasses.dataclass
class InputsKernel(Buffer):
    inputs: List[Buffer]
    def get_read_writes(self): ...
    @staticmethod
    def unwrap_storage(inputs): ...
    def is_extern(self): ...
    def __init__(self, name, layout, inputs) -> None: ...

class NopKernel(InputsKernel):
    def is_no_op(self): ...

class ConcatKernel(NopKernel):
    """
    There isn't actually a real kernel for concat, we just change the
    storage for the upstream data.
    """
    @classmethod
    def create(cls, inputs, dim): ...
    @classmethod
    def realize_into(cls, src, dst): ...
    def should_allocate(self): ...

@dataclasses.dataclass
class ExternKernel(InputsKernel):
    constant_args: Tuple[Any, ...] = ...
    kwargs: Dict[str, Any] = ...
    output_view: ReinterpretView | None = ...
    def decide_layout(self) -> None: ...
    def codegen(self, wrapper) -> None: ...
    @staticmethod
    def copy_input(x): ...
    @classmethod
    def process_kernel(cls, kernel, *args, **kwargs): ...
    @classmethod
    def convert_to_reinterpret_view(cls, x):
        """
        In order to pass this to an extern kernel we need a
        ReinterpretView not a View.  This allows us to avoid some
        uneeded copies.
        """
    @classmethod
    def realize_input(cls, x): ...
    @classmethod
    def require_stride1(cls, x): ...
    @classmethod
    def require_stride_order(cls, x, order): ...
    @classmethod
    def require_contiguous(cls, x): ...
    def apply_constraint(self) -> None: ...
    def codegen_args(self): ...
    def codegen_kwargs(self): ...
    def cpp_wrapper_codegen_kwargs(self): ...
    def codegen_size_asserts(self, wrapper) -> None: ...
    def get_group_stride(self):
        """
        get output sizes and strides, for template_codegen
        """
    def canonicalize(self):
        """
        Manually get cononicalization of the output index
        """
    def __init__(self, name, layout, inputs, constant_args, kwargs, output_view) -> None: ...

@dataclasses.dataclass
class ExternKernelOut(ExternKernel):
    output_view: ReinterpretView | None = ...
    def codegen(self, wrapper) -> None: ...
    name = ...
    kernel = ...
    cpp_kernel = ...
    def __init__(self, layout, inputs, constant_args=(), kwargs: Incomplete | None = None, output_view: Incomplete | None = None, kernel: Incomplete | None = None, cpp_kernel: Incomplete | None = None) -> None: ...
    def should_allocate(self): ...

class ExternKernelAlloc(ExternKernel):
    def codegen(self, wrapper) -> None: ...
    name: Incomplete
    def __init__(self, layout, inputs, constant_args=()) -> None: ...
    def should_allocate(self): ...
    def apply_constraint(self) -> None: ...

class InplaceBernoulliFallback(ExternKernel):
    """
    This needs to be a custom class to handle mutation properly
    """
    kernel: str
    def codegen(self, wrapper) -> None: ...
    def should_allocate(self): ...
    def get_mutation_names(self): ...
    name: Incomplete
    def __init__(self, x, *constant_args) -> None: ...

class IndexPutFallback(ExternKernel):
    """
    This needs to be a custom class to handle mutation and indices properly
    """
    kernel: str
    def codegen(self, wrapper) -> None: ...
    def should_allocate(self): ...
    indices: Incomplete
    name: Incomplete
    def __init__(self, x, indices, values, accumulate) -> None: ...

class DeviceCopy(ExternKernelOut):
    @classmethod
    def create(cls, x, device): ...
    def codegen(self, wrapper) -> None: ...

class DynamicScalar(IRNode):
    """
    The result of a call to aten._local_scalar_dense.

    This is not yet implemented.  The one model (so far) that calls this
    (fastNLP_Bert) does not actually use the result.  So we expect this
    node to get dead code eliminated.
    """
    def get_reads(self): ...

@dataclasses.dataclass
class FallbackKernel(ExternKernelAlloc):
    kernel = ...
    unflatten_args = ...
    kwargs = ...
    def __init__(self, layout, kernel, tensor_args, nontensor_args, unflatten_args, kwargs: Incomplete | None = None) -> None: ...
    def codegen_args(self): ...
    @classmethod
    def create(cls, kernel, *args, **kwargs): ...
    def apply_constraint(self): ...

@dataclasses.dataclass
class MultiOutputLayout(IRNode):
    device: torch.device
    def __init__(self, device) -> None: ...

class MultiOutput(ExternKernel):
    def codegen(self, wrapper) -> None: ...
    name: Incomplete
    index: Incomplete
    def __init__(self, layout, input, index: str) -> None: ...
    def should_allocate(self): ...

class Convolution(ExternKernelAlloc):
    kernel: str
    preferred_stride_order: Incomplete
    def __init__(self, layout, inputs, constant_args=(), preferred_stride_order: Incomplete | None = None, kernel: str = 'aten.convolution') -> None: ...
    def codegen(self, wrapper) -> None: ...
    @classmethod
    def create(cls, x: TensorBox, weight: TensorBox, bias: TensorBox, stride_: List[int], padding_: List[int], dilation_: List[int], transposed: bool, output_padding_: List[int], groups: int): ...
    def map_args(self): ...
    def get_template_tiling(self): ...

class ConvolutionUnary(ExternKernelAlloc):
    kernel: str
    def __init__(self, layout, inputs, constant_args=(), kernel: str = 'torch.ops.mkldnn._convolution_pointwise') -> None: ...
    def codegen(self, wrapper) -> None: ...
    @classmethod
    def create(cls, x: TensorBox, weight: TensorBox, bias: TensorBox, padding_: List[int], stride_: List[int], dilation_: List[int], groups: int, attr, scalars, algorithm): ...

class ConvolutionBinary(ExternKernelAlloc):
    kernel: str
    def __init__(self, layout, inputs, constant_args=(), kernel: str = 'torch.ops.mkldnn._convolution_pointwise.binary') -> None: ...
    def codegen(self, wrapper) -> None: ...
    @classmethod
    def create(cls, x: TensorBox, other: TensorBox, weight: TensorBox, bias: TensorBox, padding_: List[int], stride_: List[int], dilation_: List[int], groups: int, binary_attr: str, binary_alpha: float | None, unary_attr: str | None, unary_scalars: List | None, unary_algorithm: str | None): ...

class ConvolutionBinaryInplace(ExternKernelAlloc):
    kernel: str
    def __init__(self, kernel_layout, inputs, constant_args=(), kernel: str = 'torch.ops.mkldnn._convolution_pointwise_.binary') -> None: ...
    def codegen(self, wrapper) -> None: ...
    def get_mutation_names(self): ...
    @classmethod
    def create(cls, x: TensorBox, other: TensorBox, weight: TensorBox, bias: TensorBox, padding_: List[int], stride_: List[int], dilation_: List[int], groups: int, binary_attr: str, binary_alpha: float | None, unary_attr: str | None, unary_scalars: List | None, unary_algorithm: str | None): ...

class MKLPackedLinear(ExternKernelAlloc):
    kernel: str
    def __init__(self, layout, inputs, constant_args=(), kernel: str = 'torch.ops.mkl._mkl_linear') -> None: ...
    def codegen(self, wrapper) -> None: ...
    @classmethod
    def create(cls, x, packed_w, orig_w, batch_size): ...

class LinearUnary(ExternKernelAlloc):
    kernel: str
    def __init__(self, layout, inputs, constant_args=(), kernel: str = 'torch.ops.mkldnn._linear_pointwise') -> None: ...
    def codegen(self, wrapper) -> None: ...
    @classmethod
    def create(cls, x, w, b, attr, scalars, algorithm): ...
    def apply_constraint(self) -> None: ...

class LinearBinary(ExternKernelAlloc):
    kernel: str
    def __init__(self, layout, inputs, constant_args=(), kernel: str = 'torch.ops.mkldnn._linear_pointwise.binary') -> None: ...
    def codegen(self, wrapper) -> None: ...
    @classmethod
    def create(cls, x, y, w, b, attr): ...
    def apply_constraint(self) -> None: ...

class ConvolutionTransposeUnary(ExternKernelAlloc):
    kernel: str
    def __init__(self, layout, inputs, constant_args=(), kernel: str = 'torch.ops.mkldnn._convolution_transpose_pointwise') -> None: ...
    def codegen(self, wrapper) -> None: ...
    @classmethod
    def create(cls, x: TensorBox, weight: TensorBox, bias: TensorBox, padding_: List[int], output_padding_: List[int], stride_: List[int], dilation_: List[int], groups_: int, attr, scalars, algorithm): ...

@dataclasses.dataclass
class MutableBox(IRNode):
    """
    TensorBox / StorageBox allow in-place mutation of Tensors
    """
    data: IRNode
    def __getattr__(self, name): ...
    def __init__(self, data) -> None: ...

class TensorBox(MutableBox):
    @staticmethod
    def create(data): ...

class StorageBox(MutableBox):
    def is_input_buffer(self): ...
    data: Incomplete
    def realize(self): ...
    def realize_hint(self) -> None:
        """
        Called on buffers we expect to be forced to realize later.
        """
    def has_exceeded_max_reads(self): ...
    def mark_reuse(self, users):
        """
        A heuristic to decide if we should realize a tensor
        that is used multiple times.
        """
    def num_reads(self): ...

class InterpreterShim(torch.fx.Interpreter):
    module: Incomplete
    graph: Incomplete
    submodules: Incomplete
    garbage_collect_values: bool
    env: Incomplete
    fetch_attr: Incomplete
    name: str
    current_node: Incomplete
    def __init__(self, graph, submodules) -> None:
        """
        We don't call super() here to avoid constructing a
        GraphModule which is very expensive (it does codegen).
        """
    def run_node(self, n: torch.fx.Node) -> Any: ...
    def run(self, *args, **kwargs): ...

class LoopBody:
    """
    Captures the body of a Loops subclass into an FX graph.  Persists any
    indexing simplifications and makes it easier to analyze loop bodies.
    """
    var_ranges: Incomplete
    indexing_exprs: Incomplete
    indexing_exprs_name: Incomplete
    reads: Incomplete
    writes: Incomplete
    reads_name2expr: Incomplete
    writes_name2expr: Incomplete
    other: Incomplete
    submodules: Incomplete
    subblocks: Incomplete
    indirect_vars: Incomplete
    root_block: Incomplete
    indexing: Incomplete
    def __init__(self, fn, args, var_ranges) -> None: ...
    def debug_str(self): ...
    def add_index_expr(self, expr: sympy.Expr, category, buf_name): ...
    def add_submodule(self, block, prefix):
        """Not actually for nn.Modules, but subblocks in generated code are mapped to FX call_module opcodes"""
    def add_indirect(self): ...
    def replace_indirect(self, old, new) -> None:
        """Swap in a variable used in indirect indexing"""
    def get_index(self, name): ...
    def __call__(self, *indices): ...

class LoopBodyBlock:
    """
    Captures the body of a Loops subclass into an FX graph.
    In normal cases there will be a 1:1 mapping between LoopBody and
    LoopBodyBlock, hower in the case of ops.masked() the masked out
    operations will manifest as an extra LoopBodyBlock.
    """
    body: Incomplete
    name: str
    graph: Incomplete
    def __init__(self, body: LoopBody, fn: Callable, args: List[Any]) -> None: ...
    def __call__(self): ...
    def debug_str(self, name: str = 'block'): ...
