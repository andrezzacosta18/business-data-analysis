import sympy
import torch
from . import config as config, ir as ir, kernel as kernel, overrides as overrides, test_operators as test_operators
from .._dynamo.utils import import_submodule as import_submodule
from .cuda_properties import current_device as current_device
from .decomposition import decompositions as decompositions, get_decompositions as get_decompositions
from .ir import ExpandView as ExpandView, IndexingConstant as IndexingConstant, PermuteView as PermuteView, Pointwise as Pointwise, Reduction as Reduction, SqueezeView as SqueezeView, TensorBox as TensorBox, View as View, validate_ir as validate_ir
from .utils import ceildiv as ceildiv, developer_warning as developer_warning, sympy_product as sympy_product
from .virtualized import V as V, ops as ops
from _typeshed import Incomplete
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND as ELEMENTWISE_TYPE_PROMOTION_KIND, Number as Number, canonicalize_dims as canonicalize_dims, dtype_to_type as dtype_to_type, elementwise_dtypes as elementwise_dtypes, is_boolean_dtype as is_boolean_dtype, is_float_dtype as is_float_dtype, is_integer_dtype as is_integer_dtype
from torch.fx.experimental.symbolic_shapes import magic_methods as magic_methods, method_to_operator as method_to_operator
from typing import List, Tuple

log: Incomplete
lowerings: Incomplete
layout_constraints: Incomplete
fallbacks: Incomplete
aten: Incomplete
prims: Incomplete
needs_realized_inputs: Incomplete

def add_needs_realized_inputs(fn): ...
def add_layout_constraint(fn, constraint) -> None: ...

DTYPE_ID_LOOKUP: Incomplete

def decode_dtype(dtype: int): ...
def is_integer_type(x): ...
def is_boolean_type(x): ...
def decode_device(device): ...
def get_promoted_dtype(*args, type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND): ...
def register_lowering(aten_fn, broadcast: bool = False, type_promotion_kind=..., convert_input_to_bool: bool = False):
    """
    Shim to support decorator syntax.
    """
def broadcast_symbolic_shapes(a, b):
    """
    Broadcasting logic based on symbolic shapes.

    We give the shapes 0 and 1 concrete values, while all other shapes
    are symbolic sympy formulas.
    """
def promote_constants(inputs, override_return_dtype: Incomplete | None = None): ...
def make_pointwise(fn, override_return_dtype: Incomplete | None = None, override_device: Incomplete | None = None, override_fn_when_input_bool: Incomplete | None = None, override_fn_when_cuda_float64: Incomplete | None = None, allow_alpha: bool = False): ...
def to_dtype(x: TensorBox, dtype: torch.dtype): ...
def to_device(x: TensorBox, device: torch.device): ...
def ops_wrapper(name): ...
def register_pointwise(aten_fn, name: Incomplete | None = None, broadcast: bool = True, type_promotion_kind=..., convert_input_to_bool: bool = False, override_return_dtype: Incomplete | None = None, override_fn_when_input_bool: Incomplete | None = None, allow_alpha: bool = False, use_libdevice_for_f64: bool = False):
    """A pointwise function that maps ops.{name} to inputs"""
def where(cond, a, b): ...
def broadcast_tensors(*inputs): ...
def nop(x): ...
def squeeze(x, dim: Incomplete | None = None): ...
def squeeze_(x, dim: Incomplete | None = None): ...
def isinf(x): ...
def isnan(x): ...
def ceil(x): ...
def floor(x): ...
def round(x): ...
def trunc(x): ...
def expand(x, sizes): ...
def broadcast_in_dim(a, shape, broadcast_dimensions): ...
def expand_as(x, y): ...
def repeat(x, repeats): ...
def view(x, sizes): ...
def permute(x, dims): ...
def slice_(x, dim: int = 0, start: int = 0, end=..., step: int = 1): ...
def roll(a, shifts, dims=...):
    """
    This is based on torch._refs.roll(), but uses ir.ModularIndexing().

    We can't use the ref here because it is based on multiple calls to
    torch.cat() that this will result in terrible code.
    """
def as_strided(x, size, stride, storage_offset: Incomplete | None = None): ...
def as_strided_(x, size, stride, storage_offset: Incomplete | None = None): ...
def cat(inputs, dim: int = 0): ...
def select(x, dim, idx): ...
def split(x, sizes, dim: int = 0): ...
def split_with_sizes(x, sizes, dim: int = 0): ...
def unbind(x, dim: int = 0): ...
def unsqueeze(x, dim): ...
def unsqueeze_(x, dim): ...
def glu(x, dim: int = -1): ...
def register_onednn_fusion_ops(): ...
def fallback_handler(kernel): ...
def make_fallback(kernel, layout_constraint: Incomplete | None = None, warn: bool = True): ...
def native_dropout(x, p, train): ...
def bernoulli_(x, *args): ...
def bernoulli_p(x, *args): ...
def warn_triton_random() -> None: ...
def make_rand(fn_name): ...

fallback_rand: Incomplete
fallback_randn: Incomplete
fast_rand: Incomplete
fast_randn: Incomplete

def rand(*args, **kwargs): ...
def randn(*args, **kwargs): ...
def philox_seed_like(x): ...
def philox_rand_like(x, seed, offset): ...
def require_dense(_, *args, **kwargs): ...
def require_contiguous(_, *args, **kwargs): ...
def constrain_to_fx_strides(fx_node, *args, **kwargs): ...

FALLBACK_ALLOW_LIST: Incomplete

def convolution(x: TensorBox, weight: TensorBox, bias: TensorBox, stride: List[int], padding: List[int], dilation: List[int], transposed: bool, output_padding: List[int], groups: int): ...
def clone(x, *, memory_format: int = 0): ...
def iota(length, *, start, step, dtype, device, requires_grad): ...
def select_scatter(x, src, dim: int, index: int): ...
def slice_scatter(x, src, dim: int = 0, start: Incomplete | None = None, end: Incomplete | None = None, step: int = 1): ...
def tensor(data, *, dtype: Incomplete | None = None, device: Incomplete | None = None, layout: Incomplete | None = None, pin_memory: bool = False): ...
def as_tensor(data, dtype: Incomplete | None = None, device: Incomplete | None = None): ...
def long_tensor(data): ...
def full_like(x, fill_value, **kwargs): ...
def tensor_constructor(fill_value): ...
def empty(*size, names: Incomplete | None = None, dtype: Incomplete | None = None, layout: Incomplete | None = None, device: Incomplete | None = None, pin_memory: Incomplete | None = None, memory_format: Incomplete | None = None): ...
def create_tensor_like(creation_fn):
    """
    Shim to convert X_like(...) into X(...).  For example zeros_like() into zeros().
    """
def constant_like(fill_value): ...

empty_like: Incomplete
ones_like: Incomplete
rand_like: Incomplete
randn_like: Incomplete

def new_constant(fill_value): ...
def new_empty(x, size, *, dtype: Incomplete | None = None, layout: Incomplete | None = None, device: Incomplete | None = None, pin_memory: Incomplete | None = None): ...
def empty_strided(size, stride, *, dtype: Incomplete | None = None, layout: Incomplete | None = None, device: Incomplete | None = None, pin_memory: Incomplete | None = None): ...
def new_empty_strided(x, size, stride, *, dtype: Incomplete | None = None, layout: Incomplete | None = None, device: Incomplete | None = None, pin_memory: Incomplete | None = None): ...
def copy_strided(x, stride): ...
def full(size, fill_value, **kwargs): ...
def gather(x, dim, index): ...
def embedding(weight, indices, padding_idx: int = -1, scale_grad_by_freq: bool = False, sparse: bool = False): ...
def check_and_broadcast_indices(indices, device): ...
def index(x, indices): ...
def index_put(x, indices, values, accumulate: bool = False): ...
def index_put_as_masked_fill(self, indices, value, accumulate): ...
def index_put_fallback(self, indices, values, accumulate): ...
def index_put_(self, indices, values, accumulate: bool = False): ...
def as_strided_scatter(self, src, size, stride, storage_offset: Incomplete | None = None): ...
def scatter(x, dim: int, index, src, **kwargs): ...
def scatter_fallback(fn, self, dim: int, index, src, *, reduce: str = None, include_self: bool = True): ...
def scatter_(self, dim: int, index, src, *, reduce: str = None): ...
def scatter_add(x, dim: int, index, src): ...
def scatter_add_(x, dim: int, index, src): ...
def scatter_reduce(x, dim: int, index, src, reduction_type, **kwargs): ...

fallback_scatter_reduce_: Incomplete

def scatter_reduce_(self, dim: int, index, src, reduce, *, include_self: bool = True): ...
def upsample_nearestnd(x, output_size, scales_x: Tuple[float] = None, n: int = 2): ...
def upsample_nearest1d(x, output_size, scales: float | None = None): ...
def upsample_nearest2d(x, output_size, scales_h: float | None = None, scales_w: float | None = None): ...
def upsample_nearest3d(x, output_size, scales_d: float | None = None, scales_h: float | None = None, scales_w: float | None = None): ...
def upsample_bicubic2d_default(x, output_size, align_corners: bool, scales_h: float | None = None, scales_w: float | None = None): ...
def reflection_pad2d(x, padding): ...
def reflection_pad2d_backward(grad_output, x, padding): ...
def rev(x, dims): ...
def constant_pad_nd(x, padding, fill_value: int = 0): ...
def range_mask_low(i: sympy.Expr): ...
def range_mask_high(i: sympy.Expr, length: sympy.Expr): ...
def range_mask(i: sympy.Expr, length: sympy.Expr): ...
def constant_boundary_condition_2d(x, fill_value, padding): ...
def pooling_size(x, i, kernel_size, stride, padding, ceil_mode): ...

fallback_max_pool2d_with_indices: Incomplete

def max_pool2d_with_indices(x, kernel_size, stride: Incomplete | None = None, padding: int = 0, dilation: int = 1, ceil_mode: bool = False): ...

fallback_max_pool2d_with_indices_backward: Incomplete

def max_pool2d_with_indices_backward(grad_output, x, kernel_size, stride, padding, dilation, ceil_mode, indices): ...
def pad_adaptive_loader(x): ...

fallback_adaptive_avg_pool2d: Incomplete

def upsample_nearest2d_backward(x, output_size: Incomplete | None = None, input_size: Incomplete | None = None, scales_h: Incomplete | None = None, scales_w: Incomplete | None = None): ...

fallback_avg_pool2d: Incomplete

def avg_pool2d(x, kernel_size, stride=(), padding: int = 0, ceil_mode: bool = False, count_include_pad: bool = True, divisor_override: Incomplete | None = None): ...

fallback_avg_pool2d_backward: Incomplete

def avg_pool2d_backward(grad_output, x, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override: Incomplete | None = None): ...
def make_reduction(reduction_type: str, override_return_dtype: Incomplete | None = None): ...
def mean(x, axis: Incomplete | None = None, keepdim: bool = False, *, dtype: Incomplete | None = None): ...
def var_mean_(x, axis, correction, keepdim, return_mean): ...
def var_(x, axis: Incomplete | None = None, *, correction: Incomplete | None = None, keepdim: bool = False): ...
def var_mean(x, axis: Incomplete | None = None, *, correction: Incomplete | None = None, keepdim: bool = False): ...
def pow_recursive(x, y, dtype): ...
def pow_native(a, b): ...
def pow(a, b): ...
def mutate_to(changed, val): ...
def fill_(x, fill_value): ...
def copy_(dst, src, non_blocking: bool = False): ...
def floordiv(a, b): ...
def truncdiv(a, b): ...
def div_mode(a, b, rounding_mode: Incomplete | None = None): ...
def mul(a, b): ...
def div_prim(a, b): ...

div: Incomplete

def fmod(a, b): ...
def rsqrt(x): ...
def sum_(x, axis: Incomplete | None = None, keepdims: bool = False, *, dtype: Incomplete | None = None): ...

reduce_amax: Incomplete
reduce_amin: Incomplete
reduce_argmax: Incomplete
reduce_argmin: Incomplete
add: Incomplete

def register_pointwise_numeric(op): ...
def register_pointwise_numeric_ldf64(op): ...

exp: Incomplete
exp2: Incomplete
expm1: Incomplete
relu: Incomplete
sigmoid: Incomplete
sqrt: Incomplete
square: Incomplete
sub: Incomplete
erf: Incomplete
maximum: Incomplete
minimum: Incomplete
logical_and: Incomplete
logical_xor: Incomplete

def register_inplace(aten_op, outplace_op): ...
def sym_size(a, dim): ...
def sym_stride(a, dim): ...
def sym_numel(a): ...
def foobar(self, *args, **kwargs) -> None: ...
