import dataclasses
import sympy
import torch
import torch.fx
from .. import codecache as codecache, config as config, ir as ir, metrics as metrics
from ..codegen.wrapper import WrapperCodeGen as WrapperCodeGen
from ..utils import cache_on_self as cache_on_self, sympy_product as sympy_product, sympy_subs as sympy_subs, sympy_symbol as sympy_symbol
from ..virtualized import V as V, ops as ops
from .common import BracesBuffer as BracesBuffer, CSEVariable as CSEVariable, CppWrapperKernelArgs as CppWrapperKernelArgs, DeferredIndentedBuffer as DeferredIndentedBuffer, ExprPrinter as ExprPrinter, IndentedBuffer as IndentedBuffer, Kernel as Kernel, KernelArgs as KernelArgs, OpOverrides as OpOverrides
from _typeshed import Incomplete
from collections.abc import Generator
from torch._prims_common import is_float_dtype as is_float_dtype
from typing import ClassVar, Dict, List

DTYPE_TO_CPP: Incomplete
DTYPE_TO_ATEN: Incomplete
INDEX_TYPE: str
RTYPE_TO_CPP: Incomplete

def reduction_init(reduction_type, dtype): ...
def reduction_combine(reduction_type, var, next_value): ...
def reduction_combine_vec(reduction_type, var, next_value): ...

index_value_name_counter: int

def argmax_argmin_prefix(reduction_type, src_dtype, tmpvar): ...
def float16_reduction_prefix(rtype): ...
def parallel_num_threads(): ...
def cpp_prefix(): ...

class CppPrinter(ExprPrinter): ...

cexpr: Incomplete

@dataclasses.dataclass
class OptimizationContext:
    key: ClassVar[str] = ...
    is_masked_load: bool = ...
    is_load_as_mask: bool = ...
    dtype: torch.dtype = ...
    ops_name: str = ...
    is_most_inner_loop_irrevelant: bool = ...
    def __init__(self, is_masked_load, is_load_as_mask, dtype, ops_name, is_most_inner_loop_irrevelant) -> None: ...

class RecordOptimizationContext:
    func_name: Incomplete
    current_node: Incomplete
    opt_ctx: Incomplete
    def __init__(self, func_name: str = '') -> None: ...
    def __enter__(self): ...
    def __exit__(self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: types.TracebackType | None) -> None: ...
    def get_opt_ctx(self): ...
    def get_fx_node(self): ...

def get_current_node_opt_ctx() -> OptimizationContext: ...

class CppVecOverrides(OpOverrides):
    """Map element-wise ops to aten vectorization C++"""
    @staticmethod
    def add(a, b): ...
    @staticmethod
    def sub(a, b): ...
    @staticmethod
    def mul(a, b): ...
    @staticmethod
    def div(a, b): ...
    @staticmethod
    def abs(x): ...
    @staticmethod
    def sin(x): ...
    @staticmethod
    def cos(x): ...
    @staticmethod
    def exp(x): ...
    @staticmethod
    def exp2(x): ...
    @staticmethod
    def expm1(x): ...
    @staticmethod
    def erf(x): ...
    @staticmethod
    def sqrt(x): ...
    @staticmethod
    def eq(x, y): ...
    @staticmethod
    def ne(x, y): ...
    @staticmethod
    def lt(x, y): ...
    @staticmethod
    def gt(x, y): ...
    @staticmethod
    def le(x, y): ...
    @staticmethod
    def ge(x, y): ...
    @staticmethod
    def and_(x, y): ...
    @staticmethod
    def rsqrt(x): ...
    @staticmethod
    def pow(a, b): ...
    @staticmethod
    def log(x): ...
    @staticmethod
    def round(x): ...
    @staticmethod
    def floor(x): ...
    @staticmethod
    def ceil(x): ...
    @staticmethod
    def trunc(x): ...
    @staticmethod
    def fmod(a, b): ...
    @staticmethod
    def lgamma(x): ...
    @staticmethod
    def tan(a): ...
    @staticmethod
    def tanh(a): ...
    @staticmethod
    def reciprocal(a): ...
    @staticmethod
    def atan(x): ...
    @staticmethod
    def acos(x): ...
    @staticmethod
    def asin(x): ...
    @staticmethod
    def log10(x): ...
    @staticmethod
    def erfc(x): ...
    @staticmethod
    def nextafter(x): ...
    @staticmethod
    def copysign(a, b): ...
    @staticmethod
    def atan2(a, b): ...
    @staticmethod
    def hypot(a, b): ...
    @staticmethod
    def atanh(x): ...
    @staticmethod
    def asinh(x): ...
    @staticmethod
    def acosh(x): ...
    @staticmethod
    def constant(val, dtype): ...
    @staticmethod
    def relu(x): ...
    @staticmethod
    def sigmoid(x): ...
    @staticmethod
    def neg(x): ...
    @staticmethod
    def floordiv(a, b): ...
    @staticmethod
    def truncdiv(a, b): ...
    @staticmethod
    def minimum(a, b): ...
    @staticmethod
    def maximum(a, b): ...
    @staticmethod
    def square(a): ...
    @staticmethod
    def where(a, b, c): ...
    @staticmethod
    def sign(x): ...
    @staticmethod
    def to_dtype(x, dtype): ...
    @staticmethod
    def log1p(x): ...
    @staticmethod
    def masked(mask, body, other): ...
    @staticmethod
    def index_expr(expr, dtype): ...

class CppOverrides(OpOverrides):
    """Map element-wise ops to C++"""
    @staticmethod
    def to_dtype(x, dtype): ...
    @staticmethod
    def abs(x): ...
    @staticmethod
    def sin(x): ...
    @staticmethod
    def cos(x): ...
    @staticmethod
    def neg(x): ...
    @staticmethod
    def exp(x): ...
    @staticmethod
    def exp2(x): ...
    @staticmethod
    def expm1(x): ...
    @staticmethod
    def erf(x): ...
    @staticmethod
    def sqrt(x): ...
    @staticmethod
    def rsqrt(x): ...
    @staticmethod
    def log1p(x): ...
    @staticmethod
    def tan(x): ...
    @staticmethod
    def tanh(x): ...
    @staticmethod
    def signbit(x): ...
    @staticmethod
    def pow(a, b): ...
    @staticmethod
    def log(x): ...
    @staticmethod
    def round(x): ...
    @staticmethod
    def floor(x): ...
    @staticmethod
    def floordiv(a, b): ...
    @staticmethod
    def ceil(x): ...
    @staticmethod
    def trunc(x): ...
    @staticmethod
    def truncdiv(a, b): ...
    @staticmethod
    def fmod(a, b): ...
    @staticmethod
    def isinf(x): ...
    @staticmethod
    def isnan(x): ...
    @staticmethod
    def lgamma(x): ...
    @staticmethod
    def acos(x): ...
    @staticmethod
    def acosh(x): ...
    @staticmethod
    def asin(x): ...
    @staticmethod
    def asinh(x): ...
    @staticmethod
    def atan2(x, y): ...
    @staticmethod
    def atan(x): ...
    @staticmethod
    def atanh(x): ...
    @staticmethod
    def copysign(x, y): ...
    @staticmethod
    def hypot(x, y): ...
    @staticmethod
    def erfc(x): ...
    @staticmethod
    def log10(x): ...
    @staticmethod
    def nextafter(x, y): ...
    @staticmethod
    def relu(x): ...
    @staticmethod
    def minimum(a, b): ...
    @staticmethod
    def maximum(a, b): ...
    @staticmethod
    def where(a, b, c): ...
    @staticmethod
    def mod(a, b): ...
    @staticmethod
    def constant(val, dtype): ...
    @staticmethod
    def index_expr(expr, dtype): ...
    @staticmethod
    def masked(mask, body, other): ...
    @staticmethod
    def logical_and(a, b): ...
    @staticmethod
    def logical_or(a, b): ...
    @staticmethod
    def rand(seed: sympy.Expr, offset: sympy.Expr, dtype): ...
    @staticmethod
    def randn(seed: sympy.Expr, offset: sympy.Expr, dtype): ...
    @staticmethod
    def sigmoid(x): ...
    @staticmethod
    def sign(x): ...

class CppKernel(Kernel):
    overrides = CppOverrides
    sexpr = cexpr
    newvar_prefix: str
    suffix: str
    call_ranges: Incomplete
    ranges: Incomplete
    itervars: Incomplete
    reduction_depth: Incomplete
    reduction_prefix: Incomplete
    reduction_suffix: Incomplete
    reduction_var_map: Incomplete
    preloads: Incomplete
    poststores: Incomplete
    num_threads: Incomplete
    def __init__(self, args, num_threads) -> None: ...
    def scale_index_with_offset(self, index: sympy.Expr, scale, itervar_idx: int = -1, offset: int = 0): ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name, index, value, mode: Incomplete | None = None) -> None: ...
    def reduction(self, name, dtype, src_dtype, reduction_type, index, value) -> None: ...
    def set_ranges(self, lengths, reduction_lengths): ...
    def size_hint(self): ...
    def codegen_loops_impl(self, loop_nest, code, worksharing) -> None: ...
    def codegen_loops(self, code, worksharing) -> None: ...
    def decide_parallel_depth(self, ranges, threads): ...
    loads: Incomplete
    compute: Incomplete
    stores: Incomplete
    cse: Incomplete
    def write_to_suffix(self) -> Generator[None, None, None]: ...

class CppVecKernel(CppKernel):
    overrides = CppVecOverrides
    tiling_factor: Incomplete
    reduction_omp_dec: Incomplete
    var_vec_buf_map: Incomplete
    def __init__(self, args, num_threads, tiling_factor: int = 0) -> None: ...
    def stride_at(self, var: sympy.Symbol, index: sympy.Expr): ...
    def is_stride1_at(self, var: sympy.Symbol, index: sympy.Expr): ...
    def is_invariant_under(self, var: sympy.Symbol, index: sympy.Expr): ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name, index, value, mode: Incomplete | None = None) -> None: ...
    def reduction(self, name, dtype, src_dtype, reduction_type, index, value) -> None: ...

class CppTile2DKernel(CppVecKernel):
    """
    A vector kernel that handles the 2d tiles with the tile size defined in `tiling_factor` on
    the inner-most loop level and one of the outer loop level (`outer_tiling_idx`). When the data
    tile is accessed in a contiguous way from the outer loop axis, a transposition is applied on the
    tile to make the access contiguous from the inner-most loop axis. Then, the same vectorization
    logic from its parent `CppVecKernel` is leveraged for load/store/compute. The transposed tile load
    and store are generated into kernel.preloads and kernel.poststores buffers.

    The loop structure looks like below:
    for ...
      for i_outer ...
        for ...
          for inner_most ...
            // generated by CppTile2DKernel
            float tmp0[16*16]; at::vec::transpose_mxn<...>(tmp0, in_ptr0 + ..., ...); // into kernel.preloads
            float tmp1[16*16]; // into kernel.preloads
            for i_inner ... { // the kernel inner loop
              vectorized loads/compute/stores (e.g., load tmp0, store tmp1) // into kernel.loads/compute/stores
            }
            at::vec::transpose_mxn(out_ptr0 + ..., tmp1, ...) // into kernel.poststores
          for inner_most ... (tail)
            // generated by CppTile2DTailKernel
            ...
      for i_outer ... (tail)
        for ...
          for ...
            // generated by CppKernel
            ...
    """
    outer_tiling_idx: Incomplete
    def __init__(self, args, num_threads, tiling_factor, outer_tiling_idx) -> None: ...
    def inner_itervar(self): ...
    def need_vec_transpose(self, index): ...
    def gen_transposed_tile_load_store(self, name, var, index, is_store): ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name, index, value, mode: Incomplete | None = None) -> None: ...
    def codegen_inner_loops(self, code) -> None: ...

class CppTile2DTailKernel(CppKernel):
    """
    A scalar kernel that handles the tail of inner-most loop split from a 2d tiling. The tile of the outer
    loop axis is handled with a kernel inner loop (see method `codegen_inner_loops`).
    """
    outer_tiling_idx: Incomplete
    tiling_factor: Incomplete
    def __init__(self, args, num_threads, tiling_factor, outer_tiling_idx) -> None: ...
    def inner_itervar(self): ...
    def transform_index(self, index): ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name, index, value, mode: Incomplete | None = None) -> None: ...
    def codegen_inner_loops(self, code) -> None: ...

class CppVecKernelChecker(CppVecKernel):
    simd_vec: bool
    fast_vec_list: Incomplete
    exit_stack: Incomplete
    load_results: Incomplete
    load_supported_dtypes: Incomplete
    store_supported_dtypes: Incomplete
    store_dtypes: Incomplete
    vec_dtype: Incomplete
    def __init__(self, args, num_threads, tiling_factor) -> None: ...
    def is_indirect_indexing(self, index: sympy.Expr): ...
    def could_vec(self, name: str, index: sympy.Expr): ...
    def is_mask(self, name: str, users: Dict[torch.fx.Node, None]): ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name, index, value, mode: Incomplete | None = None): ...
    def reduction(self, name, dtype, src_dtype, reduction_type, index, value): ...
    def is_supported_cmp(self, node: torch.fx.Node): ...
    def is_load_only_block(self, sub_graph: torch.fx.Graph): ...
    def __exit__(self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: types.TracebackType | None) -> None: ...
    def __enter__(self): ...

class CppTile2DKernelChecker(CppVecKernelChecker):
    """
    Currently, we only address the situations with following constraints.
    1. There exists one and only one fp32 load/store with outer loop var having contiguous buffer accesses.
    2. When a load/store doesn't have contiguous access in an outer loop var, the access should be
       vectorizable from the inner-most dim.
    3. No reduction.
    """
    can_tile2d: bool
    outer_tiling_idx: int
    def __init__(self, args, num_threads, tiling_factor) -> None: ...
    def check_can_tile2d(self, name: str, index: sympy.Expr): ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name, index, value, mode: Incomplete | None = None): ...
    def reduction(self, name, dtype, src_dtype, reduction_type, index, value): ...
    def __exit__(self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: types.TracebackType | None) -> None: ...

class CppKernelProxy(CppKernel):
    kernel_group: Incomplete
    loop_nest: Incomplete
    call_ranges: Incomplete
    picked_vec_isa: Incomplete
    def __init__(self, kernel_group) -> None: ...
    def codegen_nodes(self, nodes): ...
    def codegen_loops(self, code, worksharing) -> None: ...

class CppScheduling:
    scheduler: Incomplete
    def __init__(self, scheduler) -> None: ...
    def group_fn(self, sizes): ...
    kernel_group: Incomplete
    def get_kernel_group(self) -> None: ...
    @staticmethod
    def can_fuse_horizontal(node1, node2): ...
    @classmethod
    def can_fuse_vertical(cls, node1, node2): ...
    def codegen_nodes(self, nodes) -> None:
        """
        Turn an set of pre-fused nodes into a C++ kernel.
        """
    def codegen_sync(self) -> None: ...
    def flush(self) -> None: ...

class KernelGroup:
    args: Incomplete
    loops_code: Incomplete
    ws: Incomplete
    stack: Incomplete
    count: int
    def __init__(self) -> None: ...
    def new_kernel(self, cls, *args): ...
    def finalize_kernel(self, new_kernel, scheduler) -> None: ...
    def codegen_define_and_call(self, wrapper) -> None: ...

class CppWrapperKernelGroup(KernelGroup):
    args: Incomplete
    def __init__(self) -> None: ...

class WorkSharing:
    code: Incomplete
    in_parallel: bool
    num_threads: Incomplete
    stack: Incomplete
    def __init__(self, code) -> None: ...
    def parallel(self, threads) -> None: ...
    def single(self): ...
    def close(self) -> None: ...
    def __enter__(self): ...
    def __exit__(self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: types.TracebackType | None) -> None: ...

@dataclasses.dataclass
class LoopLevel:
    var: sympy.Expr = ...
    size: sympy.Expr = ...
    offset: sympy.Expr = ...
    steps: sympy.Expr = ...
    parallel: int = ...
    simd_omp: bool = ...
    picked_vec_isa: codecache.VecISA = ...
    simd_nelements: int = ...
    simd_vec: bool = ...
    collapsed: bool = ...
    reduction_var_map: Dict[str, str] = ...
    parent: LoopLevel = ...
    inner: List['LoopLevel'] = ...
    kernel: CppKernel = ...
    def get_kernels(self) -> List[CppKernel]:
        """Get all kernel objects under this loop level"""
    def set_kernel(self, kernel: CppKernel):
        """
        Set the kernel under this loop level. No split is allowed under
        this loop level.
        """
    def get_loops_at(self, depth) -> List['LoopLevel']: ...
    def is_reduction(self): ...
    def split_with_tiling(self, depth, factor): ...
    def clone(self): ...
    def lines(self): ...
    def __init__(self, var, size, offset, steps, parallel, simd_omp, picked_vec_isa, simd_nelements, simd_vec, collapsed, reduction_var_map, parent, inner, kernel) -> None: ...

@dataclasses.dataclass
class LoopNestWithSplit:
    """
    A loop-nest like structure but with some loop level split along
    the loop range into the main tiling loop and the tail. It is built
    with the `build` method as a loop nest and then split with
    `split_with_tiling` at some depth.

    A typical case is for vectorization where we typically split at the inner-most
    loop level. A more complicated case is 2D tiling where we split at
    both inner-most and outer levels.
    """
    root: List[LoopLevel] = ...
    kernel: CppKernel = ...
    @staticmethod
    def build(kernel: CppKernel):
        """Build a LoopNest with the given `kernel` as the leaf"""
    def __bool__(self) -> bool: ...
    def get_loops_at(self, depth) -> List[LoopLevel]:
        """Get all the loop levels at the given `depth` (most outer loop has depth 0)"""
    def max_parallel_depth(self):
        """
        Maximal allowed depth for parallelism:
        1) Levels without splitting and
        2) All reduction or non-reduction levels
        When the loop is split at the top level, the max depth is 1.
        """
    def is_reduction_only(self):
        """
        Whether all the loops are for reduction. Reduction loops
        are always the inner most ones.
        """
    def mark_parallel(self, par_depth) -> None: ...
    def split_with_tiling(self, depth, factor):
        """
        Split the loop into main and tail loops at given `depth` so that the range
        of the main loop has range `floor_div(range, factor) * factor` and
        the tail loop handles the remainder. The main loop is tiled
        according to the `factor`.
        """
    def __init__(self, root, kernel) -> None: ...
