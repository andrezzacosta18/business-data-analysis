import dataclasses
import torch
from . import config as config, dependencies as dependencies, ir as ir, metrics as metrics
from .dependencies import StarDep as StarDep, WeakDep as WeakDep
from .sizevars import SimplifyIndexing as SimplifyIndexing
from .utils import cache_on_self as cache_on_self, cmp as cmp, has_triton as has_triton
from .virtualized import V as V
from _typeshed import Incomplete
from torch._dynamo.utils import dynamo_timed as dynamo_timed
from typing import Dict, List, Set

log: Incomplete

def pformat(obj): ...

class OutputNode:
    unmet_dependencies: Incomplete
    inverse_users: Incomplete
    def __init__(self, dep) -> None: ...
    def is_reduction(self): ...
    def get_alias_names(self): ...
    def get_name(self): ...

class BaseSchedulerNode:
    scheduler: Incomplete
    node: Incomplete
    users: Incomplete
    inverse_users: Incomplete
    recursive_predecessors: Incomplete
    min_order: Incomplete
    max_order: Incomplete
    last_usage: Incomplete
    written: bool
    def __init__(self, scheduler: Scheduler, node: ir.Buffer) -> None: ...
    def debug_str(self):
        """Longer form printout for trace logs"""
    def debug_str_extra(self): ...
    def log_details(self) -> None: ...
    def update_mutated_names(self, renames: Dict[str, str]): ...
    def add_mutation_dep(self, dep) -> None: ...
    def set_users(self, users: List['NodeUser']): ...
    def get_aliases(self): ...
    def get_mutations(self): ...
    def has_aliasing_or_mutation(self): ...
    read_writes: Incomplete
    unmet_dependencies: Incomplete
    def set_read_writes(self, rw: dependencies.ReadWrites): ...
    def used_buffer_names(self) -> Set[str]: ...
    def prune_deps(self) -> None: ...
    def prune_redundant_deps(self, name_to_fused_node):
        """
        Prunes stardeps intended for mutation ordering
        on an upstream fused node if after fusion there is another dependency
        on the fused upstream node, making the stardep redundant

        In essence this enforces an ordering on fusions. As fusions occur, prunable stardeps will
        be incrementally removed, enabling other fusions, ensuring they are fused in order.
        """
    def get_name(self) -> str: ...
    def get_first_name(self) -> str: ...
    def get_names(self) -> Set[str]: ...
    def get_nodes(self) -> List['BaseSchedulerNode']: ...
    def get_device(self): ...
    def is_reduction(self): ...
    def is_template(self): ...
    def is_extern(self): ...
    def can_inplace(self, read_dep: dependencies.MemoryDep): ...
    def allocate(self): ...
    def can_free(self): ...
    def codegen_originating_info(self, buffer, only_once: bool = True) -> None: ...

class ExternKernelSchedulerNode(BaseSchedulerNode):
    def debug_str_extra(self): ...
    def is_extern(self): ...

class NopKernelSchedulerNode(BaseSchedulerNode): ...

class SchedulerNode(BaseSchedulerNode):
    group: Incomplete
    def __init__(self, scheduler: Scheduler, node: ir.ComputedBuffer, group_fn) -> None: ...
    def debug_str_extra(self): ...
    def get_ranges(self): ...
    def is_reduction(self): ...
    def is_template(self): ...
    def run(self, *index_vars) -> None: ...
    def mark_run(self) -> None: ...
    def ranges_from_index_vars(self, index_vars): ...
    def codegen(self, index_vars) -> None: ...
    def pointwise_read_writes(self):
        """
        Get the memory dependencies in the non-reduction axis.
        """
    def can_inplace(self, read_dep: dependencies.MemoryDep): ...

class FusedSchedulerNode(BaseSchedulerNode):
    '''
    This is a "fake" scheduler node that represents a group of scheduler nodes
    that are meant to be fused together. The way it does this is by maintaining
    its unmet dependencies as the union of its constituent nodes.
    '''
    @classmethod
    def fuse(cls, node1: BaseSchedulerNode, node2: BaseSchedulerNode): ...
    snodes: Incomplete
    scheduler: Incomplete
    node: Incomplete
    users: Incomplete
    inverse_users: Incomplete
    group: Incomplete
    recursive_predecessors: Incomplete
    unmet_dependencies: Incomplete
    min_order: Incomplete
    max_order: Incomplete
    def __init__(self, scheduler: Scheduler, snodes: List[SchedulerNode]) -> None: ...
    def get_name(self) -> str: ...
    def get_first_name(self) -> str: ...
    def get_names(self) -> Set[str]: ...
    def debug_str_extra(self): ...
    def used_buffer_names(self) -> Set[str]: ...
    def get_nodes(self) -> List[BaseSchedulerNode]: ...
    def is_reduction(self): ...
    def is_template(self): ...
    def get_device(self): ...
    def has_aliasing_or_mutation(self): ...
    def update_mutated_names(self, renames: Dict[str, str]): ...
    def add_mutation_dep(self, name) -> None: ...
    def set_users(self, users: List['NodeUser']): ...
    def get_aliases(self) -> None: ...
    def get_mutations(self) -> None: ...
    def can_inplace(self, read_dep: dependencies.MemoryDep): ...
    def allocate(self) -> None: ...
    def can_free(self) -> None: ...

def pick_loop_order(stride_lengths, sizes, priority_idx=()):
    """
    A heuristic to decide loop iteration orders.  This has not been well
    tuned and may be something we should autotune.
    """

@dataclasses.dataclass
class NodeUser:
    node: BaseSchedulerNode
    can_inplace: bool = ...
    def get_name(self): ...
    def __init__(self, node, can_inplace) -> None: ...

class Scheduler:
    backends: Incomplete
    nodes: Incomplete
    available_buffer_names: Incomplete
    name_to_node: Incomplete
    name_to_fused_node: Incomplete
    mutation_real_name: Incomplete
    mutation_renames: Incomplete
    num_orig_nodes: Incomplete
    current_device: Incomplete
    buffer_names_to_free: Incomplete
    buffer_names_no_longer_needed: Incomplete
    def __init__(self, nodes) -> None: ...
    def debug_draw_graph(self) -> None:
        """Generate an image of the graph for debugging"""
    def debug_print_nodes(self, label) -> None: ...
    def compute_dependencies(self):
        """
        Create dependency edges between nodes, handling aliasing and
        mutation properly.
        """
    def dead_node_elimination(self) -> None:
        """
        Remove any nodes without users
        """
    def topological_sort_schedule(self):
        """
        Ensure self.nodes is in topologically sorted order
        """
    def compute_predecessors(self) -> None:
        """
        Populate each node.recursive_predecessors
        """
    def fuse_nodes(self) -> None:
        """
        Mutates self.nodes to combine nodes into FusedSchedulerNodes.
        """
    def fuse_nodes_once(self):
        """
        Mutates self.nodes to combine nodes into FusedSchedulerNodes.

        This relies on two key functions to control the logic:
            - self.can_fuses(): checks if a fusion is legal
            - self.score_fusion(): assigns priority to a given fusion
        """
    def prune_redundant_deps(self) -> None: ...
    def get_possible_fusions(self):
        """
        Helper to find all legal fusion opportunities, sorted by self.score_fusion()
        """
    def will_fusion_create_cycle(self, node1, node2):
        """Finds whether there's a path from src to dst caused indirectly by fusion"""
    def can_fuse(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode):
        """
        Determine if it is possible to combine node1 and node2 into a
        single fused node.
        """
    def can_fuse_vertical(self, node1, node2):
        """
        Check if it is legal to fuse a consumer (node2) into a producer (node1).

        We can fuse them if all the reads of node2 either match
        corresponding writes in node1, or are written by nodes that can
        be scheduled before the fusion of node1 and node2.
        """
    def score_fusion(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode):
        """
        Assign a score (higher comes first) to the fusion of node1
        and node2.  When different fusions conflict with each other,
        this is the way we decide what order to run them in.

        Our current score is based on:
        - Estimate of the saved memory operations
        - Fusions closer together in original order
        """
    def score_fusion_memory(self, node1, node2):
        """
        The first term in our fusion score that estimates number of saved memory operations.
        """
    def score_fusion_key(self, nodes):
        """
        Shim for list.sort(key=...)
        """
    def compute_last_usage(self) -> None:
        """
        Populate node.last_usage
        """
    def free_buffers(self) -> None:
        """Free any buffers that are no longer needed"""
    def remove_kernel_local_buffers(self) -> None:
        """
        Any buffers that are both created and have a last use in the
        same kernel can be removed.
        """
    def remove_buffer(self, name) -> None: ...
    def flush(self) -> None: ...
    def codegen_extern_call(self, scheduler_node: ExternKernelSchedulerNode): ...
    def create_backend(self, device: torch.device): ...
    def get_backend(self, device: torch.device): ...
    def codegen(self) -> None: ...
