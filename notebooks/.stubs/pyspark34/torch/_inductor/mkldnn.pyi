import torch
import torch.nn as nn
from . import config as config
from .fx_utils import matches_module_function_pattern as matches_module_function_pattern
from _typeshed import Incomplete
from torch._dynamo.utils import fake_mode_from_tensors as fake_mode_from_tensors
from torch.fx.experimental.optimization import matches_module_pattern as matches_module_pattern, replace_node_module as replace_node_module
from torch.fx.experimental.symbolic_shapes import guard_int as guard_int
from torch.fx.passes.shape_prop import ShapeProp as ShapeProp

class UnaryAttr:
    op_name: Incomplete
    scalars_attr: Incomplete
    algorithm_attr: Incomplete
    def __init__(self, op_name: str, scalars_attr: Incomplete | None = None, algorithm_attr: Incomplete | None = None) -> None: ...
    def __call__(self, unary_module: nn.Module): ...

def is_bfloat16_module(m): ...
def is_group_depthwise_conv_transpose(m): ...
def check_node_kind(current_node, modules, node_kind): ...
def check_node_is_binary(node): ...
def check_binary_op_kwargs_is_default(node): ...

class ConvUnary2d(nn.Conv2d):
    def __init__(self, conv: nn.Module, unary: nn.Module | None, input_size: list) -> None: ...
    def forward(self, input): ...

class ConvBinary2d(nn.Conv2d):
    def __init__(self, conv: nn.Module, binary_op_name: str, input_size: list) -> None: ...
    def forward(self, input, other): ...

class PackedLinear(nn.Linear):
    def __init__(self, linear: nn.Module, input_size: list) -> None: ...
    def forward(self, input): ...

class LinearUnary(nn.Linear):
    def __init__(self, linear: nn.Module, unary: nn.Module) -> None: ...
    def forward(self, input): ...

class LinearBinary(nn.Linear):
    def __init__(self, linear: nn.Module, binary_op_name: str) -> None: ...
    def forward(self, input, other): ...

class ConvTransposeUnary2d(nn.ConvTranspose2d):
    def __init__(self, conv_transpose: nn.Module, unary: nn.Module | None, input_size: list) -> None: ...
    def forward(self, input): ...

def packed_conv_eval(conv: nn.Module, input_size: list): ...
def packed_conv_transpose_eval(conv_transpose: nn.Module, input_size: list): ...
def fused_conv_unary_eval(conv: nn.Module, unary: nn.Module, input_size: list): ...
def fused_conv_binary_eval(conv: nn.Module, binary_op_name: str, input_size: list): ...
def fused_conv_binary_unary_eval(conv_binary: nn.Module, unary: nn.Module, input_size: list): ...
def packed_linear_eval(linear: nn.Module, input_size: list): ...
def fused_linear_unary_eval(linear: nn.Module, unary: nn.Module, input_size: list): ...
def fused_linear_binary_eval(linear: nn.Module, attr: str, input_size: list): ...
def fused_conv_transpose_unary_eval(conv_transpose: nn.Module, unary: nn.Module, input_size: list): ...
def mkldnn_fuse_fx(gm: torch.fx.GraphModule, example_inputs): ...
def create_unary_module(node: torch.fx.node): ...
def fuse_unary(gm: torch.fx.GraphModule): ...
def replace_and_fuse_for_binary(computation_node, node, fuse_func, attr, modules, index_node, index_pointwise) -> None: ...
def binary_inputs_meta_is_same(binary_node): ...
def fuse_binary(gm: torch.fx.GraphModule): ...
def convert_outplace_to_inplace(gm: torch.fx.GraphModule): ...
def pack_module(gm: torch.fx.GraphModule): ...

computation_op_unary_op_fusion_map: Incomplete
unary_modules_map: Incomplete
unary_ops: Incomplete
binary_attr: Incomplete
computation_op_binary_op_fusion_map: Incomplete
computation_op_packed_map: Incomplete
supported_index_list: Incomplete
