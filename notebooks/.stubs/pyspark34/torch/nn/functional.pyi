from .common_types import _ratio_any_t, _size_1_t, _size_2_opt_t, _size_2_t, _size_3_opt_t, _size_any_t
from torch import Tensor as Tensor
from torch.types import _dtype, _size
from typing import Any, Callable, Dict, List, Sequence, Tuple

GRID_SAMPLE_INTERPOLATION_MODES = Dict[str, int]
GRID_SAMPLE_PADDING_MODES = Dict[str, int]

def fractional_max_pool2d_with_indices(input: Tensor, kernel_size: _size, output_size: _size | None = ..., output_ratio: _ratio_any_t | None = ..., return_indices: bool = ..., _random_samples: Tensor | None = ...) -> Tuple[Tensor, Tensor]: ...
def fractional_max_pool3d_with_indices(input: Tensor, kernel_size: _size, output_size: _size | None = ..., output_ratio: _ratio_any_t | None = ..., return_indices: bool = ..., _random_samples: Tensor | None = ...) -> Tuple[Tensor, Tensor]: ...
def max_pool1d_with_indices(input: Tensor, kernel_size: _size, stride: _size | None = ..., padding: _size = ..., dilation: _size = ..., ceil_mode: bool = ..., return_indices: bool = ...) -> Tuple[Tensor, Tensor]: ...
def max_pool2d_with_indices(input: Tensor, kernel_size: _size, stride: _size | None = ..., padding: _size = ..., dilation: _size = ..., ceil_mode: bool = ..., return_indices: bool = ...) -> Tuple[Tensor, Tensor]: ...
def max_pool3d_with_indices(input: Tensor, kernel_size: _size, stride: _size | None = ..., padding: _size = ..., dilation: _size = ..., ceil_mode: bool = ..., return_indices: bool = ...) -> Tuple[Tensor, Tensor]: ...
def max_unpool1d(input: Tensor, indices: Tensor, kernel_size: _size, stride: _size | None = ..., padding: _size = ..., output_size: _size | None = ...) -> Tensor: ...
def max_unpool2d(input: Tensor, indices: Tensor, kernel_size: _size, stride: _size | None = ..., padding: _size = ..., output_size: _size | None = ...) -> Tensor: ...
def max_unpool3d(input: Tensor, indices: Tensor, kernel_size: _size, stride: _size | None = ..., padding: _size = ..., output_size: _size | None = ...) -> Tensor: ...
def lp_pool1d(input: Tensor, norm_type: float, kernel_size: _size_1_t, stride: _size | None | int | None = ..., ceil_mode: bool = ...) -> Tensor: ...
def lp_pool2d(input: Tensor, norm_type: float, kernel_size: _size_2_t, stride: _size | None | int | None = ..., ceil_mode: bool = ...) -> Tensor: ...
def adaptive_max_pool1d_with_indices(input: Tensor, output_size: _size, return_indices: bool = ...) -> Tuple[Tensor, Tensor]: ...
def adaptive_max_pool2d_with_indices(input: Tensor, output_size: _size_2_opt_t, return_indices: bool = ...) -> Tuple[Tensor, Tensor]: ...
def adaptive_max_pool3d_with_indices(input: Tensor, output_size: _size_3_opt_t, return_indices: bool = ...) -> Tuple[Tensor, Tensor]: ...
def adaptive_avg_pool1d(input: Tensor, output_size: _size_1_t) -> Tensor: ...
def adaptive_avg_pool2d(input: Tensor, output_size: _size_2_opt_t) -> Tensor: ...
def adaptive_avg_pool3d(input: Tensor, output_size: _size_3_opt_t) -> Tensor: ...
def dropout(input: Tensor, p: float = ..., training: bool = ..., inplace: bool = ...) -> Tensor: ...
def alpha_dropout(input: Tensor, p: float = ..., training: bool = ..., inplace: bool = ...) -> Tensor: ...
def dropout1d(input: Tensor, p: float = ..., training: bool = ..., inplace: bool = ...) -> Tensor: ...
def dropout2d(input: Tensor, p: float = ..., training: bool = ..., inplace: bool = ...) -> Tensor: ...
def dropout3d(input: Tensor, p: float = ..., training: bool = ..., inplace: bool = ...) -> Tensor: ...
def feature_alpha_dropout(input: Tensor, p: float = ..., training: bool = ..., inplace: bool = ...) -> Tensor: ...
def threshold(input: Tensor, threshold: float, value: float, inplace: bool = ...) -> Tensor: ...
def relu(input: Tensor, inplace: bool = ...) -> Tensor: ...
def glu(input: Tensor, dim: int = ...) -> Tensor: ...
def hardtanh(input: Tensor, min_val: float = ..., max_val: float = ..., inplace: bool = ...) -> Tensor: ...
def relu6(input: Tensor, inplace: bool = ...) -> Tensor: ...
def elu(input: Tensor, alpha: float = ..., inplace: bool = ...) -> Tensor: ...
def selu(input: Tensor, inplace: bool = ...) -> Tensor: ...
def celu(input: Tensor, alpha: float = ..., inplace: bool = ...) -> Tensor: ...
def leaky_relu(input: Tensor, negative_slope: float = ..., inplace: bool = ...) -> Tensor: ...
def prelu(input: Tensor, weight: Tensor) -> Tensor: ...
def rrelu(input: Tensor, lower: float = ..., upper: float = ..., training: bool = ..., inplace: bool = ...) -> Tensor: ...
def gelu(input: Any, approximate: str = ...): ...
def hardshrink(input: Tensor, lambd: float = ...) -> Tensor: ...
def tanhshrink(input: Any): ...
def softsign(input: Any): ...
def softmin(input: Tensor, dim: int | None = ..., _stacklevel: int = ..., dtype: _dtype | None = ...) -> Tensor: ...
def softmax(input: Tensor, dim: int | None = ..., _stacklevel: int = ..., dtype: _dtype | None = ...) -> Tensor: ...
def gumbel_softmax(logits: Tensor, tau: float = ..., hard: bool = ..., eps: float = ..., dim: int = ...) -> Tensor: ...
def log_softmax(input: Tensor, dim: int | None = ..., _stacklevel: int = ..., dtype: _dtype | None = ...) -> Tensor: ...
def tanh(input: Any): ...
def sigmoid(input: Any) -> Tensor: ...
def hardsigmoid(input: Tensor, inplace: bool = False) -> Tensor: ...
def linear(input: Tensor, weight: Tensor, bias: Tensor | None = ...) -> Tensor: ...
def bilinear(input1: Tensor, input2: Tensor, weight: Tensor, bias: Tensor | None = ...) -> Tensor: ...
def silu(input: Tensor, inplace: bool = False) -> Tensor: ...
def mish(input: Tensor, inplace: bool = False) -> Tensor: ...
def hardswish(input: Tensor, inplace: bool = False) -> Tensor: ...
def embedding(input: Tensor, weight: Tensor, padding_idx: int | None = ..., max_norm: float | None = ..., norm_type: float = ..., scale_grad_by_freq: bool = ..., sparse: bool = ...) -> Tensor: ...
def embedding_bag(input: Tensor, weight: Tensor, offsets: Tensor | None = ..., max_norm: float | None = ..., norm_type: float = ..., scale_grad_by_freq: bool = ..., mode: str = ..., sparse: bool = ..., per_sample_weights: Tensor | None = ..., include_last_offset: bool = ..., padding_idx: int | None = ...) -> Tensor: ...
def batch_norm(input: Tensor, running_mean: Tensor | None, running_var: Tensor | None, weight: Tensor | None = ..., bias: Tensor | None = ..., training: bool = ..., momentum: float = ..., eps: float = ...) -> Tensor: ...
def instance_norm(input: Tensor, running_mean: Tensor | None = ..., running_var: Tensor | None = ..., weight: Tensor | None = ..., bias: Tensor | None = ..., use_input_stats: bool = ..., momentum: float = ..., eps: float = ...) -> Tensor: ...
def layer_norm(input: Tensor, normalized_shape: Sequence[int], weight: Tensor | None = ..., bias: Tensor | None = ..., eps: float = ...) -> Tensor: ...
def group_norm(input: Tensor, num_groups: int, weight: Tensor | None = ..., bias: Tensor | None = ..., eps: float = ...) -> Tensor: ...
def local_response_norm(input: Tensor, size: int, alpha: float = ..., beta: float = ..., k: float = ...) -> Tensor: ...
def ctc_loss(log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor, blank: int = ..., reduction: str = ..., zero_infinity: bool = ...) -> Tensor: ...
def nll_loss(input: Tensor, target: Tensor, weight: Tensor | None = ..., size_average: bool | None = ..., ignore_index: int = ..., reduce: bool | None = ..., reduction: str = ...) -> Tensor: ...
def poisson_nll_loss(input: Tensor, target: Tensor, log_input: bool = ..., full: bool = ..., size_average: bool | None = ..., eps: float = ..., reduce: bool | None = ..., reduction: str = ...) -> Tensor: ...
def gaussian_nll_loss(input: Tensor, target: Tensor, var: Tensor, full: bool | None = ..., eps: float | None = ..., reduction: str | None = ...) -> Tensor: ...
def kl_div(input: Tensor, target: Tensor, size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ..., log_target: bool = ...) -> Tensor: ...
def cross_entropy(input: Tensor, target: Tensor, weight: Tensor | None = ..., size_average: bool | None = ..., ignore_index: int = ..., reduce: bool | None = ..., reduction: str = ..., label_smoothing: float = ...) -> Tensor: ...
def binary_cross_entropy(input: Tensor, target: Tensor, weight: Tensor | None = ..., size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ...) -> Tensor: ...
def binary_cross_entropy_with_logits(input: Tensor, target: Tensor, weight: Tensor | None = ..., size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ..., pos_weight: Tensor | None = ...) -> Tensor: ...
def smooth_l1_loss(input: Tensor, target: Tensor, size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ..., beta: float = ...) -> Tensor: ...
def huber_loss(input: Tensor, target: Tensor, reduction: str = ..., delta: float = ...) -> Tensor: ...
def l1_loss(input: Tensor, target: Tensor, size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ...) -> Tensor: ...
def mse_loss(input: Tensor, target: Tensor, size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ...) -> Tensor: ...
def margin_ranking_loss(input1: Tensor, input2: Tensor, target: Tensor, margin: float = ..., size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ...) -> Tensor: ...
def hinge_embedding_loss(input: Tensor, target: Tensor, margin: float = ..., size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ...) -> Tensor: ...
def multilabel_margin_loss(input: Tensor, target: Tensor, size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ...) -> Tensor: ...
def soft_margin_loss(input: Tensor, target: Tensor, size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ...) -> Tensor: ...
def multilabel_soft_margin_loss(input: Tensor, target: Tensor, weight: Tensor | None = ..., size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ...) -> Tensor: ...
def cosine_embedding_loss(input1: Tensor, input2: Tensor, target: Tensor, margin: float = ..., size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ...) -> Tensor: ...
def multi_margin_loss(input: Tensor, target: Tensor, p: int = ..., margin: float = ..., weight: Tensor | None = ..., size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ...) -> Tensor: ...
def upsample(input: Any, size: Any | None = ..., scale_factor: Any | None = ..., mode: str = ..., align_corners: Any | None = ...): ...
def interpolate(input: Any, size: Any | None = ..., scale_factor: Any | None = ..., mode: str = ..., align_corners: Any | None = ..., recompute_scale_factor: Any | None = ..., antialias: bool = ...): ...
def upsample_nearest(input: Any, size: Any | None = ..., scale_factor: Any | None = ...): ...
def upsample_bilinear(input: Any, size: Any | None = ..., scale_factor: Any | None = ...): ...
def grid_sample(input: Tensor, grid: Tensor, mode: str = ..., padding_mode: str = ..., align_corners: Any | None = ...) -> Tensor: ...
def affine_grid(theta: Tensor, size: List[int], align_corners: Any | None = ...) -> Tensor: ...
def pad(input: Tensor, pad: Sequence[int], mode: str = ..., value: float = ...) -> Tensor: ...
def pairwise_distance(x1: Tensor, x2: Tensor, p: float = ..., eps: float = ..., keepdim: bool = ...) -> Tensor: ...
def triplet_margin_loss(anchor: Tensor, positive: Tensor, negative: Tensor, margin: float = ..., p: float = ..., eps: float = ..., swap: bool = ..., size_average: bool | None = ..., reduce: bool | None = ..., reduction: str = ...) -> Tensor: ...
def triplet_margin_with_distance_loss(anchor: Tensor, positive: Tensor, negative: Tensor, *, distance_function: Callable[[Tensor, Tensor], Tensor] | None = ..., margin: float = ..., swap: bool = ..., reduction: str = ...) -> Tensor: ...
def normalize(input: Tensor, p: float = ..., dim: int = ..., eps: float = ..., out: Tensor | None = ...) -> Tensor: ...
def assert_int_or_pair(arg: Any, arg_name: Any, message: Any) -> None: ...
def unfold(input: Tensor, kernel_size: _size_any_t, dilation: _size_any_t = ..., padding: _size_any_t = ..., stride: _size_any_t = ...) -> Tensor: ...
def fold(input: Tensor, output_size: _size_any_t, kernel_size: _size_any_t, dilation: _size_any_t = ..., padding: _size_any_t = ..., stride: _size_any_t = ...) -> Tensor: ...
def multi_head_attention_forward(query: Tensor, key: Tensor, value: Tensor, embed_dim_to_check: int, num_heads: int, in_proj_weight: Tensor | None, in_proj_bias: Tensor | None, bias_k: Tensor | None, bias_v: Tensor | None, add_zero_attn: bool, dropout_p: float, out_proj_weight: Tensor, out_proj_bias: Tensor | None, training: bool = True, key_padding_mask: Tensor | None = None, need_weights: bool = True, attn_mask: Tensor | None = None, use_separate_proj_weight: bool = False, q_proj_weight: Tensor | None = None, k_proj_weight: Tensor | None = None, v_proj_weight: Tensor | None = None, static_k: Tensor | None = None, static_v: Tensor | None = None, average_attn_weights: bool = True, is_causal: bool = False) -> Tuple[Tensor, Tensor | None]: ...

fractional_max_pool2d: Callable
fractional_max_pool3d: Callable
max_pool1d: Callable
max_pool2d: Callable
max_pool3d: Callable
adaptive_max_pool1d: Callable
adaptive_max_pool2d: Callable
adaptive_max_pool3d: Callable
avg_pool2d: Callable
avg_pool3d: Callable
hardtanh_: Callable
elu_: Callable
leaky_relu_: Callable
logsigmoid: Callable
softplus: Callable
softshrink: Callable
one_hot: Callable
scaled_dot_product_attention: Callable
