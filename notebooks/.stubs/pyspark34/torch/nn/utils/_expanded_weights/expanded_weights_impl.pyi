import torch
from _typeshed import Incomplete
from collections.abc import Generator
from torch._decomp import decomposition_table as decomposition_table
from torch.utils._pytree import tree_map_only as tree_map_only
from typing import Callable, Dict

HANDLED_FUNCTIONS: Dict[Callable, torch.autograd.Function]
aten: Incomplete
expanded_weights_rnn_decomps: Incomplete

def batch_second(args, kwargs) -> Generator[None, None, None]: ...
def allow_smaller_batches(args, kwargs) -> Generator[None, None, None]: ...
def setup_rnn(use_input_variant, args, kwargs) -> Generator[None, None, None]: ...
def implements_per_sample_grads(torch_function): ...

class ExpandedWeight(torch.Tensor):
    batch_size: Incomplete
    batch_first: bool
    allow_smaller_batches: bool
    orig_weight: Incomplete
    loss_reduction: Incomplete
    def __init__(self, orig_weight, batch_size, loss_reduction) -> None: ...
    handled_functions = HANDLED_FUNCTIONS
    def __new__(cls, orig_weight, batch_size, loss_reduction): ...
    @classmethod
    def __torch_function__(cls, func, _, args=(), kwargs: Incomplete | None = None): ...
    @property
    def dtype(self): ...
    @property
    def data(self): ...
    @property
    def shape(self): ...
    @property
    def device(self): ...
    @property
    def is_cuda(self): ...
    def data_ptr(self): ...
    def get_device(self): ...
    def set_allow_smaller_batches(self, is_allow_smaller_batches) -> None: ...
    def set_batch_first(self, is_batch_first: bool = True) -> None: ...
