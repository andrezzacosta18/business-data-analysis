from .parametrization import BiasHook as BiasHook, FakeStructuredSparsity as FakeStructuredSparsity
from torch import Tensor as Tensor, nn as nn
from torch.nn.utils import parametrize as parametrize
from torch.nn.utils.parametrize import ParametrizationList as ParametrizationList
from typing import Callable

def prune_linear(linear: nn.Linear) -> None: ...
def prune_linear_linear(linear1: nn.Linear, linear2: nn.Linear) -> None: ...
def prune_linear_activation_linear(linear1: nn.Linear, activation: Callable[[Tensor], Tensor] | None, linear2: nn.Linear): ...
def prune_conv2d_padded(conv2d_1: nn.Conv2d) -> None: ...
def prune_conv2d(conv2d: nn.Conv2d) -> None: ...
def prune_conv2d_conv2d(conv2d_1: nn.Conv2d, conv2d_2: nn.Conv2d) -> None: ...
def prune_conv2d_activation_conv2d(conv2d_1: nn.Conv2d, activation: Callable[[Tensor], Tensor] | None, conv2d_2: nn.Conv2d):
    """
    Fusion Pattern for conv2d -> some activation module / function -> conv2d layers
    """
def prune_conv2d_pool_activation_conv2d(c1: nn.Conv2d, pool: nn.Module, activation: Callable[[Tensor], Tensor] | None, c2: nn.Conv2d) -> None: ...
def prune_conv2d_activation_pool_conv2d(c1: nn.Conv2d, activation: Callable[[Tensor], Tensor] | None, pool: nn.Module, c2: nn.Conv2d) -> None: ...
def prune_conv2d_pool_flatten_linear(conv2d: nn.Conv2d, pool: nn.Module, flatten: Callable[[Tensor], Tensor] | None, linear: nn.Linear) -> None: ...
def prune_lstm_output_linear(lstm: nn.LSTM, getitem: Callable, linear: nn.Linear) -> None: ...
def prune_lstm_output_layernorm_linear(lstm: nn.LSTM, getitem: Callable, layernorm: nn.LayerNorm | None, linear: nn.Linear) -> None: ...
