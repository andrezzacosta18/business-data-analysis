import torch
import torch._prims_common as utils
from _typeshed import Incomplete
from enum import Enum
from torch import Tensor as Tensor, sym_float as sym_float, sym_int as sym_int
from torch._decomp import register_decomposition as register_decomposition
from torch._prims_common import IntLike as IntLike, NumberType as NumberType, TensorLike as TensorLike, TensorSequenceType as TensorSequenceType
from torch._prims_common.wrappers import out_wrapper as out_wrapper
from torch.fx.experimental.symbolic_shapes import guard_int as guard_int
from torch.utils._pytree import tree_flatten as tree_flatten, tree_map as tree_map
from typing import Callable, List, Tuple

DispatchKey: Incomplete
aten: Incomplete

class Reduction(Enum):
    NONE: int
    MEAN: int
    SUM: int

def type_casts(f: Callable, type_promotion: utils.ELEMENTWISE_TYPE_PROMOTION_KIND, compute_dtype_only: bool = False): ...

compute_only_pw_cast_for_opmath: Incomplete
pw_cast_for_opmath: Incomplete
pw_cast_for_int_to_real: Incomplete

def tanh_backward(out_grad: Tensor, y: Tensor): ...
def sigmoid_backward(out_grad: Tensor, y: Tensor): ...
def softplus_backward(out_grad: Tensor, x: Tensor, beta: float, threshold: float): ...
def elu_backward(grad_output: Tensor, alpha: float, scale: float, input_scale: float, is_result: bool, self_or_result: Tensor): ...
def fill_scalar(self, value): ...
def fill_tensor(self, value: Tensor): ...
def hardsigmoid(self) -> Tensor: ...
def hardsigmoid_backward(grad_output: Tensor, self: Tensor): ...
def hardtanh_backward(grad_output: Tensor, self: Tensor, min_val: float, max_val: float): ...
def hardshrink_backward(grad_out: Tensor, self: Tensor, lambd: float): ...
def hardswish(self) -> Tensor: ...
def hardswish_backward(grad_output: Tensor, self: Tensor) -> Tensor: ...
def threshold_backward(grad_output: Tensor, self: Tensor, threshold: float): ...
def leaky_relu_backward(grad_output: Tensor, self: Tensor, negative_slope: float, self_is_result: bool): ...
def gelu_backward(grad: Tensor, self: Tensor, approximate: str = 'none'): ...
def mish_backward(grad_output: Tensor, input: Tensor): ...
def silu(self) -> Tensor: ...
def silu_backward(grad_output: Tensor, self: Tensor) -> Tensor: ...
def softshrink_backward(grad_output: Tensor, self: Tensor, lambd: float) -> Tensor: ...
def rrelu_with_noise_backward(grad_output: Tensor, self: Tensor, noise: Tensor, lower: float, upper: float, training: bool, self_is_result: bool) -> Tensor: ...
def log_sigmoid_backward(grad_output: Tensor, self: Tensor, buffer: Tensor) -> Tensor: ...
def apply_loss_reduction(loss: Tensor, reduction: int): ...
def to_real_dtype(dtype: torch.dtype): ...
def mse_loss(self, target: Tensor, reduction: int = ...) -> Tensor: ...
def mse_loss_backward(grad_output: Tensor, input: Tensor, target: Tensor, reduction: int): ...
def huber_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, reduction: int, delta: float): ...
def huber_loss_backward_out(grad_output: Tensor, self: Tensor, target: Tensor, reduction: int, delta: float, grad_input: Tensor): ...
def glu_backward(grad_output: Tensor, self: Tensor, dim: int) -> Tensor: ...
def nll_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, weight: Tensor | None, reduction: int, ignore_index: int, total_weight: Tensor) -> Tensor: ...
def nll_loss2d_backward(grad_output: Tensor, self: Tensor, target: Tensor, weight: Tensor | None, reduction: int, ignore_index: int, total_weight: Tensor) -> Tensor: ...
def binary_cross_entropy(self, target: Tensor, weight: Tensor | None = None, reduction: int = ...) -> Tensor: ...
def binary_cross_entropy_backward(grad_output: Tensor, self: Tensor, target: Tensor, weight: Tensor | None = None, reduction: int = ...) -> Tensor: ...
def soft_margin_loss(input: Tensor, target: Tensor, reduction: int = ...) -> Tensor: ...
def soft_margin_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, reduction: int = ...) -> Tensor: ...
def slice_backward(grad_output: Tensor, input_sizes: List[int], dim: int, start: int, end: int, step: int): ...
def slice_forward(self, dim: int = 0, start: int | None = None, end: int | None = None, step: int = 1): ...
def select_backward(grad_output: Tensor, input_sizes: List[int], dim: int, index: int): ...
def diagonal_backward(grad_output: Tensor, input_sizes: List[int], offset: int, dim1: int, dim2: int): ...
def im2col(input: Tensor, kernel_size: List[int], dilation: List[int], padding: List[int], stride: List[int]) -> Tensor: ...
def col2im(input: Tensor, output_size: List[int], kernel_size: List[int], dilation: List[int], padding: List[int], stride: List[int]) -> Tensor: ...
def native_dropout_backward(grad_output: Tensor, mask: Tensor, scale: float): ...
def unfold_backward(grad: Tensor, input_size: List[int], dimension: int, size: int, step: int) -> Tensor: ...
def logit_backward(grad_output: Tensor, self: Tensor, eps: float | None = None) -> Tensor: ...
def native_dropout(input: Tensor, p: float, train: bool | None): ...
def rsub_Tensor(self, other: Tensor, alpha: float = 1) -> Tensor: ...
def rsub_Scalar(self, other: float, alpha: float = 1) -> Tensor: ...
def embedding(weight: Tensor, indices: Tensor, padding_idx: int = -1, scale_grad_by_freq: bool = False, sparse: bool = False) -> Tensor: ...
def embedding_dense_backward(grad_output: Tensor, indices: Tensor, num_weights: int, padding_idx: int, scale_grad_by_freq: bool): ...
def prod(x: List[int]): ...
def split_with_sizes(self, split_sizes: List[int], dim: int = 0) -> List[Tensor]: ...
def split(self, split_size: int, dim: int = 0) -> List[Tensor]: ...
def addmm(self, mat1: Tensor, mat2: Tensor, beta: int = 1, alpha: int = 1): ...
def native_group_norm_backward(grad_output: Tensor, input: Tensor, mean: Tensor, rstd: Tensor, gamma: Tensor | None, N: int, C: int, HxW: int, group: int, output_mask: List[bool]) -> Tuple[Tensor | None, Tensor | None, Tensor | None]: ...
def native_layer_norm_backward(grad_out: Tensor, input: Tensor, normalized_shape: List[int], mean: Tensor, rstd: Tensor, weight: Tensor | None, bias: Tensor | None, output_mask: List[bool]) -> Tuple[Tensor | None, Tensor | None, Tensor | None]: ...
def native_batch_norm_helper(input: Tensor, weight: Tensor | None, bias: Tensor | None, running_mean: Tensor | None, running_var: Tensor | None, training: bool, momentum: float, eps: float, functional: bool) -> Tuple[Tensor, Tensor, Tensor, Tensor | None, Tensor | None]: ...
def native_batch_norm(input: Tensor, weight: Tensor | None, bias: Tensor | None, running_mean: Tensor | None, running_var: Tensor | None, training: bool, momentum: float, eps: float) -> Tuple[Tensor, Tensor, Tensor]: ...
def native_batch_norm_decomposition(input: Tensor, weight: Tensor | None, bias: Tensor | None, running_mean: Tensor | None, running_var: Tensor | None, training: bool, momentum: float, eps: float) -> Tuple[Tensor, Tensor, Tensor]: ...
def unsafe_chunk_py_impl(tensor, chunks, dim: int = 0) -> List[Tensor]: ...
def nop_decomposition(x): ...
def cudnn_batch_norm(input: Tensor, weight: Tensor, bias: Tensor | None, running_mean: Tensor | None, running_var: Tensor | None, training: bool, exponential_average_factor: float, epsilon: float): ...
def native_batch_norm_backward(grad_out: Tensor, input: Tensor, weight: Tensor | None, running_mean: Tensor | None, running_var: Tensor | None, save_mean: Tensor | None, save_invstd: Tensor | None, train: bool, eps: float, output_mask: List[bool]) -> Tuple[Tensor, Tensor | None, Tensor | None]: ...
def cudnn_batch_norm_backward(input: Tensor, grad_output: Tensor, weight: Tensor, running_mean: Tensor | None, running_var: Tensor | None, save_mean: Tensor | None, save_var: Tensor | None, epsilon: float, reserveSpace: Tensor): ...
def adaptive_avg_pool2d(input: Tensor, output_size: Tuple[int, int]): ...
def index_add_(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike, *, alpha: NumberType = 1): ...
def index_add(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike, *, alpha: NumberType = 1): ...
def index_copy_(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike): ...
def index_copy(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike): ...
def log_sigmoid_forward(self) -> Tuple[Tensor, Tensor]: ...
def uniform(x: Tensor, low: bool | int | float = 0.0, high: bool | int | float = 1.0): ...
def uniform_(self, low: int = 0, high: int = 1, generator: Incomplete | None = None): ...
def upsample_compute_output_size(input_size, output_size, scale_factors): ...
def get_scale_value(scales, idx): ...
def upsample_nearest1d_vec(input, output_size, scale_factors): ...
def upsample_nearest2d_vec(input, output_size, scale_factors): ...
def upsample_nearest3d_vec(input, output_size, scale_factors): ...
def upsample_nearest1d(input: Tensor, output_size: List[int], scales: float | None = None) -> Tensor: ...
def upsample_nearest2d(input: Tensor, output_size: List[int], scales_h: float | None = None, scales_w: float | None = None) -> Tensor: ...
def upsample_nearest3d(input: Tensor, output_size: List[int], scales_d: float | None = None, scales_h: float | None = None, scales_w: float | None = None) -> Tensor: ...
def gather_params(params, has_biases, has_projections): ...
def params_hiddens(params, hiddens, i, bidirectional): ...
def update_hidden_for_packed(cur_hidden, last_batch_size, batch_size, hiddens): ...
def update_hidden_for_packed_reverse(cur_hidden, last_batch_size, batch_size, inp_hidden): ...
def one_layer_rnn_data(inp, hidden, params, has_biases, hidden_fn, batch_sizes, reverse: bool = False): ...
def rnn_cell(nonlinearity): ...
def rnn_cell_data(nonlinearity): ...
def one_layer_rnn(inp, hidden, params, has_biases, hidden_fn, reverse: bool = False): ...
def rnn_tanh_input(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first): ...
def rnn_relu_input(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first): ...
def rnn_relu_data(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional): ...
def rnn_tanh_data(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional): ...
def lstm_cell(inp, hx, cx, hh_weight, hh_bias, hr_weight, chunk_dim): ...
def one_layer_lstm(inp, hidden, params, has_biases, reverse: bool = False): ...
def one_layer_lstm_data(inp, hidden, params, has_biases, batch_sizes, reverse: bool = False): ...
def lstm_impl(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first): ...
def lstm_data_impl(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional): ...
def gru_cell(inp, cur_hidden, ih_weight, ih_bias, hh_weight, hh_bias): ...
def gru_cell_data(inp, cur_hidden, ih_weight, ih_bias, hh_weight, hh_bias): ...
def gru_impl_data(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional): ...
def gru_impl(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first): ...
def upsample_bilinear2d_vec(input, output_size, align_corners, scale_factors): ...
def upsample_bilinear2d(input: Tensor, output_size: List[int], align_corners: bool, scales_h: float | None = None, scales_w: float | None = None) -> Tensor: ...
def is_same_size(a: Tensor, b: Tensor) -> bool: ...
def nll_loss_forward(self, target: Tensor, weight: Tensor | None, reduction: int, ignore_index: int) -> Tuple[Tensor, Tensor]: ...
def grid_sampler_2d(a: Tensor, grid: Tensor, interpolation_mode: int = 0, padding_mode: int = 0, align_corners: bool = False) -> Tensor: ...
def mv(self, vec): ...
def dot(self, other): ...
def binary_cross_entropy_with_logits(self, target, weight: Incomplete | None = None, pos_weight: Incomplete | None = None, reduction=...): ...
def should_fold(tensor1: torch.Tensor, dim_tensor2: int) -> bool: ...
def matmul(tensor1, tensor2): ...
def upsample_bicubic2d_default(a: Tensor, output_size: Tuple[int, int], align_corners: bool, scale_h: float | None = None, scale_w: float | None = None) -> Tensor: ...
def upsample_bicubic2d_vec(a: Tensor, output_size: Tuple[int, int] | None, align_corners: bool, scale_factors: Tuple[float, float] | None = None) -> Tensor: ...
def register_inplace(aten_op, outplace_op): ...
