import abc
import torch
import torch.nn as nn
from _typeshed import Incomplete
from enum import Enum
from torch.autograd import Function as Function
from torch.autograd.function import once_differentiable as once_differentiable
from torch.distributed.rpc import RRef as RRef
from torch.testing._internal.common_distributed import skip_if_lt_x_gpu as skip_if_lt_x_gpu
from torch.testing._internal.common_utils import IS_MACOS as IS_MACOS, sandcastle_skip_if as sandcastle_skip_if
from torch.testing._internal.dist_utils import dist_init as dist_init, initialize_pg as initialize_pg, wait_until_node_failure as wait_until_node_failure, worker_name as worker_name
from torch.testing._internal.distributed.rpc.rpc_agent_test_fixture import RpcAgentTestFixture as RpcAgentTestFixture

rpc_done: Incomplete
ctx_ids: Incomplete
known_context_ids: Incomplete
requires_grad_tensor: Incomplete

def create_tensor(): ...
def build_sparse_tensor(coalesce: bool = False, requires_grad: bool = True, dtype=...): ...
def create_torchscript_tensor() -> torch.Tensor: ...
def my_py_add(t1, t2): ...
def my_scalar_add(a, b): ...
def my_rref_add(rref_t1, t2): ...
def my_script_add(t1, t2): ...
def my_script_ref_add(ref_t1: RRef[torch.Tensor], t2: torch.Tensor) -> torch.Tensor: ...
def my_nested_rref_add(dst, rref_t1, t2): ...
def ret_requires_grad(): ...
def my_py_nested_call(t1, t2, dst, world_size, hops): ...

class SimulateBackwardError(Function):
    @staticmethod
    def forward(ctx, input): ...
    @staticmethod
    def backward(ctx, input): ...

class ExecMode(Enum):
    LOCAL: int
    RPC_SYNC: int
    REMOTE: int
    RPC_ASYNC: int

class CommonDistAutogradTest(RpcAgentTestFixture, metaclass=abc.ABCMeta):
    def context_cleanup_test_helper(self, rpc_args, func, nested: bool = False) -> None: ...

class TensorPipeAgentDistAutogradTest(CommonDistAutogradTest, metaclass=abc.ABCMeta):
    def test_graph_for_builtin_call_sparse(self) -> None: ...
    def test_graph_for_python_call_sparse(self) -> None: ...
    def test_graph_for_builtin_remote_call_sparse(self) -> None: ...
    def test_graph_for_python_remote_call_sparse(self) -> None: ...
    def test_graph_for_py_nested_call_sparse(self) -> None: ...
    def test_graph_for_py_nested_remote_call_sparse(self) -> None: ...
    def test_graph_for_py_nested_call_itself_sparse(self) -> None: ...
    def test_graph_for_py_nested_remote_call_itself_sparse(self) -> None: ...
    def test_no_graph_with_tensors_not_require_grad_sparse(self) -> None: ...
    def test_no_graph_with_tensors_not_require_grad_remote_sparse(self) -> None: ...
    def test_rpc_complex_args_sparse(self) -> None: ...
    def test_remote_complex_args_sparse(self) -> None: ...
    def test_context_cleanup_tensor_with_grad_sparse(self) -> None: ...
    def test_context_cleanup_tensor_no_grad_sparse(self) -> None: ...
    def test_context_cleanup_nested_rpc_sparse(self) -> None: ...
    def test_backward_no_grad_on_tensor_sparse(self) -> None: ...
    def test_backward_simple_sparse(self) -> None: ...
    def test_backward_simple_self_sparse(self) -> None: ...
    def test_backward_rref_multi_sparse(self) -> None: ...
    def test_backward_rref_sparse(self) -> None: ...
    def test_backward_rref_nested_sparse(self) -> None: ...
    def test_trainer_ps_sparse(self) -> None: ...
    def test_backward_multiple_round_trips_sparse(self) -> None: ...
    def test_backward_different_dtypes_sparse(self) -> None: ...
    def test_backward_simple_python_udf_sparse(self) -> None: ...
    def test_backward_simple_script_call_sparse(self) -> None: ...
    def test_nested_backward_accumulate_grads_sparse(self) -> None: ...
    def test_backwards_nested_python_udf_sparse(self) -> None: ...
    def test_mixed_requires_grad_sparse(self) -> None: ...
    def test_multiple_backward_sparse(self) -> None: ...
    def test_embedding_bag_with_no_grad_tensors(self) -> None: ...

class DistAutogradTest(CommonDistAutogradTest, metaclass=abc.ABCMeta):
    def test_autograd_context(self) -> None: ...
    def test_nested_context(self) -> None: ...
    def test_graph_for_builtin_call(self) -> None: ...
    def test_graph_for_python_call(self) -> None: ...
    def test_graph_for_builtin_remote_call(self) -> None: ...
    def test_graph_for_python_remote_call(self) -> None: ...
    def test_graph_for_py_nested_call(self) -> None: ...
    def test_graph_for_py_nested_remote_call(self) -> None: ...
    def test_graph_for_py_nested_call_itself(self) -> None: ...
    def test_graph_for_py_nested_remote_call_itself(self) -> None: ...
    def test_no_graph_with_tensors_not_require_grad(self) -> None: ...
    def test_no_graph_with_tensors_not_require_grad_remote(self) -> None: ...
    def test_grad_only_on_return_value(self) -> None: ...
    def test_grad_only_on_return_value_remote(self) -> None: ...
    def test_rpc_complex_args(self) -> None: ...
    def test_remote_complex_args(self) -> None: ...
    def test_context_cleanup_tensor_with_grad(self) -> None: ...
    def test_context_cleanup_tensor_no_grad(self) -> None: ...
    def test_context_cleanup_no_tensors(self) -> None: ...
    def test_context_cleanup_nested_rpc(self) -> None: ...
    def test_worker_ids_recorded(self) -> None: ...
    def test_dist_autograd_profiling(self): ...
    def test_error_in_context(self) -> None: ...
    def test_backward_no_grad_on_tensor(self) -> None: ...
    def test_backward_simple(self) -> None: ...
    def test_backward_simple_self(self) -> None: ...
    def test_backward_rref(self) -> None: ...
    def test_backward_rref_multi(self) -> None: ...
    def test_backward_rref_nested(self) -> None: ...
    def test_trainer_ps(self) -> None: ...
    def test_trainer_ps_torchscript_functions(self) -> None: ...
    def test_backward_multiple_round_trips(self) -> None: ...
    def test_backward_different_tensor_dims(self) -> None: ...
    def test_backward_unused_tensors(self) -> None: ...
    def test_backward_multiple_output_tensors(self) -> None: ...
    def test_backward_unused_send_function(self) -> None: ...
    def test_backward_autograd_engine_error(self) -> None: ...
    def test_backward_node_failure(self) -> None: ...
    def test_backward_without_context(self) -> None: ...
    def test_backward_without_rpc(self) -> None: ...
    def test_backward_invalid_args(self) -> None: ...
    def test_backward_multiple_roots(self) -> None: ...
    def test_backward_different_dtypes(self) -> None: ...
    def test_backward_simple_python_udf(self) -> None: ...
    def test_backward_simple_script_call(self) -> None: ...
    def test_backward_complex_python_udf(self) -> None: ...
    def test_backward_python_udf_error(self) -> None: ...
    def test_backward_node_failure_python_udf(self) -> None: ...
    def test_backwards_nested_python_udf(self) -> None: ...
    class MyBackwardFunc(Function):
        @staticmethod
        def forward(ctx, input): ...
        @staticmethod
        def backward(ctx, input): ...
    def test_clean_context_during_backward(self) -> None:
        """
        This test simulates the situation where the 'backward' call might throw
        an exception locally which would lead to the autograd context being
        cleaned up if we're using the context manager. As a result, the autograd
        context might be cleaned up while some threads are still using the
        autograd context.

        It is fine for the 'backward' call to throw an exception in this test,
        but the process should not crash.
        """
    def test_mixed_requires_grad(self) -> None: ...
    class TestDebugInfoFunc(Function):
        @staticmethod
        def forward(ctx, input): ...
        @staticmethod
        def backward(ctx, input): ...
    def test_debug_info(self) -> None: ...
    def test_async_dist_autograd(self) -> None:
        """
        This test ensures async processing for distributed autograd works
        appropriately. This is achieved by spawning multiple threads and
        hammering a single node with a lot of backward() calls.
        """
    def test_backward_accumulate_grads(self) -> None: ...
    def test_nested_backward_accumulate_grads(self) -> None: ...
    def test_multiple_backward(self) -> None: ...
    def test_multiple_backward_with_errors(self) -> None: ...
    def test_backward_verify_hooks(self): ...
    def test_no_grad_copy(self):
        """
        Similar to test in test_autograd.py.
        """
    def test_no_grad_copy_sparse(self): ...
    def test_grad_copy_sparse_indices_extra_ref(self): ...
    hook_called_times: int
    def test_post_hooks(self): ...
    def test_thread_local_context_id(self) -> None: ...

class CudaDistAutogradTest(CommonDistAutogradTest, metaclass=abc.ABCMeta):
    def test_gpu_simple(self) -> None: ...
    def test_gpu_to_cpu_continuation(self) -> None: ...
    def test_gpu_to_cpu_continuation_gpu_root(self) -> None: ...

class FaultyAgentDistAutogradTest(RpcAgentTestFixture, metaclass=abc.ABCMeta):
    def context_cleanup_test_helper(self, rpc_args, func) -> None: ...
    def test_context_cleanup_tensor_with_grad(self) -> None: ...
    def test_verify_backend_options(self) -> None: ...

class WrapperModule(nn.Module):
    model: Incomplete
    def __init__(self, model, device) -> None: ...
    def forward(self, *args): ...
    def gradients(self, ctx_id): ...

class TensorPipeCudaDistAutogradTest(RpcAgentTestFixture, metaclass=abc.ABCMeta):
    def test_device_maps_backward_pass(self) -> None: ...
    class MyRemoteCompute(torch.nn.Module):
        def forward(self, input): ...
    class MyLocalCompute(torch.nn.Module):
        next_stage: Incomplete
        def __init__(self, next_stage) -> None: ...
        def forward(self, input): ...
    def test_dist_autograd_sync_streams(self) -> None: ...
    def test_gradients_synchronizations(self) -> None: ...
