from _typeshed import Incomplete
from collections.abc import Generator
from torch import Tensor as Tensor
from torch.utils._mode_utils import all_same_mode as all_same_mode, no_dispatch as no_dispatch
from torch.utils._python_dispatch import TorchDispatchMode as TorchDispatchMode
from torch.utils._pytree import tree_flatten as tree_flatten, tree_map as tree_map, tree_unflatten as tree_unflatten
from typing import Callable

def check_attr_consistency(wrapper_tensor, metadata_name, metadata_accessor) -> None: ...
def check_metadata_consistency(wrapper_tensor, CCT): ...
def is_view_fn(func): ...
def is_inplace_view_fn(func): ...
def is_inplace(func): ...
def generate_cct_and_mode(autograd_view_consistency: bool = True): ...
def is_tensorlist(lst): ...
def maybe_map(fn, should_map, arg): ...
def wrap(arg, CCT, cct_mode): ...
def generate_subclass_choices(flat_args, CCT, cct_mode) -> Generator[Incomplete, None, None]: ...
def generate_subclass_choices_args_kwargs(args, kwargs, CCT, cct_mode) -> Generator[Incomplete, None, None]: ...
def raise_composite_compliance_error(err, additional_info: str = '') -> None: ...
def check_all_permutations(op, args, kwargs, assert_equal_fn): ...
def check_with_mode(op, args, kwargs, assert_equal_fn): ...
def gather_leaf_tensors(args, kwargs): ...
def compute_expected_grads(op, args, kwargs, output_process_fn_grad: Incomplete | None = None, gradcheck_wrapper: Incomplete | None = None): ...
def check_backward_formula(op: Callable, args, kwargs, output_process_fn_grad: Incomplete | None = None, gradcheck_wrapper: Incomplete | None = None, assert_equal_fn: Incomplete | None = None): ...
def check_forward_ad_formula(op: Callable, args, kwargs, gradcheck_wrapper: Incomplete | None = None, assert_equal_fn: Incomplete | None = None): ...
