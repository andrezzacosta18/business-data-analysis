import torch
from _typeshed import Incomplete
from collections.abc import Generator
from torch.testing import make_tensor as make_tensor
from torch.testing._internal.common_dtype import all_types_and as all_types_and
from torch.testing._internal.opinfo.core import OpInfo as OpInfo, SampleInput as SampleInput

def to_numpy(tensor): ...

class NumpyCube(torch.autograd.Function):
    @staticmethod
    def forward(input): ...
    @staticmethod
    def setup_context(ctx, inputs, output) -> None: ...
    @staticmethod
    def backward(ctx, grad_output, grad_saved): ...
    @staticmethod
    def vmap(info, in_dims, input): ...
    @staticmethod
    def jvp(ctx, input_tangent): ...

class CubeGenVmap(torch.autograd.Function):
    generate_vmap_rule: bool
    @staticmethod
    def forward(x): ...
    @staticmethod
    def setup_context(ctx, inputs, outputs) -> None: ...
    @staticmethod
    def backward(ctx, grad_output, grad_saved): ...
    @staticmethod
    def jvp(ctx, input_tangent): ...

def sample_inputs_numpy_cube(opinfo, device, dtype, requires_grad, **kwargs) -> Generator[Incomplete, None, None]: ...

class NumpyCubeNotComposable(torch.autograd.Function):
    @staticmethod
    def forward(input): ...
    @staticmethod
    def setup_context(ctx, inputs, output) -> None: ...
    @staticmethod
    def backward(ctx, grad_output, grad_saved): ...

class NumpyMul(torch.autograd.Function):
    @staticmethod
    def forward(x, y): ...
    @staticmethod
    def setup_context(ctx, inputs, output) -> None: ...
    @staticmethod
    def backward(ctx, grad_output): ...
    @staticmethod
    def vmap(info, in_dims, x, y): ...
    @staticmethod
    def jvp(ctx, x_tangent, y_tangent): ...

def sample_inputs_numpy_mul(opinfo, device, dtype, requires_grad, **kwargs) -> Generator[Incomplete, None, None]: ...

class MulGenVmap(torch.autograd.Function):
    generate_vmap_rule: bool
    @staticmethod
    def forward(x, y): ...
    @staticmethod
    def setup_context(ctx, inputs, outputs) -> None: ...
    @staticmethod
    def backward(ctx, grad_output): ...
    @staticmethod
    def jvp(ctx, x_tangent, y_tangent): ...

class NumpyExp_(torch.autograd.Function):
    @staticmethod
    def forward(x): ...
    @staticmethod
    def setup_context(ctx, inputs, output) -> None: ...
    @staticmethod
    def backward(ctx, grad_output): ...
    @staticmethod
    def vmap(info, in_dims, x): ...
    @staticmethod
    def jvp(ctx, x_tangent): ...

class NumpySort(torch.autograd.Function):
    @staticmethod
    def forward(x, dim): ...
    @staticmethod
    def setup_context(ctx, inputs, output) -> None: ...
    @staticmethod
    def backward(ctx, grad_output, _0, _1): ...
    @staticmethod
    def vmap(info, in_dims, x, dim): ...
    @staticmethod
    def jvp(ctx, x_tangent, _): ...

class SortGenVmap(torch.autograd.Function):
    generate_vmap_rule: bool
    @staticmethod
    def forward(x, dim): ...
    @staticmethod
    def setup_context(ctx, inputs, outputs) -> None: ...
    @staticmethod
    def backward(ctx, grad_output, _0, _1): ...
    @staticmethod
    def jvp(ctx, x_tangent, _): ...

def sample_inputs_numpy_sort(opinfo, device, dtype, requires_grad, **kwargs) -> Generator[Incomplete, None, None]: ...

class NumpyTake(torch.autograd.Function):
    @staticmethod
    def forward(x, ind, ind_inv, dim): ...
    @staticmethod
    def setup_context(ctx, inputs, output) -> None: ...
    @staticmethod
    def backward(ctx, grad_output): ...
    @staticmethod
    def vmap(info, in_dims, x, ind, ind_inv, dim): ...
    @staticmethod
    def jvp(ctx, x_tangent, ind_tangent, ind_inv_tangent, _): ...

class TakeGenVmap(torch.autograd.Function):
    generate_vmap_rule: bool
    @staticmethod
    def forward(x, ind, ind_inv, dim): ...
    @staticmethod
    def setup_context(ctx, inputs, outputs) -> None: ...
    @staticmethod
    def backward(ctx, grad_output): ...
    @staticmethod
    def jvp(ctx, x_tangent, ind_tangent, ind_inv_tangent, _): ...

class Select(torch.autograd.Function):
    @staticmethod
    def forward(x, idx): ...
    @staticmethod
    def setup_context(ctx, inputs, output) -> None: ...
    @staticmethod
    def backward(ctx, grad_output): ...
    @staticmethod
    def vmap(info, in_dims, x, idx): ...
    @staticmethod
    def jvp(ctx, x_tangent, _): ...

class SelectGenVmap(torch.autograd.Function):
    generate_vmap_rule: bool
    @staticmethod
    def forward(x, idx): ...
    @staticmethod
    def setup_context(ctx, inputs, outputs) -> None: ...
    @staticmethod
    def backward(ctx, grad_output): ...
    @staticmethod
    def jvp(ctx, x_tangent, _): ...

def sample_inputs_select(opinfo, device, dtype, requires_grad, **kwargs) -> Generator[Incomplete, None, None]: ...

class ScaleGradGenVmap(torch.autograd.Function):
    generate_vmap_rule: bool
    scale: float
    @staticmethod
    def forward(x): ...
    @staticmethod
    def setup_context(ctx, inputs, outputs) -> None: ...
    @staticmethod
    def backward(ctx, grad_output): ...
    @staticmethod
    def jvp(ctx, x_tangent): ...

class ZeroGradientsGenVmap(torch.autograd.Function):
    generate_vmap_rule: bool
    @staticmethod
    def forward(x, y): ...
    @staticmethod
    def setup_context(ctx, inputs, outputs) -> None: ...
    @staticmethod
    def backward(ctx, gx, gy): ...
    @staticmethod
    def jvp(ctx, gx, gy): ...

autograd_function_db: Incomplete
