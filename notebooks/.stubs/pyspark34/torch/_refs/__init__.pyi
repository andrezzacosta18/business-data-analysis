import torch
from _typeshed import Incomplete
from torch._prims_common import DeviceLikeType, DimsSequenceType, DimsType, NumberType, ShapeType, StrideType, TensorLike, TensorLikeType, TensorOrNumberLikeType, TensorSequenceType
from typing import List, Sequence, Tuple, overload

__all__ = ['abs', 'acos', 'acosh', 'asinh', 'asin', 'atan', 'atanh', 'bitwise_not', 'ceil', 'conj_physical', 'cos', 'cosh', 'digamma', 'erf', 'erfinv', 'erfc', 'exp', 'expm1', 'exp2', 'fill', 'floor', 'frac', 'index_add', 'index_copy', 'index_copy_', 'index_select', 'index_fill', 'index_fill_', 'isfinite', 'isinf', 'isposinf', 'isneginf', 'isnan', 'isreal', 'i0', 'lerp', 'lgamma', 'log', 'log1p', 'log2', 'log10', 'log_softmax', 'nan_to_num', 'neg', 'positive', 'reciprocal', 'round', 'sigmoid', 'sgn', 'sign', 'signbit', 'sin', 'sinc', 'sinh', 'softmax', 'sqrt', 'square', 'tan', 'tanh', 'trace', 'trunc', 'add', 'atan2', 'bitwise_and', 'bitwise_left_shift', 'bitwise_or', 'bitwise_right_shift', 'bitwise_xor', 'clamp_min', 'clamp_max', 'copysign', 'div', 'eq', 'float_power', 'floor_divide', 'fmax', 'fmin', 'fmod', 'gcd', 'ge', 'gt', 'heaviside', 'hypot', 'igamma', 'igammac', 'imag', 'isclose', 'lcm', 'le', 'logical_and', 'logical_not', 'logical_or', 'logical_xor', 'lt', 'maximum', 'minimum', 'mul', 'ne', 'nextafter', 'pow', 'real', 'rpow', 'remainder', 'rsub', 'rtruediv', 'rfloordiv', 'sub', 'true_divide', 'trunc_divide', 'xlogy', 'addcdiv', 'addcmul', 'clamp', 'masked_fill', 'where', 'clone', 'copy_to', 'item', 'to', 'all', 'amax', 'amin', 'any', 'mean', 'std', 'std_mean', 'sum', 'sum_to_size', 'prod', 'var', 'var_mean', 'addr', 'atleast_1d', 'atleast_2d', 'atleast_3d', 'as_strided', 'broadcast_shapes', 'broadcast_tensors', 'broadcast_to', 'cat', 'chunk', 'column_stack', 'conj', 'constant_pad_nd', 'contiguous', 'diag_embed', 'diag', 'diagonal', 'diagonal_copy', 'diagonal_scatter', 'dsplit', 'dstack', 'expand', 'expand_as', 'flatten', 'flip', 'fliplr', 'flipud', 'hsplit', 'hstack', 'meshgrid', 'movedim', 'narrow', 'narrow_copy', 'native_group_norm', 'native_layer_norm', 'permute', 'ravel', 'repeat', 'reshape', 'roll', 'rot90', 'rsqrt', 'stack', 'swap_axes', 'squeeze', 't', 'T', 'tensor_split', 'transpose', 'unfold', 'unfold_copy', 'unsqueeze', 'view', 'vsplit', 'vstack', 'unflatten', 'unbind', 'triu', 'tril', 'triu_indices', 'tril_indices', 'arange', 'empty', 'empty_like', 'empty_strided', 'eye', 'full', 'full_like', 'linspace', 'logspace', 'ones', 'ones_like', 'randn', 'scalar_tensor', 'zeros', 'zeros_like', 'allclose', 'equal', 'bucketize']

def abs(a): ...
def acos(a): ...
def acosh(a): ...
def asin(a): ...
def asinh(a): ...
def atan(a): ...
def atanh(a): ...
def bitwise_not(a): ...
def ceil(a): ...
def conj_physical(input: TensorLikeType): ...
def cos(a): ...
def cosh(a): ...
def digamma(a): ...
def erf(a): ...
def erfinv(a): ...
def erfc(a): ...
def exp(a): ...
def expm1(a): ...
def exp2(a): ...
def fill(a: TensorLikeType, value: NumberType) -> TensorLikeType: ...
def floor(a): ...
def frac(x: TensorLikeType) -> TensorLikeType: ...
def imag(a: TensorLikeType) -> TensorLikeType: ...
def isfinite(a: TensorLikeType) -> TensorLikeType: ...
def isinf(a: TensorLikeType) -> TensorLikeType: ...
def isposinf(a: TensorLikeType) -> TensorLikeType: ...
def isneginf(a: TensorLikeType) -> TensorLikeType: ...
def isnan(a: TensorLikeType) -> TensorLikeType: ...
def isreal(a: TensorLikeType) -> TensorLikeType: ...
def i0(a): ...
def lgamma(a): ...
def log(a): ...
def log1p(a): ...
def log2(a): ...
def log10(a): ...
def log_softmax(a: TensorLikeType, dim: int, dtype: torch.dtype | None = None) -> TensorLikeType: ...
def nan_to_num(a: TensorLikeType, nan: NumberType | None = 0.0, posinf: NumberType | None = None, neginf: NumberType | None = None) -> TensorLikeType: ...
def neg(a): ...
def positive(a: TensorLikeType) -> TensorLikeType: ...
def real(a: TensorLikeType) -> TensorLikeType: ...
def reciprocal(a): ...
def round(a): ...
def rsqrt(a): ...
def sigmoid(a: TensorLikeType) -> TensorLikeType: ...
def sgn(a): ...
def sign(a): ...
def signbit(a): ...
def sin(a): ...
def sinc(a): ...
def sinh(a): ...
def sqrt(a): ...
def square(a: TensorLikeType) -> TensorLikeType: ...
def tan(a): ...
def tanh(a): ...
def trunc(a): ...
def add(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType, *, alpha: NumberType | None = None):
    """
    Reference implementation of torch.add
    """
def atan2(a, b): ...
def bitwise_and(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def bitwise_left_shift(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def bitwise_or(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def bitwise_right_shift(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def bitwise_xor(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def copysign(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType): ...
def div(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType, *, rounding_mode: str | None = None):
    """
    Reference implementation of torch.div
    """
def eq(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def pow(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType) -> TensorLikeType: ...
def float_power(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType) -> Tensor: ...
def floor_divide(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType): ...
def fmax(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def fmin(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def fmod(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def gcd(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def ge(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def gt(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def heaviside(input: TensorLikeType, values: TensorLikeType) -> TensorLikeType: ...
def hypot(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def igamma(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def igammac(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def isclose(a: TensorLikeType, b: TensorLikeType, rtol: float = 1e-05, atol: float = 1e-08, equal_nan: bool = False) -> TensorLikeType: ...
def lcm(a: TensorLikeType, b: TensorLikeType): ...
def le(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def logical_and(a: TensorLikeType, b: TensorLikeType): ...
def logical_not(a: TensorLikeType): ...
def logical_or(a: TensorLikeType, b: TensorLikeType): ...
def logical_xor(a: TensorLikeType, b: TensorLikeType): ...
def lt(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def maximum(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def minimum(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def mul(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def ne(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def nextafter(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def remainder(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def rsub(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType, *, alpha: NumberType | None = None): ...
def sub(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType, *, alpha: NumberType | None = None):
    """
    Reference implementation of torch.sub
    """
def true_divide(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def xlogy(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType): ...
def trunc_divide(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType): ...
def addcdiv(self, tensor1: TensorLikeType, tensor2: TensorLikeType, *, value: NumberType = 1) -> TensorLikeType:
    """
    Reference implementation of torch.addcdiv
    """
def addcmul(self, tensor1: TensorLikeType, tensor2: TensorLikeType, *, value: NumberType = 1) -> TensorLikeType:
    """
    Reference implementation of torch.addcmul
    """
def clamp(a: TensorLikeType, min: TensorOrNumberLikeType | None = None, max: TensorOrNumberLikeType | None = None) -> TensorLikeType: ...
def clamp_min(self, min: TensorOrNumberLikeType = None) -> TensorLikeType: ...
def clamp_max(self, max: TensorOrNumberLikeType = None) -> TensorLikeType: ...
def where(pred: Tensor, a: TensorOrNumberLikeType | None = None, b: TensorOrNumberLikeType | None = None):
    """ """
def clone(a: TensorLikeType, *, memory_format: torch.memory_format = ...) -> TensorLikeType: ...
def copy_to(a: Tensor, b: Tensor, *, allow_cross_device: bool = True): ...
def item(a: TensorLikeType) -> NumberType: ...
def to(a: TensorLikeType, *args, **kwargs) -> TensorLikeType: ...
py_all = all

def all(a: TensorLikeType, dim: DimsType | None = None, keepdim: bool = False) -> TensorLikeType: ...
py_any = any

def any(a: TensorLikeType, dim: DimsType | None = None, keepdim: bool = False) -> TensorLikeType: ...
def sum(a: TensorLikeType, dim: int | None | List[int] | None = None, keepdim: bool = False, *, dtype: torch.dtype | None = None, out: Tensor | None = None) -> TensorLikeType: ...
def sum_to_size(a: Tensor, *shape) -> Tensor: ...
def prod(a: TensorLikeType, dim: int | None | List[int] | None = None, keepdim: bool = False, *, dtype: Incomplete | None = None, out: Tensor | None = None) -> TensorLikeType: ...
def amin(a: TensorLikeType, dim: DimsType | None = None, keepdim: bool = False, *, out: Tensor | None = None) -> TensorLikeType: ...
def amax(a: TensorLikeType, dim: DimsType | None = None, keepdim: bool = False, *, out: Tensor | None = None) -> TensorLikeType: ...
def var(a: TensorLikeType, dim: DimsType | None = None, unbiased: bool | None = None, keepdim: bool = False, *, correction: int | None = None) -> TensorLikeType: ...
def std(a: TensorLikeType, dim: int | None | List[int] | None = None, unbiased: bool | None = None, keepdim: bool = False, *, correction: int | None = None) -> TensorLikeType: ...
def mean(a: TensorLikeType, dim: DimsType | None = None, keepdim: bool = False, *, dtype: Incomplete | None = None, out: Incomplete | None = None) -> TensorLikeType: ...
def std_mean(a: TensorLikeType, dim: DimsType | None = None, *, unbiased: bool | None = None, keepdim: bool = False, correction: int | None = None): ...
def var_mean(a: TensorLikeType, dim: DimsType | None = None, unbiased: bool | None = None, keepdim: bool = False, *, correction: int | None = None): ...
def addr(self, vec1: TensorLikeType, vec2: TensorLikeType, *, beta: NumberType = 1, alpha: NumberType = 1) -> TensorLikeType: ...
def atleast_1d(arg: TensorLikeType | Sequence[TensorLikeType], *args: TensorLikeType) -> TensorLikeType | Tuple[TensorLikeType, ...]:
    """Reference implementation of :func:`torch.atleast_1d`."""
def atleast_2d(arg: TensorLikeType | Sequence[TensorLikeType], *args: TensorLikeType) -> TensorLikeType | Tuple[TensorLikeType, ...]:
    """Reference implementation of :func:`torch.atleast_2d`."""
def atleast_3d(arg: TensorLikeType | Sequence[TensorLikeType], *args: TensorLikeType) -> TensorLikeType | Tuple[TensorLikeType, ...]:
    """Reference implementation of :func:`torch.atleast_3d`."""
def as_strided(a: TensorLikeType, size: ShapeType, stride: StrideType, storage_offset: int | None = None) -> TensorLikeType: ...
def broadcast_shapes(*shapes) -> ShapeType: ...
def broadcast_tensors(*tensors) -> List[TensorLikeType]: ...
def broadcast_to(a: TensorLikeType, size: ShapeType) -> TensorLikeType: ...
def cat(tensors: TensorSequenceType, dim: int = 0) -> TensorLikeType: ...
def column_stack(tensors: TensorSequenceType) -> TensorLikeType: ...
def conj(input: TensorLikeType) -> TensorLikeType: ...
def constant_pad_nd(input: TensorLikeType, pad: List[int], value: NumberType = 0) -> TensorLikeType: ...
def contiguous(a: Tensor, *, memory_format: torch.memory_format = ...) -> Tensor: ...
def dstack(tensors: TensorSequenceType) -> TensorLikeType: ...
def expand(a: Tensor, *shape) -> Tensor: ...
def expand_as(a: Tensor, b: Tensor) -> Tensor: ...
def chunk(a: TensorLikeType, chunks: int, dim: int = 0) -> Tuple[TensorLikeType, ...]: ...
def flatten(a: TensorLikeType, start_dim: int = 0, end_dim: int = -1) -> TensorLikeType: ...
def flip(a: TensorLikeType, dims: DimsSequenceType) -> TensorLikeType: ...
def fliplr(a: TensorLikeType) -> TensorLikeType: ...
def flipud(a: TensorLikeType) -> TensorLikeType: ...
def narrow(a: TensorLikeType, dim: int, start: int | TensorLikeType, length: int) -> TensorLikeType: ...

narrow_copy: Incomplete

def native_group_norm(input: Tensor, weight: Tensor | None, bias: Tensor | None, batch_size: int, num_channels: int, flattened_inner_size: int, num_groups: int, eps: float) -> Tuple[Tensor, Tensor, Tensor]: ...
def native_layer_norm(input: Tensor, normalized_shape: ShapeType, weight: Tensor | None, bias: Tensor | None, eps: float) -> Tuple[Tensor, Tensor, Tensor]: ...
def permute(a: TensorLikeType, *dims) -> TensorLikeType: ...
def repeat(a: Tensor, *repeat_shape) -> Tensor: ...
def reshape(a: TensorLikeType, *shape: ShapeType) -> TensorLikeType: ...
def roll(a: TensorLikeType, shifts: DimsType, dims: DimsType = ...) -> TensorLikeType:
    """Reference implementation of :func:`torch.roll`."""
def rot90(a: TensorLikeType, k: int = 1, dims: DimsSequenceType = (0, 1)) -> TensorLikeType:
    """Reference implementation of :func:`torch.rot90`."""
def stack(tensors: TensorSequenceType, dim: int = 0) -> TensorLikeType: ...
def softmax(a: TensorLikeType, dim: int, dtype: torch.dtype | None = None) -> TensorLikeType: ...
def hstack(tensors: TensorSequenceType) -> TensorLikeType: ...
def vstack(tensors: TensorSequenceType) -> TensorLikeType: ...
def unflatten(a: TensorLikeType, dim: int, sizes: ShapeType) -> TensorLikeType: ...
def unbind(t: TensorLikeType, dim: int = 0) -> TensorSequenceType: ...
def index_copy(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike): ...
def index_copy_(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike): ...
def index_fill(x: TensorLike, dim: int, index: TensorLike, value: NumberType | TensorLike): ...
def index_fill_(x: TensorLike, dim: int, index: TensorLike, value: NumberType | TensorLike): ...
def index_add(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike, *, alpha: NumberType = 1): ...
def index_select(x: TensorLike, dim: int, index: TensorLike): ...
def squeeze(a: TensorLikeType, dim: DimsType | None = None) -> TensorLikeType: ...
def tensor_split(a: TensorLikeType, indices_or_sections: Tensor | DimsType, dim: int = 0) -> Tuple[TensorLikeType, ...]: ...
def hsplit(a: TensorLikeType, indices_or_sections: DimsType) -> Tuple[TensorLikeType, ...]: ...
def vsplit(a: TensorLikeType, indices_or_sections: DimsType) -> Tuple[TensorLikeType, ...]: ...
def diag(self, offset: int = 0) -> TensorLikeType: ...
def diagonal_scatter(input: TensorLikeType, src: TensorLikeType, offset: int = 0, dim1: int = 0, dim2: int = 1) -> TensorLikeType: ...
def diagonal(self, offset: int = 0, dim1: int = 0, dim2: int = 1) -> TensorLikeType:
    """
    Reference implementation of torch.diagonal
    """

diagonal_copy: Incomplete

def diag_embed(t: TensorLikeType, offset: int = 0, dim1: int = -2, dim2: int = -1) -> TensorLikeType:
    """
    Reference implementation of torch.diag_embed
    """
def dsplit(a: TensorLikeType, sections: DimsType) -> TensorSequenceType: ...
def t(a: TensorLikeType): ...
def T(a: TensorLikeType) -> TensorLikeType: ...
def transpose(a: TensorLikeType, dim0: int, dim1: int) -> TensorLikeType: ...
swap_axes = transpose

def unfold(self, dimension: int, size: int, step: int) -> TensorLikeType: ...
def unfold_copy(self, dimension: int, size: int, step: int): ...
def unsqueeze(a: TensorLikeType, dim: int) -> TensorLikeType: ...
def view(a: TensorLikeType, *shape: ShapeType) -> TensorLikeType: ...
def ravel(a: TensorLikeType) -> TensorLikeType: ...
def empty(*shape, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, requires_grad: bool = False, pin_memory: bool = False, memory_format: torch.memory_format = ...) -> TensorLikeType: ...
def zeros(*size, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
def ones(*size, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
def empty_like(a: TensorLikeType, *, dtype: torch.dtype | None = None, device: torch.device | None = None, layout: torch.layout | None = None, pin_memory: bool = False, requires_grad: bool = False, memory_format: torch.memory_format = ...) -> TensorLikeType: ...
def arange(start: NumberType = 0, end: NumberType | None = None, step: NumberType = 1, *, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
def lerp(start: Tensor, end: Tensor, weight: Tensor | NumberType): ...
def linspace(start: NumberType, end: NumberType, steps: NumberType, *, dtype: torch.dtype | None = None, device: torch.device | None = None, layout: torch.layout = ..., pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
def logspace(start: NumberType, end: NumberType, steps: NumberType, base: NumberType = 10, *, dtype: torch.dtype | None = None, device: torch.device | None = None, layout: torch.layout = ..., pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
@overload
def meshgrid(tensors: Sequence[TensorLikeType], indexing: str): ...
@overload
def meshgrid(*tensors: TensorLikeType, indexing: str): ...
def movedim(input: TensorLikeType, source: int | DimsSequenceType, destination: int | DimsSequenceType) -> TensorLikeType:
    """
    Reference implementation of torch.movedim
    """
def empty_strided(shape: ShapeType | Tuple[ShapeType], strides: StrideType, *, dtype: torch.dtype | None = None, device: torch.device | None = None, layout: torch.layout = ..., requires_grad: bool = False, pin_memory: bool = False) -> TensorLikeType: ...
def eye(n: int, m: int | None = None, *, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType:
    """
    Reference implementation of torch.eye
    """
def full(shape: ShapeType, fill_value: NumberType, *, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
def full_like(a: TensorLikeType, fill_value: NumberType, *, dtype: torch.dtype | None = None, layout: torch.layout | None = None, device: torch.device | None = None, pin_memory: bool = False, requires_grad: bool = False, memory_format: torch.memory_format = ...) -> TensorLikeType: ...
def zeros_like(a: TensorLikeType, *, dtype: torch.dtype | None = None, layout: torch.layout | None = None, device: torch.device | None = None, pin_memory: bool = False, requires_grad: bool = False, memory_format: torch.memory_format = ...) -> TensorLikeType: ...
def ones_like(a: TensorLikeType, *, dtype: torch.dtype | None = None, layout: torch.layout | None = None, device: torch.device | None = None, pin_memory: bool = False, requires_grad: bool = False, memory_format: torch.memory_format = ...) -> TensorLikeType: ...
def randn(*shape, dtype: torch.dtype | None = None, device: torch.device | None = None, layout: torch.layout | None = None, requires_grad: bool = False, pin_memory: bool = False) -> TensorLikeType: ...
def scalar_tensor(a: NumberType, *, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, pin_memory: bool = False) -> TensorLikeType: ...
def masked_fill(a: TensorLikeType, mask: TensorLikeType, value: TensorOrNumberLikeType): ...
def allclose(a: TensorLikeType, b: TensorLikeType, rtol: float = 1e-05, atol: float = 1e-08, equal_nan: bool = False) -> bool:
    """
    Reference implementation of torch.allclose
    """
def equal(a: TensorLikeType, b: TensorLikeType) -> bool: ...
def trace(self) -> TensorLikeType: ...

rtruediv: Incomplete
rfloordiv: Incomplete
rpow: Incomplete

def triu(a: TensorLikeType, diagonal: int = 0) -> TensorLikeType: ...
def tril(a: TensorLikeType, diagonal: int = 0) -> TensorLikeType: ...
def tril_indices(row: int, col: int, offset: int = 0, *, dtype: torch.dtype = ..., layout: torch.layout = ..., device: DeviceLikeType = 'cpu', pin_memory: bool = False) -> TensorLikeType: ...
def triu_indices(row: int, col: int, offset: int = 0, *, dtype: torch.dtype = ..., layout: torch.layout = ..., device: DeviceLikeType = 'cpu', pin_memory: bool = False) -> TensorLikeType: ...
def bucketize(a: TensorLikeType, boundaries: TensorLikeType, *, out_int32: bool = False, right: bool = False): ...
