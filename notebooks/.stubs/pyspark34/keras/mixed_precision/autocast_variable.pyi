import tensorflow.compat.v2 as tf
from _typeshed import Incomplete
from keras.distribute import distributed_training_utils as distributed_training_utils

def numpy_text(tensor, is_repr: bool = False):
    """Human readable representation of a tensor's numpy value."""

class AutoCastVariableSpec(tf.types.experimental.TraceType):
    """TraceType for AutoCastVariableSpec for tracing with tf.function.

    This class implements the Type for AutoCastVariable used in tracing.
    """
    def __init__(self, value) -> None: ...
    def is_subtype_of(self, other) -> bool:
        """If the other spec is the same as `self`, return True."""
    def most_specific_common_supertype(self, others):
        """`self` is the common supertype if all input types match it."""
    def placeholder_value(self, placeholder_context: Incomplete | None = None):
        """Use the AutoCastVariable value itself as a placeholder."""
    def __hash__(self) -> int: ...
    def __eq__(self, other) -> bool: ...

class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):
    """Variable that casts itself to a different dtype in applicable contexts.

    This class wraps a floating-point `tf.Variable`. It emulates the variable
    interface and delegates to the wrapped variable, but it additionally will
    cast the wrapped variable under an `enable_auto_cast_variables(dtype)`
    context manager.

    For example:

    >>> v = tf.Variable(1.0, dtype=tf.float32)
    >>> v = AutoCastVariable(v)
    >>> tf.identity(v).dtype
    tf.float32
    >>> with enable_auto_cast_variables(tf.float16):
    ...   tf.identity(v).dtype
    tf.float16

    The purpose of this class is to allow Keras layers to create variables in
    float32, and automatically cast them to float16 or bfloat16 when the layer
    is called.
    """
    def __init__(self, variable) -> None:
        """Creates an AutoCastVariable instance.

        Args:
          variable: A floating-point resource variable to wrap.

        Raises:
          ValueError: If `variable` is not a floating-point resource variable
        """
    @property
    def dtype(self):
        """The dtype of the underlying variable, before any casts are done."""
    @property
    def true_dtype(self):
        """Deprecated alias of `dtype`."""
    def value(self): ...
    def read_value(self): ...
    def sparse_read(self, indices, name: Incomplete | None = None):
        """Reads the value of this variable sparsely, using `gather`."""
    def gather_nd(self, indices, name: Incomplete | None = None):
        """Gather slices of the variable into a Tensor."""
    def __getattr__(self, name): ...
    def set_shape(self, shape): ...
    @property
    def trainable(self): ...
    @property
    def synchronization(self): ...
    @property
    def aggregation(self): ...
    def eval(self, session: Incomplete | None = None): ...
    def initialized_value(self): ...
    @property
    def initial_value(self): ...
    @property
    def constraint(self): ...
    def assign(self, value, use_locking: Incomplete | None = None, name: Incomplete | None = None, read_value: bool = True): ...
    def assign_add(self, delta, use_locking: Incomplete | None = None, name: Incomplete | None = None, read_value: bool = True): ...
    def assign_sub(self, delta, use_locking: Incomplete | None = None, name: Incomplete | None = None, read_value: bool = True): ...
    def scatter_sub(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_add(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_max(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_min(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_mul(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_div(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_update(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def batch_scatter_update(self, sparse_delta, use_locking: bool = False, name: Incomplete | None = None): ...
    def scatter_nd_sub(self, indices, updates, name: Incomplete | None = None): ...
    def scatter_nd_add(self, indices, updates, name: Incomplete | None = None): ...
    def scatter_nd_update(self, indices, updates, name: Incomplete | None = None): ...
    def load(self, value, session: Incomplete | None = None): ...
    @property
    def name(self): ...
    @property
    def initializer(self): ...
    @property
    def device(self): ...
    @property
    def op(self): ...
    @property
    def graph(self): ...
    @property
    def shape(self): ...
    def get_shape(self): ...
    def __tf_tracing_type__(self, context): ...
    def to_proto(self, export_scope: Incomplete | None = None): ...
    def from_proto(self, variable_def, import_scope: Incomplete | None = None): ...
    def __add__(self, o): ...
    def __radd__(self, o): ...
    def __sub__(self, o): ...
    def __rsub__(self, o): ...
    def __mul__(self, o): ...
    def __rmul__(self, o): ...
    def __truediv__(self, o): ...
    def __rtruediv__(self, o): ...
    def __floordiv__(self, o): ...
    def __rfloordiv__(self, o): ...
    def __mod__(self, o): ...
    def __rmod__(self, o): ...
    def __lt__(self, o): ...
    def __le__(self, o): ...
    def __gt__(self, o): ...
    def __ge__(self, o): ...
    def __getitem__(self, o): ...
    def __pow__(self, o, modulo: Incomplete | None = None): ...
    def __rpow__(self, o): ...
    def __neg__(self): ...
    def __abs__(self): ...
    def __div__(self, o): ...
    def __rdiv__(self, o): ...
    def __matmul__(self, o): ...
    def __rmatmul__(self, o): ...

def create_autocast_variable(variable):
    """Creates an AutoCastVariable that wraps another variable.

    This typically just returns `AutoCastVariable(variable)`. But, if the
    variable is a DistributedVariable or one of its subclasses, we instead
    dynamically create a class that subclasses from both AutoCastVariable and
    variable.__class__. This is so the returned variable will still pass
    `isinstance(variable, variable.__class__)`, which is required for
    DistributedVariables and its subclasses to work properly.

    Args:
      variable: A floating-point resource variable to wrap.

    Returns:
      An AutoCastVariable that wraps the variable.
    """

class enable_auto_cast_variables:
    """Context manager which enables the autocasting of `AutoCastVariable`s.

    Under this context manager, `AutoCastVariable`s will be cast to `dtype` if
    `dtype` is floating-point. Otherwise, `AutoCastVariable`s will not be cast.
    """
    def __init__(self, dtype) -> None: ...
    def __enter__(self) -> None: ...
    def __exit__(self, type_arg: type[BaseException] | None, value_arg: BaseException | None, traceback_arg: types.TracebackType | None) -> None: ...
