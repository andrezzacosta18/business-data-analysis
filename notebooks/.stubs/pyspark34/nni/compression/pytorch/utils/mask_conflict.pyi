from .shape_dependency import ChannelDependency as ChannelDependency, GroupDependency as GroupDependency, InputChannelDependency as InputChannelDependency
from .utils import get_module_by_name as get_module_by_name
from _typeshed import Incomplete

def fix_mask_conflict(masks, model, dummy_input, traced: Incomplete | None = None):
    """
    MaskConflict fix the mask conflict for the channel dependencies
    and group dependency.

    Parameters
    ----------
    masks : dict/str
        A dict object that stores the masks or the path of the mask file
    model : torch.nn.Module
        model to fix the mask conflict
    dummy_input : torch.Tensor/list of tensors/dict of tensors
        input example to trace the model
    traced : torch._C.torch.jit.TopLevelTracedModule
        the traced model of the target model, is this parameter is not None,
        we donnot use the model and dummpy_input to get the trace graph.
    """

class MaskFix:
    model: Incomplete
    dummy_input: Incomplete
    traced: Incomplete
    masks: Incomplete
    def __init__(self, masks, model: Incomplete | None = None, dummy_input: Incomplete | None = None, traced: Incomplete | None = None) -> None: ...
    def fix_mask(self) -> None: ...
    def export(self, path) -> None:
        """
        Export the masks after fixing the conflict to file.
        """

class GroupMaskConflict(MaskFix):
    """
    GroupMaskConflict fix the mask conflict between the layers that
    has group dependecy with each other.

    Parameters
    ----------
    masks : dict
        a dict object that stores the masks
    model : torch.nn.Module
        model to fix the mask conflict
    dummy_input : torch.Tensor
        input example to trace the model
    traced : torch._C.torch.jit.TopLevelTracedModule
        the traced model of the target model, is this parameter is not None,
        we donnot use the model and dummpy_input to get the trace graph.
    """
    def __init__(self, masks, model, dummy_input, traced: Incomplete | None = None) -> None: ...
    def fix_mask(self):
        """
        Fix the mask conflict before the mask inference for the layers that
        has group dependencies. This function should be called before the
        mask inference of the 'speedup' module.
        """

class ChannelMaskConflict(MaskFix):
    """
    ChannelMaskConflict fix the mask conflict between the layers that
    has channel dependecy with each other.

    Parameters
    ----------
    masks : dict
        a dict object that stores the masks
    model : torch.nn.Module
        model to fix the mask conflict
    dummy_input : torch.Tensor
        input example to trace the model
    graph : torch._C.torch.jit.TopLevelTracedModule
        the traced graph of the target model, is this parameter is not None,
        we donnot use the model and dummpy_input to get the trace graph.
    """
    conv_prune_dim: Incomplete
    channel_prune_type: Incomplete
    def __init__(self, masks, model, dummy_input, traced: Incomplete | None = None) -> None: ...
    def fix_mask(self):
        """
        Fix the mask conflict before the mask inference for the layers that
        has shape dependencies. This function should be called before the
        mask inference of the 'speedup' module. Only structured pruning masks
        are supported.
        """

def detect_channel_prune_type(masks, model):
    """
    User can prune a channel through two ways: 1) prune
    the corresponding filter of the conv layer(all the
    filter related pruner), 2) prune the BN layers that
    followed after a conv(Slim pruner). This function find
    the pruning type of the masks.

    Parameters
    ----------
    masks: dict
        A dict object that stores the masks.
    model: nn.Module
        Model object which the mask can be applied on.

    Returns:
    -------
    prune_type: str
        Could be Filter or Batchnorm
    """
def detect_mask_prune_dim(masks, model):
    """
    Detect how the masks of convolutional layers are pruned.

    Parameters
    ----------
    masks: dict
        A dict object that stores the masks.
    model: nn.Module
        Model object which the mask can be applied on.
    Returns:
    -------
        How the masks of convolutional layers are pruned, this depends on pruning algorithms, it should
        return 1 for masks generated by AMCPruner, and returns 0 for masks generated by the rest
        NNI builtin pruners.
        0: filter pruning, prune filters of weights which causes channels of output feature maps are pruned.
        1: channel pruning, prune kernels corresponding to each input channels which causes channels of
           input feature maps are pruned.
    """
