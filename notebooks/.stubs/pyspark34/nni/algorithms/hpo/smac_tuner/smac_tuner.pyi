from .convert_ss_to_scenario import generate_scenario as generate_scenario
from _typeshed import Incomplete
from nni import ClassArgsValidator as ClassArgsValidator
from nni.common.hpo_utils import validate_search_space as validate_search_space
from nni.tuner import Tuner as Tuner
from nni.utils import OptimizeMode as OptimizeMode, extract_scalar_reward as extract_scalar_reward

logger: Incomplete

class SMACClassArgsValidator(ClassArgsValidator):
    def validate_class_args(self, **kwargs) -> None: ...

class SMACTuner(Tuner):
    """
    `SMAC <https://www.cs.ubc.ca/~hutter/papers/10-TR-SMAC.pdf>`__ is based on Sequential Model-Based Optimization (SMBO).
    It adapts the most prominent previously used model class (Gaussian stochastic process models)
    and introduces the model class of random forests to SMBO in order to handle categorical parameters.

    The SMAC supported by nni is a wrapper on `the SMAC3 github repo <https://github.com/automl/SMAC3>`__,
    following NNI tuner interface :class:`nni.tuner.Tuner`. For algorithm details of SMAC, please refer to the paper
    :footcite:t:`hutter2011sequential`.

    Note that SMAC on nni only supports a subset of the types in
    :doc:`search space </hpo/search_space>`:
    ``choice``, ``randint``, ``uniform``, ``loguniform``, and ``quniform``.

    Note that SMAC needs additional installation using the following command:

    .. code-block:: bash

        pip install nni[SMAC]

    ``swig`` is required for SMAC. for Ubuntu ``swig`` can be installed with ``apt``.

    Examples
    --------

    .. code-block::

        config.tuner.name = 'SMAC'
        config.tuner.class_args = {
            'optimize_mode': 'maximize'
        }

    Parameters
    ----------
    optimize_mode : str
        Optimize mode, 'maximize' or 'minimize', by default 'maximize'
    config_dedup : bool
        If True, the tuner will not generate a configuration that has been already generated.
        If False, a configuration may be generated twice, but it is rare for relatively large search space.
    """
    logger: Incomplete
    optimize_mode: Incomplete
    total_data: Incomplete
    optimizer: Incomplete
    smbo_solver: Incomplete
    first_one: bool
    update_ss_done: bool
    loguniform_key: Incomplete
    categorical_dict: Incomplete
    cs: Incomplete
    dedup: Incomplete
    def __init__(self, optimize_mode: str = 'maximize', config_dedup: bool = False) -> None: ...
    def update_search_space(self, search_space) -> None:
        """
        Convert search_space to the format that ``SMAC3`` could recognize, thus, not all the search space types
        are supported. In this function, we also do the initialization of `SMAC3`, i.e., calling ``self._main_cli``.

        NOTE: updating search space during experiment running is not supported.

        Parameters
        ----------
        search_space : dict
            The format could be referred to search space spec (https://nni.readthedocs.io/en/latest/Tutorial/SearchSpaceSpec.html).
        """
    def receive_trial_result(self, parameter_id, parameters, value, **kwargs) -> None:
        """
        Receive a trial's final performance result reported through :func:``nni.report_final_result`` by the trial.
        GridSearchTuner does not need trial's results.

        Parameters
        ----------
        parameter_id : int
            Unique identifier of used hyper-parameters, same with :meth:`generate_parameters`.
        parameters : dict
            Hyper-parameters generated by :meth:`generate_parameters`.
        value : dict
            Result from trial (the return value of :func:`nni.report_final_result`).

        Raises
        ------
        RuntimeError
            Received parameter id not in ``self.total_data``
        """
    def param_postprocess(self, challenger_dict):
        """
        Postprocessing for a set of hyperparameters includes:
            1. Convert the values of type ``loguniform`` back to their initial range.
            2. Convert ``categorical``: categorical values in search space are changed to list of numbers before,
               those original values will be changed back in this function.

        Parameters
        ----------
        challenger_dict : dict
            challenger dict

        Returns
        -------
        dict
            dict which stores copy of challengers
        """
    def generate_parameters(self, parameter_id, **kwargs):
        """
        Generate one instance of hyperparameters (i.e., one configuration).
        Get one from SMAC3's ``challengers``.

        Parameters
        ----------
        parameter_id : int
            Unique identifier for requested hyper-parameters. This will later be used in :meth:`receive_trial_result`.
        **kwargs
            Not used

        Returns
        -------
        dict
            One newly generated configuration
        """
    def generate_multiple_parameters(self, parameter_id_list, **kwargs):
        """
        Generate mutiple instances of hyperparameters. If it is a first request,
        retrieve the instances from initial challengers. While if it is not, request
        new challengers and retrieve instances from the requested challengers.

        Parameters
        ----------
        parameter_id_list: list of int
            Unique identifiers for each set of requested hyper-parameters.
            These will later be used in :meth:`receive_trial_result`.
        **kwargs
            Not used

        Returns
        -------
        list
            a list of newly generated configurations
        """
    def import_data(self, data) -> None:
        """
        Import additional data for tuning.

        Parameters
        ----------
        data : list of dict
            Each of which has at least two keys, ``parameter`` and ``value``.
        """
